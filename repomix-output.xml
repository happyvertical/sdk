This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.changeset/
  config.json
  README.md
.claude/
  commands/
    backlog.md
    deploy.md
    develop.md
    fresh.md
    icebox.md
    issue.md
    issues-close.md
    pr.md
    qa.md
    README.md
    todo.md
.gitea/
  workflows/
    agents/
      claude/
        on-assigned-issue.yaml
        on-assigned-pr.yaml
        test-issue-trigger.yaml
    on-merge-master.yaml
.github/
  scripts/
    validate-security.sh
  workflows/
    claude.yaml
    on-merged-master.yaml
    on-pr-master-dependabot.yml
  config-dependabot.yml
  SECURITY.md
.husky/
  _/
    pre-commit
    prepare-commit-msg
docs/
  adr/
    0001-use-typescript-for-all-packages.md
    README.md
    template.md
  workflow/
    DEFINITION_OF_DONE.md
    DEFINITION_OF_READY.md
    KANBAN.md
    README.md
    setup-project-board.sh
packages/
  ai/
    src/
      client.ts
      index.spec.ts
      index.ts
      message.ts
      thread.ts
    CLAUDE.md
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
  files/
    src/
      fetch.ts
      filesystem-local.ts
      filesystem.ts
      index.spec.ts
      index.ts
    CLAUDE.md
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
  pdf/
    src/
      index.spec.ts
      index.ts
      modules.d.ts
    CLAUDE.md
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
  smrt/
    src/
      class.ts
      collection.ts
      content.ts
      contents.spec.ts
      contents.ts
      document.ts
      fields.ts
      human.ts
      index.ts
      object.ts
      pleb.ts
      utils.spec.ts
      utils.ts
    CLAUDE.md
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
  spider/
    src/
      crawl4ai.spec.ts
      crawl4ai.ts
      index.spec.ts
      index.ts
    CLAUDE.md
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
  sql/
    src/
      index.spec.ts
      index.ts
      postgres.spec.ts
      postgres.ts
      sqlite.spec.ts
      sqlite.ts
      types.ts
    .gitignore
    .prettierrc
    CLAUDE.md
    docker-compose.yml
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
  utils/
    src/
      index.spec.ts
      index.ts
    CLAUDE.md
    package.json
    README.md
    tsconfig.json
    vitest.config.ts
scripts/
  generate-docs.js
  update-package-versions.cjs
  validate-build.js
  validate-changeset-config.js
  validate-package-json.js
.gitignore
.nvmrc
biome.json
bunfig.toml
CHANGELOG.md
CLAUDE.md
CONTRIBUTING.md
lefthook.yml
LICENSE
package.json
pnpm-workspace.yaml
README.md
setup_dev.sh
tsconfig.json
typedoc.json
vitest.config.ts
vitest.setup.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".changeset/config.json">
{
  "$schema": "https://unpkg.com/@changesets/config@3.0.4/schema.json",
  "changelog": "@changesets/cli/changelog",
  "commit": false,
  "fixed": [],
  "linked": [],
  "access": "restricted",
  "baseBranch": "main",
  "updateInternalDependencies": "patch",
  "ignore": []
}
</file>

<file path=".changeset/README.md">
# Changesets

Hello and welcome! This folder has been automatically generated by `@changesets/cli`, a build tool that works
with multi-package repos, or single-package repos to help you version and publish your code. You can
find the full documentation for it [in our repository](https://github.com/changesets/changesets)

We have a quick list of common questions to get you started engaging with this project in
[our documentation](https://github.com/changesets/changesets/blob/main/docs/common-questions.md)
</file>

<file path=".gitea/workflows/on-merge-master.yaml">
name: merged to master

on:
  push:
    branches:
      - master-disabled

jobs:
  versioning:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history.

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install pnpm
        run: npm install -g pnpm

      - name: Show pnpm version
        run: pnpm --version

      - name: Configure pnpm registry and authentication
        run: |
          pnpm config set @happyvertical:registry=https://git.grffn.net/api/packages/happyvertical/npm/
          pnpm config set @have:registry=https://git.grffn.net/api/packages/happyvertical/npm/
          echo "//git.grffn.net/api/packages/happyvertical/npm/:_authToken=${{ secrets.GITEATOKEN }}" > .npmrc

      - name: Cache pnpm store
        uses: actions/cache@v4
        with:
          path: ~/.pnpm-store
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install

      - name: Install Playwright Browsers
        run: pnpm dlx playwright install

      - name: Cache Playwright Browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright # Default Playwright cache directory.
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Build packages
        run: pnpm build

      - name: Commit and push changes for release
        run: |
          git config user.name "Gitea Bot"
          git config user.email "bot@example.com"

      - name: Bump version and release
        run: pnpm run release

      - name: Commit and push changes
        run: |
          git add .
          git commit -m "chore(release): bump version to $(node -p "require('./package.json').version")" --no-verify
          git push --no-verify --follow-tags

      - name: Publish packages
        run: pnpm run publish-packages
</file>

<file path="packages/ai/src/index.spec.ts">
import { it, expect } from 'vitest';
import { AIClient, OpenAIClient } from './client.js';
import { AIThread } from './thread.js';
it.skip('should create an AIClient and send it a message', async () => {
  console.log(process.env.OPENAI_API_KEY);
  const client = await OpenAIClient.create({
    apiKey: process.env.OPENAI_API_KEY!,
  });
  const result = await client.message('What is the capital of France?');
  expect(result.toLowerCase()).toContain('paris');
}, 30000);

it.skip('should create an AIThread and ask it a question', async () => {
  const options = {
    ai: {
      type: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    },
    prompt: 'What is the capital of France?',
  };

  const ai = await AIClient.create(options.ai);

  // lets talk about it
  const thread = await AIThread.create({
    ai: options.ai,
  });

  await thread.addSystem('You are a helpful assistant.'); // Add the system message

  await thread.addReference('Meeting Minutes', minutes);

  await thread.add({
    role: 'user',
    content:
      'Summarize the key decisions from the meeting and how they impact the budget.',
  });

  const response = await thread.do('Write a short summary.'); // The prompt here is now used *in addition to* the history.

  // const response = await thread.do({
  //   prompt: 'Write an article about what happened in the last meeting',
  //   responseFormat: 'html',
  // });
  // console.log({ response });

  console.log(response);
}, 30000);

const minutes =
  'V, 1/ 7\n' +
  'NRL — un ON\n' +
  'Town\n' +
  'Minutes of the Regular of the Council of the Town of Bentley November 26, 2024\n' +
  'Date and Place\n' +
  'In Attendance\n' +
  'Call to Order\n' +
  'Indigenous Acknowledgement\n' +
  'Agenda\n' +
  'Minutes of the Regular Meeting of the Council of the Town of Bentley held Tuesday, November 26, 2024, at 6:30 p.m., in the Bentley Municipal Office\n' +
  'Mayor Greg Rathjen Deputy Mayor Valiquette Councillor Eastman Councillor Hansen Councillor Grimsdale CAO, Marc Fortais\n' +
  'Mayor Rathjen called the regular council meeting to order at 6:30pm\n' +
  '“We acknowledge that we are meeting on Treaty 6 Territory and Home of Metis Nation Region 3, on land that is part of a historic agreement involving mutuality and respect. We recognize all the many First Nations, Metis, Inuit, and non-First Nations whose footsteps have marked these lands.”\n' +
  'Read by Mayor Rathjen\n' +
  'Motion 228/2024 Moved by Councillor Hansen, “THAT the agenda of the November 26, 2024, regular meeting of council be amended to include the following items as other business:\n' +
  '1) Gull Lake East Trail – letter of support to Lacombe County 2) Local Sustainability Grant Application\n' +
  'Carried\n' +
  'Motion 229/2024 Moved by Councillor Grimsdale, “THAT the‘amended agenda of the October 26, 2024, regular meeting of council be accepted.”\n' +
  'Carried\n' +
  'Regular Council Meeting Minutes November 26, 2024\n' +
  'Previous Minutes\n' +
  'Financial\n' +
  'New Business\n' +
  'Motion 23012024 Moved by Deputy Mayor Valiquette, “THAT the minutes of the October 22, 2024, Regular Meeting of Council be accepted.”\n' +
  'Carried\n' +
  'Motion 231/2024 Moved by Councillor Hansen, “THAT the minutes of the October 22, 2024, Organizational Meeting of Council be accepted.”\n' +
  'Carried\n' +
  'a) Prepaid Cheque Listing – Cheques No. 20240828 to 20240913\n' +
  'Motion 232/2024 Moved by Councillor Eastman, “THAT Cheques No. 20240778 to 20240827 be received for information.”\n' +
  'Carried\n' +
  'a) Delegation – Lacombe County Tourism – 2024 Annual Report\n' +
  'Motion 233/2024 Moved by Councillor Eastman, “THAT the report presented by Lacombe County Tourism – 2024 Annual Report be accepted as information: AND\n' +
  'THAT administration be directed to include funding for 2025 to support Lacombe Tourism in the preliminary budget for consideration by Mayor and Council.\n' +
  'Carried\n' +
  'b) Land Sale – Lot 41, Block 1, Plan 2320333 Motion 234/2024 Moved by Councillor Grimsdale, “THAT Mayor and Council approve the sale of 5604 48A Street (Lot 41, Block 1, Plan 2320333) located in the Tonw of Bentley, within the Sunset Heights Subdivision to Shane David Imber and Diane Marie Imber for the amount of $62,000\n' +
  '(including any applicable GST) subject to the following terms and conditions: Excepting thereout aII mines and minerals\n' +
  'Purchaser Shane David Imber Diane Marie Imber\n' +
  'Sale Price The Sale price is $62,000 including any applicable GST. But does not include any development costs or permits.\n' +
  'l\n' +
  'Regular Council Meeting Minutes November 26, 2024\n' +
  'Environmental Considerations The subject property is sold on an ”as is – where is” basis.\n' +
  'Fees and Disbursements The purchaser shall be responsible for all legal and registration fees associated with the transaction.\n' +
  'Vendor Conditions Subject to approval of this agreement by Town of Bentley Council before 9:00pm November 27, 2024, Seller will not provide an RPR\n' +
  'Purchaser Conditions Financing condition before 9:00pm November 29, 2024\n' +
  'Completion Day Contract completed, the purchase price fully paid and vacant possession given to the buyer at 12 noon on January 6, 2025 (this was amended form the original proposed date of January 2, 2025)"\n' +
  'Carried\n' +
  'c) Lacombe County – RC1 Grant Request $675,000 – Arena Slab Replacement\n' +
  'Motion 235/2024 Moved by Councillor Grimsdale, “THAT Mayor and Council, authorize CAO Marc Fortais to submit an RC1 grant application to Lacombe County to request funding of up to $675,000 (the maximum amount to fund 50% of the project costs for completion and replacement of‘ the Bentley Arena Slab, boards and glass; AND\n' +
  'The project to be completed in the 2025 budget year.”\n' +
  'Carried\n' +
  'd) Asset Management Phase III e Stormwater Plan o Wastewater Plan * Transportation Plan\n' +
  'Motion 236/2024 Moved by Deputy Mayor Valiquette, “THAT Mayor and Council approve the asset management plan reports prepared by Stantec for Stormwater, Wastewater, and Transportation; AND\n' +
  'I\n' +
  'Regular Council Meeting Minutes November 26, 2024\n' +
  'Break\n' +
  'Correspondence\n' +
  'Other Business\n' +
  'THAT Administration be directed to provide Mayor and Council with a reasonable rate strategy for utilities as a part of the 2025 Preliminary Budget approval process to ensure the establishment of a reasonable amount of reserve that does not create an excessive burden for the local rate payer.”\n' +
  'Carried\n' +
  'Councillor Grimsdale requested a break prior to reviewing the 3™ Quarter Financial Report\n' +
  'Motion 237/2024 Moved by Councillor Grimsdale, “THAT Mayor and Council take a short break of 10 minutes at 7:26pm, prior to reviewing the 3 Quarter Financial Report to be presented by the CAO.”\n' +
  'Carried\n' +
  'Mayor Rathjen called the meeting to order at 7:34 pm concluding the break.\n' +
  'e) 3” Quarter Financial Report Motion 238/2024 Moved by Councillor Eastman, “THAT the 3 Quarter Financial Report and presentation be accepted as information by Mayor and Council.\n' +
  'Carried\n' +
  'a) Lacombe County Council Highlights October 24, 2024 b) Lacombe County Council Highlights November 14, 2024\n' +
  'Motion 239/2024 Moved by Deputy Mayor Valiquette, “THAT correspondence item a to b be accepted as information.”\n' +
  'Carried\n' +
  'a) Gull Lake East Trail – Letter of Support to Lacombe County Motion 240/2024 Moved by Councillor Hansen, “THAT Mayor and Council provide a letter of support to Lacombe County for their grant application to the Alberta Strategic Transportation Infrastructure Program for the purpose of paving a 5km trail on the east side of Gull Lake.”\n' +
  'Carried\n' +
  'Regular Council Meeting Minutes November 26, 2024\n' +
  'b) Local Sustainability Grant Application – Town of Bentley Motion 241//2024 Moved by Councillor Eastman, “THAT the CAO be directed to apply to the Local Growth and Sustainability Grant program, under the sustainability component to support a sewer main replacement along 48” Ave in the 2026 budgetary year.”\n' +
  'Carried\n' +
  'Council Reports\n' +
  'a) Mayor Rathjen b) Deputy Mayor Valiquette c) Councillor Grimsdale d) Councillor Eastman e) Councillor Hansen\n' +
  'Motion 242/2024 Moved by Councillor Grimsdale, “THAT the council reports for October be accepted as information.”\n' +
  'Carried\n' +
  'Adjournment\n' +
  'Mayor Rathjen adjourned the meeting at 8:20pm\n' +
  'N ‘ pe. Mayor Greg Rathjen CAO Marc rtais\n' +
  '_— ee —— –— Regular Council Meeting Minutes November 26, 2024';
</file>

<file path="packages/ai/CLAUDE.md">
# @have/ai: AI Model Interface Package

## Purpose and Responsibilities

The `@have/ai` package provides a standardized interface for interacting with various AI models. It currently focuses on OpenAI's API but is designed with an abstraction layer to potentially support other AI providers in the future. This package:

- Offers a unified client interface for AI text completions
- Handles configuration for different AI providers
- Manages streaming responses and progress callbacks
- Provides type definitions for AI model parameters

## Key APIs

### Creating an AI Client

```typescript
import { getAIClient } from '@have/ai';

// Create an OpenAI client (default)
const client = await getAIClient({
  apiKey: 'your-api-key',
  baseUrl: 'https://api.openai.com/v1' // optional
});

// The client can then be used for text completions
```

### Text Completions

```typescript
import { getAIClient } from '@have/ai';

const client = await getAIClient({ apiKey: 'your-api-key' });

// Basic completion
const result = await client.textCompletion("What is the capital of France?");

// Completion with options
const resultWithOptions = await client.textCompletion("Generate a poem about coding", {
  model: "gpt-4o",
  temperature: 0.7,
  maxTokens: 500
});
```

### Streaming Responses

```typescript
import { getAIClient } from '@have/ai';

const client = await getAIClient({ apiKey: 'your-api-key' });

// Stream response with progress callback
const result = await client.textCompletion("Generate a long story", {
  stream: true,
  onProgress: (partialMessage) => {
    console.log("Received chunk:", partialMessage);
  }
});
```

### Configuration Options

The package supports various configuration options for AI completions:

```typescript
// Example with multiple options
const result = await client.textCompletion("Your prompt here", {
  model: "gpt-4o",            // AI model to use
  temperature: 0.7,           // Randomness (0-2)
  maxTokens: 500,             // Maximum response length
  stop: ["\n\n", "THE END"],  // Stop sequences
  role: "user",               // Message role
  history: [                  // Conversation history
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Who are you?" },
    { role: "assistant", content: "I'm an AI assistant." }
  ]
});
```

## Internal Architecture

The package uses a factory pattern with an abstract base class:

- `AIClient`: Abstract base class defining the common interface
- `OpenAIClient`: Implementation for OpenAI's API
- `getAIClient()`: Factory function that returns the appropriate client

The design allows for future expansion to other AI providers by creating new client implementations that extend the base class.

## Dependencies

- `openai`: The official OpenAI JavaScript/TypeScript client

## Development Guidelines

### Adding New AI Providers

To add support for a new AI provider:

1. Create a new client class that extends `AIClient`
2. Implement the required methods (especially `textCompletion`)
3. Update the type guard and factory function in `getAIClient()`
4. Add appropriate type definitions for the new provider's options

### Testing

The package includes tests for verifying client behavior:

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

Mock the actual API calls in tests to avoid external dependencies.

### Building

Build the package with:

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Keep API credentials secure (never hard-code them)
- Handle streaming responses efficiently
- Use appropriate error handling for API calls
- Set reasonable timeouts for AI model requests
- Consider rate limits of AI providers in implementation

This package serves as an abstraction layer over AI services, allowing the rest of the SDK to use AI capabilities without being tightly coupled to specific providers.
</file>

<file path="packages/ai/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "composite": true,
    "declaration": true,
    "declarationMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["dist", "../../node_modules"]
}
</file>

<file path="packages/files/src/index.spec.ts">
// index.test.ts
import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';
import * as fs from 'node:fs';
import * as path from 'node:path';
import {
  isFile,
  isDirectory,
  ensureDirectoryExists,
  upload,
  download,
  downloadFileWithCache,
  listFiles,
} from './index';
import { createServer, Server } from 'node:http';
import { TMP_DIR } from '@have/utils';

// Mock fs modulesq
// vi.mock('node:fs');
// vi.mock('node:fs/promises');

describe('File utilities', () => {
  let tmpDir: string;
  let server: Server;
  let serverUrl: string;

  beforeEach(() => {
    // Create a unique temporary directory for each test
    tmpDir = path.join(TMP_DIR, 'file-utils-test');
    fs.mkdirSync(tmpDir, { recursive: true });

    // Create and start test server
    server = createServer((req, res) => {
      if (req.method === 'PUT' && req.url === '/upload') {
        let data = '';
        req.on('data', (chunk) => {
          data += chunk;
        });
        req.on('end', () => {
          res.writeHead(200, { 'Content-Type': 'text/plain' });
          res.end('Upload successful');
        });
      } else if (req.method === 'PUT') {
        // Explicitly handle failed uploads
        res.writeHead(403, { 'Content-Type': 'text/plain' });
        res.end('Upload failed');
      } else if (req.url === '/test.txt') {
        res.writeHead(200, { 'Content-Type': 'text/plain' });
        res.end('Test content');
      } else {
        res.writeHead(404);
        res.end('Not found');
      }
    });

    return new Promise<void>((resolve) => {
      server.listen(0, '127.0.0.1', () => {
        const addr = server.address();
        if (addr && typeof addr === 'object') {
          serverUrl = `http://127.0.0.1:${addr.port}`;
        }
        resolve();
      });
    });
  });

  afterEach(() => {
    // Clean up temporary directory and server after each test
    fs.rmSync(tmpDir, { recursive: true, force: true });
    return new Promise<void>((resolve) => server.close(() => resolve()));
  });

  describe('isFile', () => {
    it('should return stat when path is a file', () => {
      const filePath = path.join(tmpDir, 'test.txt');
      fs.writeFileSync(filePath, 'test content');

      const result = isFile(filePath);
      expect(result).toBeTruthy();
      // expect(isDirectory(result?.)).toBe(false);
    });

    it('should return false when path is a directory', () => {
      const dirPath = path.join(tmpDir, 'test-dir');
      fs.mkdirSync(dirPath);

      const result = isFile(dirPath);
      expect(result).toBe(false);
    });

    it('should return false when path does not exist', () => {
      const result = isFile(path.join(tmpDir, 'non-existent.txt'));
      expect(result).toBe(false);
    });
  });

  describe('isDirectory', () => {
    it('should return true when path is a directory', () => {
      const dirPath = path.join(tmpDir, 'test-dir');
      fs.mkdirSync(dirPath);

      const result = isDirectory(dirPath);
      expect(result).toBe(true);
    });

    it('should throw error when path exists but is not a directory', () => {
      const filePath = path.join(tmpDir, 'test.txt');
      fs.writeFileSync(filePath, 'test content');

      expect(() => isDirectory(filePath)).toThrow();
    });

    it('should return false when path does not exist', () => {
      const result = isDirectory(path.join(tmpDir, 'non-existent-dir'));
      expect(result).toBe(false);
    });
  });

  describe('ensureDirectoryExists', () => {
    it('should create directory if it does not exist', async () => {
      const dirPath = path.join(tmpDir, 'new-dir');
      await ensureDirectoryExists(dirPath);
      expect(fs.existsSync(dirPath)).toBe(true);
      expect(fs.statSync(dirPath).isDirectory()).toBe(true);
    });

    it('should not throw if directory already exists', async () => {
      const dirPath = path.join(tmpDir, 'existing-dir');
      fs.mkdirSync(dirPath);

      await expect(ensureDirectoryExists(dirPath)).resolves.not.toThrow();
    });
  });

  describe('listFiles', () => {
    it('should list all files when no match pattern is provided', async () => {
      const files = ['file1.txt', 'file2.jpg', 'file3.png'];
      for (const file of files) {
        fs.writeFileSync(path.join(tmpDir, file), 'test content');
      }

      const result = await listFiles(tmpDir);
      expect(result.sort()).toEqual(files.sort());
    });

    it('should filter files based on match pattern', async () => {
      const files = ['file1.txt', 'file2.jpg', 'file3.png'];
      for (const file of files) {
        fs.writeFileSync(path.join(tmpDir, file), 'test content');
      }

      const result = await listFiles(tmpDir, { match: /\.txt$/ });
      expect(result).toEqual(['file1.txt']);
    });
  });

  describe('upload', () => {
    it('should upload data successfully', async () => {
      const response = await upload(`${serverUrl}/upload`, 'test-data');
      expect(response.ok).toBe(true);
    });

    it('should throw error on failed upload', async () => {
      await expect(
        upload(`${serverUrl}/nonexistent`, 'test-data'),
      ).rejects.toThrow();
    });
  });

  describe('download', () => {
    it('should download file successfully', async () => {
      const localPath = path.join(tmpDir, 'test.txt');
      const downloaded = await download(`${serverUrl}/test.txt`, localPath);
      expect(fs.existsSync(localPath)).toBe(true);
    });

    it('should throw error on failed download', async () => {
      await expect(
        download(`${serverUrl}/nonexistent`, path.join(tmpDir, 'nonexistent')),
      ).rejects.toThrow();
    });
  });

  describe('downloadFileWithCache', () => {
    it('should download and cache a text file', async () => {
      const targetPath = path.join(tmpDir, 'test.txt');

      await downloadFileWithCache(`${serverUrl}/test.txt`, targetPath);
      // console.log('targetPath', targetPath);
      // Verify the file exists and has content
      expect(fs.existsSync(targetPath)).toBe(true);
    });
  });
});
</file>

<file path="packages/files/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "dist",
    "rootDir": "src",
    "composite": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
</file>

<file path="packages/pdf/src/index.spec.ts">
import { it, expect } from 'vitest';
import { join } from 'node:path';
import { fileURLToPath } from 'node:url';
import { extractTextFromPDF } from './index.js';
// skipping because it takes too long and doesnt provide much value
it('should extract text from an image pdf', async () => {
  // const pdfPath = path.join(
  //   __dirname,
  //   '..',
  //   '/Signed-Meeting-Minutes-October-8-2024-Regular-Council-Meeting-1.pdf',
  // );

  // const text = await scribe.extractText([pdfPath]);
  // console.log(text);
  const pdfPath = join(
    fileURLToPath(new URL('.', import.meta.url)),
    '..',
    'test',
    'Signed-Meeting-Minutes-October-8-2024-Regular-Council-Meeting-1.pdf',
  );
  const text = await extractTextFromPDF(pdfPath);
  expect(text).toBeDefined();
  // console.log(text);
}, 30000);

it('should extract text from a problematic pdf', async () => {
  const pdfPath = join(
    fileURLToPath(new URL('.', import.meta.url)),
    '..',
    'test',
    'Agenda-Package-October-24-2023-Regular-Council-Meeting.pdf',
  );
  const text = await extractTextFromPDF(pdfPath);
  expect(text).toBeDefined();
}, 30000);
</file>

<file path="packages/pdf/CLAUDE.md">
# @have/pdf: PDF Processing Package

## Purpose and Responsibilities

The `@have/pdf` package provides tools for working with PDF documents, focusing on:

- Extracting text and content from PDF files
- Converting PDFs to other formats (text, JSON)
- OCR capabilities for image-based PDFs
- Analyzing PDF structure and metadata
- Processing PDF documents for AI consumption

This package is particularly useful for AI agents that need to analyze document content, extract information from PDFs, or process document collections.

## Key APIs

### Basic PDF Text Extraction

```typescript
import { extractText } from '@have/pdf';

// Extract text from a PDF file
const text = await extractText('/path/to/document.pdf');

// Extract text with options
const textWithOptions = await extractText('/path/to/document.pdf', {
  pages: [1, 2, 3],        // Specific pages to extract
  includeMetadata: true,   // Include document metadata
  preserveFormatting: true // Attempt to preserve original formatting
});
```

### PDF Content Analysis

```typescript
import { analyzePdf } from '@have/pdf';

// Get detailed analysis of PDF content
const analysis = await analyzePdf('/path/to/document.pdf');

console.log(analysis.metadata);    // Document metadata
console.log(analysis.pageCount);   // Number of pages
console.log(analysis.structure);   // Document structure
console.log(analysis.textContent); // Extracted text
console.log(analysis.images);      // Information about embedded images
```

### OCR for Image-Based PDFs

```typescript
import { performOcr } from '@have/pdf';

// Extract text from image-based PDF using OCR
const result = await performOcr('/path/to/scanned-document.pdf', {
  language: 'eng',           // OCR language
  improveResolution: true,   // Enhance image before OCR
  outputFormat: 'text'       // Output format (text, json, hocr)
});

console.log(result.text);         // Extracted text
console.log(result.confidence);   // OCR confidence score
```

### PDF to JSON Conversion

```typescript
import { pdfToJson } from '@have/pdf';

// Convert PDF to structured JSON
const json = await pdfToJson('/path/to/document.pdf');

// The result includes structured content with layout information
console.log(json.pages);           // Array of page objects
console.log(json.pages[0].texts);  // Text elements on first page
console.log(json.pages[0].tables); // Detected tables on first page
```

### PDF Metadata Extraction

```typescript
import { extractMetadata } from '@have/pdf';

// Extract only metadata from a PDF
const metadata = await extractMetadata('/path/to/document.pdf');

console.log(metadata.title);       // Document title
console.log(metadata.author);      // Author
console.log(metadata.creationDate); // Creation date
console.log(metadata.keywords);    // Keywords
```

## Dependencies

The package has the following dependencies:

- `pdfjs-dist`: Mozilla's PDF.js for PDF parsing and rendering
- `tesseract.js`: For OCR capabilities
- `scribe.js-ocr`: Enhanced OCR processing
- `date-fns`: For date manipulation
- `pluralize`: For text processing utilities

## Development Guidelines

### PDF Processing Considerations

- PDFs can be large and complex; implement streaming where possible
- Handle different PDF versions and features gracefully
- Consider memory usage when processing large documents
- Implement timeout mechanisms for long-running operations

### OCR Strategy

- Use OCR only when necessary (image-based PDFs)
- Implement pre-processing to improve OCR results
- Consider language-specific OCR models for better accuracy
- Cache OCR results to avoid repeated processing

### Error Handling

- Handle malformed or password-protected PDFs
- Provide meaningful error messages for different failure modes
- Implement fallback strategies when primary extraction fails
- Validate PDF files before processing

### Testing

The package includes tests for verifying PDF processing:

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

Tests use sample PDFs of different types and complexity.

### Building

Build the package with:

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Log processing steps for debugging complex PDF issues
- Implement retry mechanisms for unreliable operations
- Use appropriate timeout values for processing operations
- Consider progressive enhancement (basic text extraction first, OCR as fallback)
- Normalize extracted text for consistent downstream processing
- Preserve document structure when possible for better analysis

This package provides specialized tools for working with PDF documents, making them accessible to AI agents and other components of the HAVE SDK.
</file>

<file path="packages/pdf/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "composite": true,
    "declaration": true,
    "declarationMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["dist", "node_modules", "**/*.test.ts"]
}
</file>

<file path="packages/smrt/src/contents.spec.ts">
import { it, expect } from 'vitest';
import os from 'node:os';
import path from 'node:path';
import fs from 'node:fs';
import { makeSlug } from '@have/utils';
import { Contents } from './contents.js';
import { faker } from '@faker-js/faker';

const TMP_DIR = path.resolve(`${os.tmpdir()}/.have-sdk-tests/contents`);
fs.mkdirSync(TMP_DIR, { recursive: true });

it('should be able to getOrInsert a content item', async () => {
  const contents = await Contents.create({
    ai: {
      type: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    },
    db: {
      url: `file:${TMP_DIR}/test.db`,
    },
  });

  const fakeContentData = {
    title: faker.lorem.sentence(),
    body: faker.lorem.paragraph(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
  };

  const content = await contents.getOrUpsert(fakeContentData);
  expect(content.id).toBeDefined();

  const content2 = await contents.getOrUpsert(fakeContentData);
  expect(content2.id).toBe(content.id);

  const got = await contents.get({ id: content.id });
  expect(got?.id).toEqual(content.id);
});

it('should respect the context of the slug', async () => {
  const contents = await Contents.create({
    ai: {
      type: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    },
    db: {
      url: `file:${TMP_DIR}/test.db`,
    },
  });

  const fakeContentData = {
    title: faker.lorem.sentence(),
    body: faker.lorem.paragraph(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
  };

  const slug = makeSlug(fakeContentData.title);

  const content = await contents.getOrUpsert({
    ...fakeContentData,
    url: 'http://setinfirst.com',
    slug,
    context: 'contextA',
  });
  expect(content.id).toBeDefined();

  const different = await contents.getOrUpsert({
    ...fakeContentData,
    slug,
    context: 'contextB',
    source: 'set in different context',
  });
  expect(different.id).not.toBe(content.id);

  const contextA = await contents.get({
    slug,
    context: 'contextA',
  });

  const contextB = await contents.get({
    slug,
    context: 'contextB',
  });

  const updated = await contents.getOrUpsert({
    description: 'foo',
    slug,
    context: 'contextA',
  });

  expect(updated.id).toBeDefined();
  expect(updated.description).toBe('foo');
  expect(updated.id).toBe(contextA?.id);
});

// skipped because it takes a long time
it.skip('should be able to mirror a bit of content give a url', async () => {
  const contents = await Contents.create({
    ai: {
      type: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    },
    db: {
      url: `file:${TMP_DIR}/test.db`,
    },
  });

  const created = await contents.mirror({
    url: 'https://townofbentley.ca/wp-content/uploads/2024/12/Signed-Minutes-November-26-2024-Regular-Council-Meeting.pdf',
    mirrorDir: `${TMP_DIR}/mirror-test`,
  });
  expect(created?.id).toBeDefined();
}, 60000);

it.skip('should be able to sync a content dir', async () => {
  const contents = await Contents.create({
    ai: {
      type: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    },
    db: {
      url: `file:${TMP_DIR}/test.db`,
    },
    fs: {
      type: 'filesystem',
      cacheDir: `${TMP_DIR}/cache`,
    },
  });

  // for (let x = 0; x < 10; x++) {
  await contents.getOrUpsert({
    type: 'article',
    title: faker.lorem.sentence(),
    description: faker.lorem.sentence(),
    body: faker.lorem.paragraph(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
  });
  // }

  // await contents.syncContentDir({ contentDir: `${TMP_DIR}/content` });
});

it('should be able to list content', async () => {
  const contents = await Contents.create({
    ai: {
      type: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    },
    db: {
      url: `file::memory:?cache=shared`, //todo: memory doesnt work because we pass around the connection,
    },
  });

  const fakeContentData = {
    type: 'article',
    title: faker.lorem.sentence(),
    body: faker.lorem.paragraph(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
  };

  const content = await contents.getOrUpsert(fakeContentData);
  await content.save();

  const fakeContentData2 = {
    title: faker.lorem.sentence(),
    body: faker.lorem.paragraph(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
  };

  const content2 = await contents.getOrUpsert(fakeContentData2);
  await content2.save();

  const fakeContentData3 = {
    title: faker.lorem.sentence(),
    body: faker.lorem.paragraph(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
  };
  const content3 = await contents.getOrUpsert(fakeContentData3);
  await content3.save();

  expect(content.id).toBeDefined();

  // const content2 = await contents.getOrUpsert(fakeContentData);
  // expect(content2.id).toBe(content.id);

  const articles = await contents.list({
    where: {
      type: 'article',
    }
  });
  expect(articles?.length).toEqual(1);

  const articleCount = await contents.count({
    where: {
      type: 'article',
    }
  });
  expect(articleCount).toEqual(1);
});
</file>

<file path="packages/smrt/src/pleb.ts">
import { BaseObject } from './object.js';
import type { BaseObjectOptions } from './object.js';

export interface PlebOptions extends BaseObjectOptions {}

export class Pleb<T extends PlebOptions = PlebOptions> extends BaseObject<T> {
  constructor(options: T) {
    super(options);
    this._className = this.constructor.name;
  }

  static async create(options: PlebOptions) {
    const pleb = new Pleb(options);
    await pleb.initialize();
    return pleb;
  }

  protected async initialize(): Promise<void> {
    await super.initialize();
    // const db = await getDatabase();
    // const schema = await syncSchema(options.schema);
  }

  // protected async getThread(options: {
  //   prompt: string;
  //   references: Content[];
  //   ai: GetAIClientOptions;
  // }) {
  //   const ai = options.ai
  //     ? await getAIClient(options.ai)
  //     : await getAIClient(this.options.ai);

  //   const thread = await AIThread.create({
  //     ai,
  //   });

  //   thread.addMessage({
  //     role: 'system',
  //     content: options.prompt,
  //   });

  //   for (const reference of options.references) {
  //     thread.addMessage({
  //       role: 'system',
  //       content: JSON.stringify(reference),
  //     });
  //   }

  //   const contentPrompt = `
  //     You are a writer for a local newspaper.
  //     You are given a bit of content from the internet and you are asked to write a short article about it.
  //     The article should be 100 words or less.
  //   `;
  //   const body = await thread.addMessage(contentPrompt);

  //   console.log(body);
  // }
}
</file>

<file path="packages/smrt/src/utils.spec.ts">
import { it, expect } from 'vitest';
import { fieldsFromClass, contentToString, stringToContent } from './utils.js';
import { faker } from '@faker-js/faker';
import { Content } from './content.js';
// Test class with various field types
class TestClass {
  test_string: string = 'test';
  test_number: number = 123;
  test_date: Date = new Date();
  private _privateField: string = 'private';
  methodField() {
    return true;
  }
}

it('should get fields from a class without values', () => {
  const fields = fieldsFromClass(TestClass);
  expect(fields).toEqual({
    test_string: {
      name: 'test_string',
      type: 'TEXT',
    },
    test_number: {
      name: 'test_number',
      type: 'INTEGER',
    },
    test_date: {
      name: 'test_date',
      type: 'DATETIME',
    },
  });

  // Verify private and method fields are excluded
  expect(fields).not.toHaveProperty('_privateField');
  expect(fields).not.toHaveProperty('methodField');
});

it('should get fields from a class with values', () => {
  const values = {
    test_string: 'custom value',
    test_number: 456,
    test_date: '2024-01-01',
    extraField: 'should not appear',
  };

  const fields = fieldsFromClass(TestClass, values);

  expect(fields).toEqual({
    test_string: {
      name: 'test_string',
      type: 'TEXT',
      value: 'custom value',
    },
    test_number: {
      name: 'test_number',
      type: 'INTEGER',
      value: 456,
    },
    test_date: {
      name: 'test_date',
      type: 'DATETIME',
      value: '2024-01-01',
    },
  });

  // Verify extra field from values doesn't appear
  expect(fields).not.toHaveProperty('extraField');
});

it.only('should be able to parse a content string', () => {
  const data = {
    type: 'article',
    title: faker.lorem.sentence(),
    author: faker.person.fullName(),
    publish_date: faker.date.recent(),
    body: faker.lorem.paragraph(),
  };

  const toString = contentToString(data as Content);
  const toObject = stringToContent(toString);
  expect(toObject).toEqual(data);
});
</file>

<file path="packages/smrt/CLAUDE.md">
# @have/smrt: AI Agent Framework Package

## Purpose and Responsibilities

The `@have/smrt` package is the core framework for building vertical AI agents in the HAVE SDK. It integrates functionality from all other packages to provide:

- A coherent object model for AI agents with database persistence
- Collection-based management of objects
- Automatic schema generation and database table creation
- Standardized interfaces for AI interactions
- Utilities for working with different data sources

Despite its tongue-in-cheek name, this package is the central nervous system of the SDK, connecting database capabilities, file system operations, and AI model interactions.

## Key Concepts

### BaseClass

The foundation for all classes in the framework, providing:
- Initialization logic
- Access to AI client and database interfaces
- Shared utilities

### BaseObject

Extends BaseClass to represent individual entities that:
- Can be saved to a database
- Have unique identifiers (id, slug, etc.)
- Support property-based schema generation
- Include timestamps (created_at, updated_at)

### BaseCollection

Extends BaseClass to represent collections of objects that:
- Automatically set up database tables based on object schemas
- Provide CRUD operations for managing objects
- Support flexible querying with multiple operators
- Handle relationships between objects

## Key APIs

### Defining a Custom Object

```typescript
import { BaseObject } from '@have/smrt';

class Document extends BaseObject<any> {
  title: string = '';
  content: string = '';
  category: string = '';
  tags: string[] = [];
  isPriority: boolean = false;
  
  constructor(options: any) {
    super(options);
    // Copy properties from options to this instance
    Object.assign(this, options);
  }
  
  async summarize() {
    // Use AI to summarize the document
    if (this.options.ai && this.content) {
      return this.options.ai.textCompletion(
        `Summarize this document: ${this.content.substring(0, 2000)}`
      );
    }
    return null;
  }
}
```

### Defining a Collection

```typescript
import { BaseCollection } from '@have/smrt';
import { Document } from './document';

class DocumentCollection extends BaseCollection<Document> {
  static readonly _itemClass = Document;
  
  constructor(options: any) {
    super(options);
  }
  
  async findSimilar(documentId: string) {
    const document = await this.get(documentId);
    if (!document) return [];
    
    // Custom logic to find similar documents
    return this.list({
      where: { category: document.category },
      limit: 5
    });
  }
}
```

### Using Objects and Collections

```typescript
import { getAIClient } from '@have/ai';
import { getSqliteClient } from '@have/sql';
import { DocumentCollection } from './documentCollection';

async function main() {
  // Set up dependencies
  const ai = await getAIClient({ apiKey: 'your-api-key' });
  const db = await getSqliteClient({ filename: 'documents.db' });
  
  // Create and initialize collection
  const documents = new DocumentCollection({ ai, db });
  await documents.initialize();
  
  // Create and save an object
  const doc = await documents.create({
    title: 'Getting Started',
    content: 'This is a guide to getting started with the HAVE SDK...',
    category: 'Documentation'
  });
  await doc.save();
  
  // Query objects
  const docs = await documents.list({
    where: { category: 'Documentation' },
    limit: 10,
    orderBy: 'created_at DESC'
  });
  
  // Use AI capabilities
  const summary = await doc.summarize();
}
```

### Advanced Querying

```typescript
// Complex query example
const results = await collection.list({
  where: {
    'created_at >': '2023-01-01',
    'priority': 'high',
    'status in': ['pending', 'in-progress'],
    'title like': '%important%'
  },
  orderBy: ['priority DESC', 'created_at DESC'],
  limit: 20,
  offset: 0
});

// Count matching records
const count = await collection.count({
  where: { category: 'reports' }
});
```

## Internal Architecture

The package uses:
- Schema generation based on class properties
- SQLite triggers for automatic timestamp management
- A consistent pattern for database operations
- Integration with AI models via the `@have/ai` package

## Dependencies on Other Packages

`@have/smrt` depends on all other packages in the SDK:

- `@have/ai`: For AI model interactions
- `@have/files`: For file system operations
- `@have/pdf`: For PDF document processing
- `@have/sql`: For database operations
- `@have/spider`: For web content retrieval
- `@have/utils`: For utility functions

## Development Guidelines

### Extending the Framework

To extend the framework:

1. Create custom objects by extending `BaseObject`
2. Create custom collections by extending `BaseCollection`
3. Define properties on objects that will be persisted to the database
4. Implement custom methods that leverage AI capabilities when needed

### Database Schema Considerations

- Object properties define the database schema
- Initialize properties with default values in the constructor
- Use appropriate JavaScript types for proper schema generation
- Properties are converted to snake_case for database columns

### Testing

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

### Building

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Clearly define object schemas with appropriate defaults
- Use transactions for complex database operations
- Keep AI prompts clear and focused
- Handle failures gracefully, especially for AI and database operations
- Follow the collection pattern for managing groups of related objects
- Initialize properties in constructors to ensure proper schema generation

The `@have/smrt` package exemplifies the "fast and loose" approach mentioned in the README, prioritizing developer velocity while maintaining enough structure for consistent and reliable behavior.
</file>

<file path="packages/smrt/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "composite": true,
    "declaration": true,
    "declarationMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["dist", "node_modules", "**/*.spec.ts"]
}
</file>

<file path="packages/spider/src/crawl4ai.spec.ts">
import { it, expect } from 'vitest';

it('should be able to use crawl4ai', () => {
  expect(true).toBe(true);
});
</file>

<file path="packages/spider/CLAUDE.md">
# @have/spider: Web Crawling and Content Extraction Package

## Purpose and Responsibilities

The `@have/spider` package (formerly known as "web") provides tools for crawling websites, extracting content, and processing web data. It is designed to:

- Navigate and interact with web pages programmatically
- Extract structured content from HTML pages
- Convert web content into clean, readable formats
- Handle different content types and structures
- Support both simple requests and browser-based interactions

This package is particularly useful for AI agents that need to process web content for analysis, search, or knowledge extraction.

## Key APIs

### Basic Web Scraping

```typescript
import { scrapeUrl } from '@have/spider';

// Extract content from a URL
const content = await scrapeUrl('https://example.com/article');

// The result includes extracted text, metadata, and other useful information
console.log(content.title);       // Page title
console.log(content.text);        // Main content text
console.log(content.description); // Meta description
console.log(content.links);       // Array of links found on the page
```

### Browser-Based Crawling

```typescript
import { Browser } from '@have/spider';

// Initialize a browser instance
const browser = await Browser.create();

// Navigate to a page
const page = await browser.newPage('https://example.com');

// Wait for specific content to load
await page.waitForSelector('.content-loaded');

// Extract content after JavaScript execution
const content = await page.extractContent();

// Take a screenshot
await page.screenshot({ path: '/path/to/screenshot.png' });

// Close the browser when done
await browser.close();
```

### HTML Parsing with Cheerio

```typescript
import { parseHtml } from '@have/spider';

// Parse HTML content
const $ = parseHtml('<html><body><div class="content">Hello</div></body></html>');

// Extract data using Cheerio selectors
const text = $('.content').text();
const links = $('a').map((i, el) => $(el).attr('href')).get();
```

### Readability Processing

```typescript
import { makeReadable } from '@have/spider';

// Convert HTML to a clean, readable format
const article = await makeReadable('<html>complex page html...</html>');

// Access the cleaned content
console.log(article.title);   // Extracted title
console.log(article.content); // Clean HTML content
console.log(article.textContent); // Plain text content
```

### Recursive Crawling

```typescript
import { crawlSite } from '@have/spider';

// Crawl a site with options
const results = await crawlSite('https://example.com', {
  maxDepth: 2,
  maxPages: 50,
  include: [/\/blog\//],
  exclude: [/\/author\//],
  delay: 1000
});

// Process the results
for (const page of results) {
  console.log(page.url, page.title, page.text);
}
```

## Dependencies

The package has the following dependencies:

- `@have/files`: For file system operations (saving content, screenshots)
- `@have/utils`: For utility functions
- `@mozilla/readability`: For extracting readable content from web pages
- `playwright`: For browser automation and JavaScript-rendered content
- `cheerio`: For HTML parsing and manipulation

## Development Guidelines

### Web Scraping Ethics and Performance

- Respect `robots.txt` directives
- Implement rate limiting to avoid overwhelming servers
- Use conditional requests with appropriate headers (If-Modified-Since)
- Cache results when appropriate
- Add proper user agent identification

### Browser Management

- Reuse browser instances when possible
- Close browser instances when done to prevent resource leaks
- Use headless mode for production environments
- Handle network errors and timeouts gracefully

### Content Extraction

- Normalize extracted content (whitespace, encoding)
- Preserve important metadata (title, author, date)
- Remove boilerplate elements (ads, navigation)
- Convert relative URLs to absolute URLs
- Handle different content types appropriately

### Testing

The package includes tests for verifying scraping behavior:

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

Use mock servers or recorded responses for testing to avoid external dependencies.

### Building

Build the package with:

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Implement proper error handling for network issues
- Use selectors that are resistant to minor HTML changes
- Cache responses to reduce load on target servers
- Set appropriate timeouts for page loading
- Clean up resources (especially browser instances) after use

This package provides the web interaction capabilities needed by AI agents to process online content effectively and responsibly.
</file>

<file path="packages/spider/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "composite": true
  },
  "include": ["src/**/*"]
}
</file>

<file path="packages/sql/src/index.spec.ts">
import { it, describe, expect } from "vitest";
import { getDatabase } from "./index.js";
import path from "path";
import { tmpdir } from "os";
import { syncSchema } from "./index.js";
import { buildWhere } from "./index.js";
const TMP_DIR = path.resolve(`${tmpdir()}/kissd`);

it.skip("should be able to get the adapter for a postgres database", async () => {
  const db = await getDatabase({
    type: "postgres",
    database: process.env.SQLOO_NAME || "sqloo",
    host: process.env.SQLOO_HOST || "localhost",
    user: process.env.SQLOO_USER || "sqloo",
    password: process.env.SQLOO_PASS || "sqloo",
    port: Number(process.env.SQLOO_PORT) || 5432,
  });
  expect(db.client).toBeDefined();
});

it("should be able to get the adapter for a sqlite database", async () => {
  const db = await getDatabase({
    type: "sqlite",
  });
  expect(db.client).toBeDefined();
});

it("should be able to get the adapter for an in memory sqlite database", async () => {
  const db = await getDatabase({
    type: "sqlite",
    url: ":memory:",
  });
  expect(db.client).toBeDefined();
});

it("should be able to sync a table schema", async () => {
  const db = await getDatabase({
    type: "sqlite",
    url: ":memory:",
  });
  // console.log({ db });
  await syncSchema({
    db,
    schema: `
        CREATE TABLE test (
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          name TEXT
        )
      `,
  });
});

it("should handle basic usage with different operators", () => {
  const result = buildWhere({
    status: "active",
    "price >": 100,
    "stock <=": 5,
    "category in": ["A", "B"],
    "name like": "%shirt%",
  });

  expect(result.sql).toBe(
    "WHERE status = $1 AND price > $2 AND stock <= $3 AND category IN $4 AND name LIKE $5",
  );
  expect(result.values).toEqual(["active", 100, 5, ["A", "B"], "%shirt%"]);
});

it("should handle NULL values correctly", () => {
  const result = buildWhere({
    deleted_at: null,
    "updated_at !=": null,
    status: "active",
  });

  expect(result.sql).toBe(
    "WHERE deleted_at IS NULL AND updated_at IS NOT NULL AND status = $1",
  );
  expect(result.values).toEqual(["active"]);
});

it("should handle price range conditions", () => {
  const result = buildWhere({
    "price >=": 10,
    "price <": 100,
  });

  expect(result.sql).toBe("WHERE price >= $1 AND price < $2");
  expect(result.values).toEqual([10, 100]);
});

it("should handle date filtering with null check", () => {
  const startDate = new Date("2024-01-01");
  const endDate = new Date("2024-12-31");

  const result = buildWhere({
    "created_at >": startDate,
    "created_at <=": endDate,
    deleted_at: null,
  });

  expect(result.sql).toBe(
    "WHERE created_at > $1 AND created_at <= $2 AND deleted_at IS NULL",
  );
  expect(result.values).toEqual([startDate, endDate]);
});

it("should handle LIKE operators for search", () => {
  const result = buildWhere({
    "title like": "%search%",
    "description like": "%search%",
    status: "published",
  });

  expect(result.sql).toBe(
    "WHERE title LIKE $1 AND description LIKE $2 AND status = $3",
  );
  expect(result.values).toEqual(["%search%", "%search%", "published"]);
});

it("should handle IN clauses with arrays", () => {
  const result = buildWhere({
    "role in": ["admin", "editor"],
    active: true,
    "last_login !=": null,
  });

  expect(result.sql).toBe(
    "WHERE role IN $1 AND active = $2 AND last_login IS NOT NULL",
  );
  expect(result.values).toEqual([["admin", "editor"], true]);
});
</file>

<file path="packages/sql/src/postgres.spec.ts">
import { it, expect, describe, beforeEach, afterEach } from "vitest";
import { randomUUID } from "crypto";
import { getDatabase } from "./index.js";
describe.skip("postgres tests", () => {
  let db: Awaited<ReturnType<typeof getDatabase>>;
  beforeEach(async () => {
    db = await getDatabase({
      type: "postgres",
      database: process.env.SQLOO_NAME || "sqloo",
      host: process.env.SQLOO_HOST || "localhost",
      user: process.env.SQLOO_USER || "sqloo",
      password: process.env.SQLOO_PASS || "sqloo",
      port: Number(process.env.SQLOO_PORT) || 5432,
    });

    await db.execute`
      create extension if not exists "uuid-ossp";
      drop table if exists contents;
      create table contents (
        id uuid primary key not null default (uuid_generate_v4()),
        title text, 
        body text
      )
    `;
  });

  afterEach(async () => {
    await db.execute`drop table contents`;
    await db.client.end();
  });

  it("should be able to perform a statement", async () => {
    const result = await db.many`
      select * from contents
    `;
    expect(result).toEqual(expect.arrayContaining([]));
  });

  it("should be able to insert data", async () => {
    const inserted = await db.insert("contents", {
      title: "hello",
      body: "world",
    });
    expect(inserted).toBeDefined();
    expect(inserted.affected).toBe(1);
  });

  it("should be able to insert multiple rows at a time", async () => {
    const inserted = await db.insert("contents", [
      {
        title: "hello",
        body: "world",
      },
      {
        title: "hi",
        body: "universe",
      },
    ]);
    expect(inserted.affected).toBe(2);
  });

  it("should be able to query data with a condition", async () => {
    await db.insert("contents", { title: "hello", body: "world" });
    const result = await db.many`
      select * from contents where title = ${"hello"}
    `;
    expect(result[0]).toEqual(
      expect.objectContaining({
        id: expect.any(String),
        title: "hello",
        body: "world",
      }),
    );
  });

  it("should be able to get a single row", async () => {
    await db.insert("contents", { title: "hello", body: "world" });
    const result = await db.single`
      select * from contents where title = ${"hello"}
    `;
    expect(result).toEqual(
      expect.objectContaining({
        id: expect.any(String),
        title: "hello",
        body: "world",
      }),
    );
  });

  it("should be able to update a row", async () => {
    const id = randomUUID();
    const inserted = await db.insert("contents", {
      id,
      title: "hello",
      body: "world",
    });
    expect(inserted.affected).toBe(1);
    const updated = await db.update(
      "contents",
      { id },
      { title: "hi", body: "universe" },
    );
    expect(updated.affected).toBe(1);
    const result = await db.oO`
      select * from contents where id = ${id}
    `;
    expect(result?.id).toEqual(id);
    expect(result?.title).toEqual("hi");
    expect(result?.body).toEqual("universe");
  });
});
</file>

<file path="packages/sql/src/sqlite.spec.ts">
import { it, expect, describe, beforeEach, afterEach } from "vitest";
import { randomUUID } from "node:crypto";
import { getDatabase } from "./index.js";
// import type { Database } from "./types";

describe("sqlite tests", () => {
  let db: any;

  beforeEach(async () => {
    db = await getDatabase({
      type: "sqlite",
    });
    await db.execute`
      create table contents (
        id uuid primary key not null,
        title text, 
        body text
      )
    `;
  });

  afterEach(async () => {
    await db.execute`
      drop table contents
    `;
  });

  it("should be able to perform a statement", async () => {
    const result = await db.many`
      select * from contents
    `;
    expect(result).toEqual([]);
  });

  it("should be able to insert data", async () => {
    const data = {
      id: randomUUID(),
      title: "hello",
      body: "world",
    } as const;
    const inserted = await db.insert("contents", data);
    expect(inserted).toBeDefined();
  });

  it("should be able to query data with a condition", async () => {
    const data = {
      id: randomUUID(),
      title: "hello",
      body: "world",
    } as const;
    await db.insert("contents", data);
    const result = await db.single`
      select * from contents where id = ${data.id}
    `;
    expect(result).toEqual({
      id: data.id,
      title: data.title,
      body: data.body,
    });
  });

  it("should be able to update a row", async () => {
    const data = {
      id: randomUUID(),
      title: "hello",
      body: "world",
    } as const;
    await db.insert("contents", data);
    await db.update(
      "contents",
      { id: data.id },
      { title: "hi", body: "universe" },
    );
    const result = await db.single`
      select * from contents where id = ${data.id}
    `;
    expect(result).toEqual({ id: data.id, title: "hi", body: "universe" });
  });
});
</file>

<file path="packages/sql/.gitignore">
node_modules/
.envrc
.env
</file>

<file path="packages/sql/.prettierrc">
{
  "tabWidth": 2,
  "useTabs": false
}
</file>

<file path="packages/sql/CLAUDE.md">
# @have/sql: Database Interface Package

## Purpose and Responsibilities

The `@have/sql` package provides a standardized interface for SQL database operations, with specific support for SQLite and PostgreSQL. It is designed to:

- Abstract away database-specific implementation details
- Provide a consistent API for common database operations
- Support schema synchronization for easy table creation and updates
- Handle query building and parameter binding securely
- Enable vector search capabilities with SQLite-VSS

Unlike full-featured ORMs, this package is intentionally lightweight, focusing on providing just enough abstraction while maintaining direct SQL access when needed.

## Key APIs

### Database Client Creation

```typescript
import { getSqliteClient, getPostgresClient } from '@have/sql';

// Create an SQLite client
const sqliteDb = await getSqliteClient({
  filename: 'database.db'  // Creates in-memory database if not specified
});

// Create a PostgreSQL client
const pgDb = await getPostgresClient({
  host: 'localhost',
  port: 5432,
  database: 'mydb',
  user: 'username',
  password: 'password'
});
```

### Basic Query Operations

```typescript
// Run a simple query
const { rows } = await db.query('SELECT * FROM users WHERE id = ?', ['user123']);

// Run a parameterized query with named parameters
const result = await db.query(
  'INSERT INTO users (id, name, email) VALUES (?, ?, ?)',
  ['user123', 'John Doe', 'john@example.com']
);

// Get a single value
const count = await db.pluck`SELECT COUNT(*) FROM users WHERE active = ${true}`;
```

### Schema Operations

```typescript
// Define a schema
const schema = {
  users: {
    id: { type: 'TEXT', primaryKey: true },
    name: { type: 'TEXT', notNull: true },
    email: { type: 'TEXT', unique: true },
    created_at: { type: 'TEXT' },
    updated_at: { type: 'TEXT' }
  }
};

// Synchronize schema (creates or updates tables as needed)
await syncSchema({ db, schema });
```

### Query Building

```typescript
// Build a WHERE clause from an object
const { sql, values } = buildWhere({
  status: 'active',
  'created_at >': '2023-01-01',
  'role in': ['admin', 'editor']
});

// Use in a query
const { rows } = await db.query(
  `SELECT * FROM users ${sql}`,
  values
);
```

### Transaction Support

```typescript
// Run operations in a transaction
await db.transaction(async (tx) => {
  await tx.query('INSERT INTO users (id, name) VALUES (?, ?)', ['user1', 'User One']);
  await tx.query('INSERT INTO profiles (user_id, bio) VALUES (?, ?)', ['user1', 'My bio']);
});
```

### Vector Search (with SQLite-VSS)

```typescript
// Create a vector search table
await db.query(`
  CREATE VIRTUAL TABLE IF NOT EXISTS embeddings USING vss0(
    id TEXT,
    embedding(1536) FLOAT,
    content TEXT
  );
`);

// Insert vector data
await db.query(`
  INSERT INTO embeddings (id, embedding, content) VALUES (?, ?, ?)
`, [
  'doc1', 
  new Float32Array([0.1, 0.2, /* ... */]).buffer, 
  'Document content'
]);

// Perform vector search
const { rows } = await db.query(`
  SELECT id, content, distance
  FROM embeddings
  WHERE vss_search(embedding, ?)
  LIMIT 10
`, [new Float32Array([0.2, 0.3, /* ... */]).buffer]);
```

## Dependencies

The package has the following dependencies:

- `@libsql/client`: For SQLite database operations
- `sqlite-vss`: For vector search capabilities in SQLite
- `pg`: For PostgreSQL database operations

## Development Guidelines

### Adding New Database Features

When adding new features:

1. Ensure consistent API across database engines
2. Implement for both SQLite and PostgreSQL where applicable
3. Add appropriate error handling and type checking
4. Write tests that work with both database engines

### Query Safety

- Always use parameterized queries to prevent SQL injection
- Validate table and column names when dynamically generating SQL
- Use the query building utilities for complex conditions
- Add type definitions for query results where possible

### Schema Management

- Keep schema definitions declarative and database-agnostic
- Use proper SQL types that work across database engines
- Include constraints (primary keys, foreign keys, etc.) in schema definitions
- Follow a consistent pattern for timestamps and metadata columns

### Testing

The package includes tests for verifying database operations:

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

Tests use in-memory databases to avoid external dependencies.

### Building

Build the package with:

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Use transactions for operations that need to be atomic
- Close database connections when they're no longer needed
- Consider performance implications of schema designs
- Use appropriate indexing for frequently queried columns
- Keep vector dimensions consistent when using vector search

This package provides a foundation for data persistence in the HAVE SDK, designed to be lightweight but powerful enough for AI-driven applications.
</file>

<file path="packages/sql/docker-compose.yml">
version: "3.1"
services:
  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: sqloo
      POSTGRES_PASSWORD: sqloo
      POSTGRES_DB: sqloo
    ports:
      - "35432:5432"
</file>

<file path="packages/sql/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "composite": true,
    "declaration": true,
    "declarationMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["dist", "node_modules", "**/*.test.ts"]
}
</file>

<file path="packages/utils/src/index.spec.ts">
import { it, expect } from 'vitest';
import { parseAmazonDateString, sleep, waitFor } from './index.js';

it('should have a test', () => {
  expect(true).toBe(true);
});

it.skip('should waitFor "it"', async () => {
  let attempts = 0;
  const result = await waitFor(
    async () => {
      attempts++;
      if (attempts >= 5) {
        return true;
      }
    },
    {
      timeout: 0, // 0 = don't timeout
      delay: 10,
    },
  );
  expect(result).toEqual(true);
});

it.skip('should waitFor "it" only so long', async () => {
  let attempts = 0;
  expect.assertions(1);
  await expect(
    waitFor(
      async () => {
        attempts++;
        await sleep(1000);
        if (attempts >= 10) {
          return true;
        }
      },
      {
        delay: 1000, // should tick 3 times
        timeout: 30000,
      },
    ),
  ).rejects.toEqual('Timed out');
});

it.skip('shoulde able to parse an amazon date', () => {
  const result = parseAmazonDateString('20220223T215409Z');
  expect(result).toBeDefined();
});
</file>

<file path="packages/utils/CLAUDE.md">
# @have/utils: Utility Functions Package

## Purpose and Responsibilities

The `@have/utils` package provides foundational utility functions used throughout the HAVE SDK. It serves as the base dependency for most other packages and offers common functionality for:

- File system path handling
- URL parsing and manipulation
- String manipulation and formatting
- Date handling and parsing
- Data structure transformation
- Unique ID generation

This package is intentionally lightweight with minimal external dependencies and focused on providing pure utility functions.

## Key APIs

### ID and Slug Generation

```typescript
// Generate a UUID
const id = makeId();

// Convert a string to a URL-friendly slug
const slug = makeSlug("My Example Title");
```

### Path Utilities

```typescript
// Get a temporary directory path
const tempPath = tmpdir("my-folder");

// Extract path components from a URL
const urlPathString = urlPath("https://example.com/path/to/resource");

// Get filename from URL
const filename = urlFilename("https://example.com/path/to/file.pdf");
```

### String Manipulation

```typescript
// Convert keys to camelCase
const camelCaseObj = keysToCamel({ some_key: "value" });

// Convert keys to snake_case
const snakeCaseObj = keysToSnake({ someKey: "value" });

// Convert string to camelCase
const camelString = camelCase("some-string-here");

// Convert string to snake_case
const snakeString = snakeCase("someStringHere");
```

### Date Utilities

```typescript
// Parse a date from a string
const date = dateInString("Report_January_2023.pdf");

// Format a date in a pretty format
const formatted = prettyDate("2023-01-15T12:00:00Z");
```

### Async Utilities

```typescript
// Wait for a condition with timeout
await waitFor(
  async () => { /* return something when ready */ },
  { timeout: 5000, delay: 100 }
);

// Sleep for a duration
await sleep(1000);
```

### Type Checking

```typescript
// Check if value is an array
const isArrayVal = isArray(value);

// Check if value is a plain object
const isObjVal = isPlainObject(value);

// Check if string is a URL
const isValidUrl = isUrl("https://example.com");
```

## Dependencies

`@have/utils` has minimal external dependencies:

- `@paralleldrive/cuid2`: For ID generation
- `date-fns`: For date manipulation
- `pluralize`: For singular/plural string transformation
- `uuid`: For UUID generation

## Development Guidelines

### Adding New Utilities

When adding new utility functions:

1. Keep functions pure and focused on a single responsibility
2. Add type definitions for parameters and return values
3. Use descriptive names that indicate function purpose
4. Write unit tests for each new function

### Testing

The package uses Vitest for testing. Run tests with:

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

### Building

Build the package with:

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Keep utility functions stateless when possible
- Prefer functional programming patterns
- Document complex functions with JSDoc comments
- Write utilities to be reusable across different contexts
- Consider performance implications for functions that may be called frequently

This package should remain lightweight and focused on general-purpose utilities that might be needed by multiple other packages.
</file>

<file path="packages/utils/tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "dist",
    "rootDir": "src",
    "composite": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
</file>

<file path="scripts/update-package-versions.cjs">
const fs = require('fs');
const path = require('path');

const rootPackageJson = require('../package.json');
const newVersion = rootPackageJson.version;

const packagesDir = path.join(__dirname, '..', 'packages'); // Assuming packages are in 'packages' dir
const packageDirs = fs
  .readdirSync(packagesDir)
  .filter((dir) => fs.statSync(path.join(packagesDir, dir)).isDirectory());

packageDirs.forEach((packageDirName) => {
  const packageJsonPath = path.join(
    packagesDir,
    packageDirName,
    'package.json',
  );
  if (fs.existsSync(packageJsonPath)) {
    const packageJson = require(packageJsonPath);
    packageJson.version = newVersion;
    fs.writeFileSync(
      packageJsonPath,
      JSON.stringify(packageJson, null, 2) + '\n',
    );
    console.log(`Updated version in ${packageJsonPath} to ${newVersion}`);
  }
});

console.log('Package versions updated successfully.');
</file>

<file path=".nvmrc">
v22
</file>

<file path="bunfig.toml">
# Bun workspace configuration
[install]
# Use exact versions for reproducible builds
exact = true

# Enable workspace linking
cache = true

# Prefer local registry
registry = "https://registry.npmjs.org/"

# Install peer dependencies automatically
auto = true

# Production optimizations
production = false

# Trust configuration
trusted = ["@have/*"]

[install.scopes]
# Workspace packages
"@have" = { registry = "workspace:" }

[run]
# Use system shell for scripts
shell = "system"

# Environment variables
env = { NODE_ENV = "development" }

[test]
# Default test command
preload = []
coverage = false
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file. See [standard-version](https://github.com/conventional-changelog/standard-version) for commit guidelines.

### [0.0.50](https://git.grffn.net/happyvertical/sdk/compare/v0.0.49...v0.0.50) (2025-05-20)


### Features

* more details about assigned in test trigger ([3bc89a6](https://git.grffn.net/happyvertical/sdk/commit/3bc89a6eb0a2a42bde8c6436802878025e52b1bd))

### [0.0.49](https://git.grffn.net/happyvertical/sdk/compare/v0.0.48...v0.0.49) (2025-05-20)


### Features

* a setup_dev script for those who have all the repos in the same parent, eg me ([176c9da](https://git.grffn.net/happyvertical/sdk/commit/176c9daa057cd1237ca931980910ca98d2cf7b80))
* add gitea workflows for Claude agent integration ([a293227](https://git.grffn.net/happyvertical/sdk/commit/a2932276a0fb7de73dc5af86187e77ea1edb827c))
* added prettyDate function to utils ([41b0dfc](https://git.grffn.net/happyvertical/sdk/commit/41b0dfcdd484106105b6274cea2479e2bf6aca02))
* here comes the agentic coding, CLAUDE.md ([0bde29f](https://git.grffn.net/happyvertical/sdk/commit/0bde29f22ac9063fa6c21554088108306574ee98))
* migrated to biome ([b2c611c](https://git.grffn.net/happyvertical/sdk/commit/b2c611c7b8948f0cc0d0703434bb765ea4973429))
* the starts of a contributing guide ripped off from repomix base rules ([c0d7425](https://git.grffn.net/happyvertical/sdk/commit/c0d7425a605107df9707452cd41fc1bde6a0b7d6))
* update git hooks to use lefthook ([ad5261e](https://git.grffn.net/happyvertical/sdk/commit/ad5261ef917c1815c4f08cd56d1d8504acc33f2c))


### Bug Fixes

* setup_dev relative to script and use bin/env ([af3ccd1](https://git.grffn.net/happyvertical/sdk/commit/af3ccd17fbc262f27828f795c147bace54812f36))

### [0.0.48](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.47...v0.0.48) (2025-03-26)


### Features

* **smrt:** added count method to collection ([1feef8d](https://git.grffn.net/happyvertical/have-sdk/commit/1feef8ddaecf1a2ebcfeb97ac950b82d8ce2a90e))


### Bug Fixes

* **svelte:** description in list rendered as html ([83284fb](https://git.grffn.net/happyvertical/have-sdk/commit/83284fbda58d829cd0b73ba09076cdd360375df3))

### [0.0.47](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.46...v0.0.47) (2025-03-12)


### Bug Fixes

* import uses .js ([f52f924](https://git.grffn.net/happyvertical/have-sdk/commit/f52f92491784b8c4bec87376ace4ca752af19e8a))

### [0.0.46](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.45...v0.0.46) (2025-03-12)

### [0.0.45](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.44...v0.0.45) (2025-03-12)


### Features

* added orderBy to collection list ([caa9ded](https://git.grffn.net/happyvertical/have-sdk/commit/caa9ded99ad80bca8a1d3b135e82951dfec3d860))

### [0.0.44](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.43...v0.0.44) (2025-02-26)


### Features

* build:watch script for packages ([2e05566](https://git.grffn.net/happyvertical/have-sdk/commit/2e05566816d1965a16e7d7db68ce34eec9f01260))
* standardised buildWhere for sql queries, comparisons operators managed in object keys ([2b33211](https://git.grffn.net/happyvertical/have-sdk/commit/2b332118764d8731a32f38464c47244fab0f78ae))
* standardized scripts in like packages, added dev script ([c388a34](https://git.grffn.net/happyvertical/have-sdk/commit/c388a344af994f05f51b573cc66ffbd9eda3bb7e))

### [0.0.43](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.42...v0.0.43) (2025-02-22)


### Features

* replace semi-implemented "depreacted" field with "state" ([75a0b4a](https://git.grffn.net/happyvertical/have-sdk/commit/75a0b4ac4a21a1092563d832e241093e0d2f42ff))

### [0.0.42](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.41...v0.0.42) (2025-02-22)


### Bug Fixes

* loadFromSlug context default to blank string ([2c07bf7](https://git.grffn.net/happyvertical/have-sdk/commit/2c07bf702683884e0632a5bce7aaecf0f42eb8b1))

### [0.0.41](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.40...v0.0.41) (2025-02-22)


### Bug Fixes

* missing context vars ([e0c3db8](https://git.grffn.net/happyvertical/have-sdk/commit/e0c3db80d35eba692c7433a6fc7c705bd777b564))

### [0.0.40](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.39...v0.0.40) (2025-02-21)


### Features

* added a general purpose context to go along with the slug ([bbf9ef2](https://git.grffn.net/happyvertical/have-sdk/commit/bbf9ef29f7eb778f547941985237e10037cf90c8))

### [0.0.39](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.38...v0.0.39) (2025-02-19)


### Features

* **smrt:** added toJSON to Content ([897cb4f](https://git.grffn.net/happyvertical/have-sdk/commit/897cb4f431108b6b37ee51fea8b5f74cc4bea755))

### [0.0.38](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.37...v0.0.38) (2025-02-18)


### Bug Fixes

* storybook fixes ([d74d142](https://git.grffn.net/happyvertical/have-sdk/commit/d74d1423b3fb789c429ddc24bfa1815578ee8a1e))

### [0.0.37](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.36...v0.0.37) (2025-02-17)


### Features

* (barely) improve styling of article list and article ([9e6b5be](https://git.grffn.net/happyvertical/have-sdk/commit/9e6b5be456d18a45ae771a1684af3cd1a8217ef6))


### Bug Fixes

* css imports ([4c92e94](https://git.grffn.net/happyvertical/have-sdk/commit/4c92e941f6753e46a6baa9a3e8bf0583f3b551e0))

### [0.0.36](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.35...v0.0.36) (2025-02-17)


### Features

* added status column to content ([7830150](https://git.grffn.net/happyvertical/have-sdk/commit/78301500e2ae8045ac0440df0d89b6f6f5ca2537))

### [0.0.35](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.34...v0.0.35) (2025-02-17)


### Bug Fixes

* collection.get formats data to js ([a3e8dea](https://git.grffn.net/happyvertical/have-sdk/commit/a3e8deab555139eba8b03d8c40f6a9e16da7a4a2))

### [0.0.34](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.33...v0.0.34) (2025-02-16)


### Features

* specify contents directory for mirror function ([868f7c8](https://git.grffn.net/happyvertical/have-sdk/commit/868f7c8f0fa7a5fcaa56ce9a13872930c3da0422))

### [0.0.33](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.32...v0.0.33) (2025-02-15)


### Bug Fixes

* **svelte:** remove test style ([b32b3d5](https://git.grffn.net/happyvertical/have-sdk/commit/b32b3d5f37b6bd9e080b4362ff5f088de6c82ab4))

### [0.0.32](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.31...v0.0.32) (2025-02-15)


### Bug Fixes

* contentToString and stringToContent dont need to be async ([8279dd9](https://git.grffn.net/happyvertical/have-sdk/commit/8279dd92feb7c61652e5191bc26cace16dc2d901))

### [0.0.31](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.30...v0.0.31) (2025-02-15)


### Bug Fixes

* move pg from devDeps to deps in sql ([d27cdca](https://git.grffn.net/happyvertical/have-sdk/commit/d27cdca65880a0f2d22fa984a3ad7a40572c5462))

### [0.0.30](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.29...v0.0.30) (2025-02-15)

### [0.0.29](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.28...v0.0.29) (2025-02-15)


### Bug Fixes

* removed build config ([1dafd38](https://git.grffn.net/happyvertical/have-sdk/commit/1dafd382aa82450c4f07dbf2db81b3dad891a5a5))

### [0.0.28](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.27...v0.0.28) (2025-02-15)


### Features

* added vitest.config to smrt, vitest.workspace to root ([2791894](https://git.grffn.net/happyvertical/have-sdk/commit/279189473509d04054c6c46a0b1f7b8ceaa07ce4))
* ignore .svelte-kit ([8ba87d9](https://git.grffn.net/happyvertical/have-sdk/commit/8ba87d936d9a720f608a635bfb346a484a14cc9a))
* **smrt:** contentToString and stringToContent functions ([3b7b004](https://git.grffn.net/happyvertical/have-sdk/commit/3b7b004cc5261da990778e368ef23d85a5cb7740))

### [0.0.27](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.26...v0.0.27) (2025-02-15)

### [0.0.26](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.25...v0.0.26) (2025-02-15)


### Bug Fixes

* dont verify commit in version bump ([42c27a1](https://git.grffn.net/happyvertical/have-sdk/commit/42c27a1cef4a1ecbb1c9ca487217f8230439269e))

### [0.0.25](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.24...v0.0.25) (2025-02-15)


### Features

* package component exports ([edf178d](https://git.grffn.net/happyvertical/have-sdk/commit/edf178d36020bdd59481c2e3a954fdf78db0afbb))
* **svelte:** a very basic article component intial commit ([d7c279b](https://git.grffn.net/happyvertical/have-sdk/commit/d7c279bee760cb95b01d56dfddcb86cf3e8095a8))

### [0.0.24](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.23...v0.0.24) (2025-02-14)

### [0.0.23](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.22...v0.0.23) (2025-02-14)


### Bug Fixes

* remove the import style that i thought i already had ([32cf26a](https://git.grffn.net/happyvertical/have-sdk/commit/32cf26a3b77374f3c357dabc35a649f37145cbc2))

### [0.0.22](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.21...v0.0.22) (2025-02-14)


### Features

* moved styles to own directory, renamed export styles ([00b9b6a](https://git.grffn.net/happyvertical/have-sdk/commit/00b9b6a67ed005c0e8dde0fef3a976db8f898fc7))


### Bug Fixes

* added clsx and tailwind-merge to deps ([caf1f5b](https://git.grffn.net/happyvertical/have-sdk/commit/caf1f5bc85352fa5e294f211492f116b02dafe2d))
* dont import styles in index.ts ([96eab25](https://git.grffn.net/happyvertical/have-sdk/commit/96eab25070c1395cf18493905b95e703ea457d8b))

### [0.0.21](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.20...v0.0.21) (2025-02-14)


### Features

* **svelte:** export styles ([e3a3c9e](https://git.grffn.net/happyvertical/have-sdk/commit/e3a3c9e632af89fa673e2e3220bcc79de0a3b6c8))

### [0.0.20](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.19...v0.0.20) (2025-02-14)


### Bug Fixes

* more default exports ([c5a5be5](https://git.grffn.net/happyvertical/have-sdk/commit/c5a5be54db6e84c062e71e8c80e4b418c47d8605))

### [0.0.19](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.18...v0.0.19) (2025-02-14)


### Features

* **svelte:** added utils and tailwind-merge dep ([2876a80](https://git.grffn.net/happyvertical/have-sdk/commit/2876a80501cbf18e53215c641847220c9f331134))

### [0.0.18](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.17...v0.0.18) (2025-02-14)


### Bug Fixes

* set custom registry to default and also also in the svelte npmrc ([9908bde](https://git.grffn.net/happyvertical/have-sdk/commit/9908bded4e9f2fa8d3029230ed3071d580d1fe91))

### [0.0.17](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.16...v0.0.17) (2025-02-14)


### Features

* export Card ([38201be](https://git.grffn.net/happyvertical/have-sdk/commit/38201be9a5940e39ab552e8f8a44025fba11b1eb))


### Bug Fixes

* card export ([47f7807](https://git.grffn.net/happyvertical/have-sdk/commit/47f7807f158565566c19ed32ea395fef09697739))

### [0.0.16](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.15...v0.0.16) (2025-02-14)


### Features

* installed eslint, fixed a couple errors, added some rules to skip others and a bunch of warnings ([c3178bc](https://git.grffn.net/happyvertical/have-sdk/commit/c3178bc7ec8433d32b8191d35f4c247f5d5ea441))
* **svelte:** initial commit ([028c884](https://git.grffn.net/happyvertical/have-sdk/commit/028c884ecdf2c0e8f37e4298d30fb711dc2e6268))


### Bug Fixes

* better typing ([92c032d](https://git.grffn.net/happyvertical/have-sdk/commit/92c032dfdffceb66b87a742d3748054a135086fe))
* unignore lib and commit sveltes ([4440ae1](https://git.grffn.net/happyvertical/have-sdk/commit/4440ae12ce72e01854f2680fa15555010b88e759))

### [0.0.15](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.14...v0.0.15) (2025-02-13)


### Bug Fixes

* missed some contentDir ([46f7448](https://git.grffn.net/happyvertical/have-sdk/commit/46f744826ceaae6b36ad1f50eb855cf5c2581da4))

### [0.0.14](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.13...v0.0.14) (2025-02-13)


### Bug Fixes

* **pdf:** add vite config, more pdfs ([386fcc7](https://git.grffn.net/happyvertical/have-sdk/commit/386fcc7a21263cc052aaa419a0647629d25b4464))

### [0.0.13](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.12...v0.0.13) (2025-02-13)

### [0.0.12](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.11...v0.0.12) (2025-02-13)

### [0.0.11](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.10...v0.0.11) (2025-02-13)


### Bug Fixes

* .js on dynamic imports for db adapter ([0e11c6b](https://git.grffn.net/happyvertical/have-sdk/commit/0e11c6be252b834aaf24d9dc43950e66af4227b0))

### [0.0.10](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.9...v0.0.10) (2025-02-13)


### Bug Fixes

* proper extension for pdfjs ([01ffa20](https://git.grffn.net/happyvertical/have-sdk/commit/01ffa20f236cae3b3f9e348b85b843d630f28bc1))

### [0.0.9](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.8...v0.0.9) (2025-02-13)


### Bug Fixes

* couple missed ones ([febcbb1](https://git.grffn.net/happyvertical/have-sdk/commit/febcbb1672cdeff05006c1e314d4fc27b6c43485))

### [0.0.8](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.7...v0.0.8) (2025-02-13)

### [0.0.7](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.6...v0.0.7) (2025-02-13)

### [0.0.6](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.5...v0.0.6) (2025-02-12)

### [0.0.5](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.4...v0.0.5) (2025-02-12)

### [0.0.4](https://git.grffn.net/happyvertical/have-sdk/compare/v0.0.3...v0.0.4) (2025-02-12)

### 0.0.3 (2025-02-12)


### Features

* added publish command to root package.json ([20a6b00](https://git.grffn.net/happyvertical/have-sdk/commit/20a6b00b5ea9c239d71146783eded7090b2c044e))
* **ai:** intial commit ([12b2039](https://git.grffn.net/happyvertical/have-sdk/commit/12b20393b29d6248a5c3749beb6318736474b20f))
* **db:** initial commit, will be renamed from sql to db ([c6e2010](https://git.grffn.net/happyvertical/have-sdk/commit/c6e2010b0ef51af1772db961dc3ffebe49fbe75b))
* **files:** initial commit ([4ae52a9](https://git.grffn.net/happyvertical/have-sdk/commit/4ae52a94f2d91f5abb4ec8af4889a93c1ca44954))
* initial commit ([2d174da](https://git.grffn.net/happyvertical/have-sdk/commit/2d174da8910155b7d969d88a91210d5fba73c195))
* **pdf:** intiaial commit ([1d99717](https://git.grffn.net/happyvertical/have-sdk/commit/1d99717a259a866e1476f20e70f00c2441306883))
* publish-packages script ([3495ef3](https://git.grffn.net/happyvertical/have-sdk/commit/3495ef3064e3b96e0ff30a1600715aa8e3287cde))
* **smrt:** inital comit .. i mean c-o-m-m-i-t ([5251819](https://git.grffn.net/happyvertical/have-sdk/commit/525181921cb55a5b7e4856ac85205a8221a2dcfd))
* **spider:** initial commit ([d23c0c7](https://git.grffn.net/happyvertical/have-sdk/commit/d23c0c73de53735921a472ded710a0f52d91c364))
* **svelte:** initial commit ([384a812](https://git.grffn.net/happyvertical/have-sdk/commit/384a812cdb0843e0f18d1eb783db3847dd71722a))
* typescript happy.. for now ([ed0071e](https://git.grffn.net/happyvertical/have-sdk/commit/ed0071e7d449fce9ed9103d28f22b23a4bc0579b))
* use standard commits to bump version ([c2a789b](https://git.grffn.net/happyvertical/have-sdk/commit/c2a789ba253002aa8d0bb07a51372f6ed10c5925))
* **utils:** initial commit ([3a03ba2](https://git.grffn.net/happyvertical/have-sdk/commit/3a03ba210115ef23f3713bfdbf2ce1c0844aa5a3))


### Bug Fixes

* add auth for package repo ([fe3b7e0](https://git.grffn.net/happyvertical/have-sdk/commit/fe3b7e0d7792ed98e9c87400ea587d03da4da3d3))
* added build, skip verify in commit - should only need one, trying both ([0be2e61](https://git.grffn.net/happyvertical/have-sdk/commit/0be2e61f2d79d5a988a2aad642b00f0b6bea7267))
* added uuid dep ([fea128a](https://git.grffn.net/happyvertical/have-sdk/commit/fea128ad7ba3058947b3755befd713af4ae0fdf5))
* align svelte with base tsconfig compiler options ([fb25953](https://git.grffn.net/happyvertical/have-sdk/commit/fb259537fe5067124dbef21998c1c4d475efd2e8))
* build and typescript fixes for cicd.. i think baseUrl in tsconfig was the head vampire ([62fc552](https://git.grffn.net/happyvertical/have-sdk/commit/62fc552ef528553767bd9f041b6a7a7a5c7d7832))
* config git before release ([55b161b](https://git.grffn.net/happyvertical/have-sdk/commit/55b161be820fd7b29f7c8a16fd159254b99265a9))
* consolidate vite includes to root config ([f02d454](https://git.grffn.net/happyvertical/have-sdk/commit/f02d45495ed28495f711ac684becebdb4999f1a9))
* fetch in exports ([7c753da](https://git.grffn.net/happyvertical/have-sdk/commit/7c753da0754b7c8c77956620764443c3a41d20fc))
* install playwright browsers in cicd ([b0498c6](https://git.grffn.net/happyvertical/have-sdk/commit/b0498c6e7938f3d07f1425badeb07e1e1e048cc3))
* more getTempDir missed ([bb77fba](https://git.grffn.net/happyvertical/have-sdk/commit/bb77fba462d99d8698979a9e245a56d17d6746f0))
* remove packages from deps, exports from files for now ([253b777](https://git.grffn.net/happyvertical/have-sdk/commit/253b777fbf44f45a88608d9297e4eaf7421b3fa5))
* set root to private ([c6d7bb7](https://git.grffn.net/happyvertical/have-sdk/commit/c6d7bb7760e96b3d369926bdd297fef78f07645e))
* setup customer registry sooner ([27dfa34](https://git.grffn.net/happyvertical/have-sdk/commit/27dfa343c288d7914eea54e285a1c14b85f4212d))
* try just no-verify while investigating new build error for svelte ([bf3f5cc](https://git.grffn.net/happyvertical/have-sdk/commit/bf3f5ccef8bb943e5eb940bb9b5c052a23a95ad3))
* verticle -> vertical ([81b8ade](https://git.grffn.net/happyvertical/have-sdk/commit/81b8adec768382abe4170900b621e1cfc74e748d))

### [0.0.2](https://git.grffn.net:2222/happyvertical/have-sdk/compare/v0.0.1...v0.0.2) (2025-02-12)

### 0.0.1 (2025-02-12)


### Features

* added publish command to root package.json ([20a6b00](https://git.grffn.net:2222/happyvertical/have-sdk/commit/20a6b00b5ea9c239d71146783eded7090b2c044e))
* **ai:** intial commit ([12b2039](https://git.grffn.net:2222/happyvertical/have-sdk/commit/12b20393b29d6248a5c3749beb6318736474b20f))
* **db:** initial commit, will be renamed from sql to db ([c6e2010](https://git.grffn.net:2222/happyvertical/have-sdk/commit/c6e2010b0ef51af1772db961dc3ffebe49fbe75b))
* **files:** initial commit ([4ae52a9](https://git.grffn.net:2222/happyvertical/have-sdk/commit/4ae52a94f2d91f5abb4ec8af4889a93c1ca44954))
* initial commit ([2d174da](https://git.grffn.net:2222/happyvertical/have-sdk/commit/2d174da8910155b7d969d88a91210d5fba73c195))
* **pdf:** intiaial commit ([1d99717](https://git.grffn.net:2222/happyvertical/have-sdk/commit/1d99717a259a866e1476f20e70f00c2441306883))
* publish-packages script ([3495ef3](https://git.grffn.net:2222/happyvertical/have-sdk/commit/3495ef3064e3b96e0ff30a1600715aa8e3287cde))
* **smrt:** inital comit .. i mean c-o-m-m-i-t ([5251819](https://git.grffn.net:2222/happyvertical/have-sdk/commit/525181921cb55a5b7e4856ac85205a8221a2dcfd))
* **spider:** initial commit ([d23c0c7](https://git.grffn.net:2222/happyvertical/have-sdk/commit/d23c0c73de53735921a472ded710a0f52d91c364))
* **svelte:** initial commit ([384a812](https://git.grffn.net:2222/happyvertical/have-sdk/commit/384a812cdb0843e0f18d1eb783db3847dd71722a))
* typescript happy.. for now ([ed0071e](https://git.grffn.net:2222/happyvertical/have-sdk/commit/ed0071e7d449fce9ed9103d28f22b23a4bc0579b))
* **utils:** initial commit ([3a03ba2](https://git.grffn.net:2222/happyvertical/have-sdk/commit/3a03ba210115ef23f3713bfdbf2ce1c0844aa5a3))


### Bug Fixes

* add auth for package repo ([fe3b7e0](https://git.grffn.net:2222/happyvertical/have-sdk/commit/fe3b7e0d7792ed98e9c87400ea587d03da4da3d3))
* added build, skip verify in commit - should only need one, trying both ([0be2e61](https://git.grffn.net:2222/happyvertical/have-sdk/commit/0be2e61f2d79d5a988a2aad642b00f0b6bea7267))
* added uuid dep ([fea128a](https://git.grffn.net:2222/happyvertical/have-sdk/commit/fea128ad7ba3058947b3755befd713af4ae0fdf5))
* align svelte with base tsconfig compiler options ([fb25953](https://git.grffn.net:2222/happyvertical/have-sdk/commit/fb259537fe5067124dbef21998c1c4d475efd2e8))
* build and typescript fixes for cicd.. i think baseUrl in tsconfig was the head vampire ([62fc552](https://git.grffn.net:2222/happyvertical/have-sdk/commit/62fc552ef528553767bd9f041b6a7a7a5c7d7832))
* consolidate vite includes to root config ([f02d454](https://git.grffn.net:2222/happyvertical/have-sdk/commit/f02d45495ed28495f711ac684becebdb4999f1a9))
* fetch in exports ([7c753da](https://git.grffn.net:2222/happyvertical/have-sdk/commit/7c753da0754b7c8c77956620764443c3a41d20fc))
* more getTempDir missed ([bb77fba](https://git.grffn.net:2222/happyvertical/have-sdk/commit/bb77fba462d99d8698979a9e245a56d17d6746f0))
* remove packages from deps, exports from files for now ([253b777](https://git.grffn.net:2222/happyvertical/have-sdk/commit/253b777fbf44f45a88608d9297e4eaf7421b3fa5))
* set root to private ([c6d7bb7](https://git.grffn.net:2222/happyvertical/have-sdk/commit/c6d7bb7760e96b3d369926bdd297fef78f07645e))
* setup customer registry sooner ([27dfa34](https://git.grffn.net:2222/happyvertical/have-sdk/commit/27dfa343c288d7914eea54e285a1c14b85f4212d))
* try just no-verify while investigating new build error for svelte ([bf3f5cc](https://git.grffn.net:2222/happyvertical/have-sdk/commit/bf3f5ccef8bb943e5eb940bb9b5c052a23a95ad3))
* verticle -> vertical ([81b8ade](https://git.grffn.net:2222/happyvertical/have-sdk/commit/81b8adec768382abe4170900b621e1cfc74e748d))
</file>

<file path="CONTRIBUTING.md">
# Coding Guidelines
- Follow the Airbnb JavaScript Style Guide.
- Add comments to clarify non-obvious logic. **Ensure all comments are written in English.**
- Provide corresponding unit tests for all new features.
- After implementation, verify changes by running:
  ```bash
  pnpm lint  # Ensure code style compliance
  pnpm test  # Verify all tests pass
  ```

## Commit Messages
- Follow the [Conventional Commits](https://www.conventionalcommits.org/) specification for all commit messages
- Always include a scope in your commit messages
- Format: `type(scope): Description`
- Types: feat, fix, docs, style, refactor, test, chore, etc.
- Scope should indicate the affected part of the codebase (cli, core, website, security, etc.)
- Description should be clear and concise in present tense
- Description must start with a capital letter

## Pull Request Guidelines
- All pull requests must follow the template:
  ```md
  <!-- Please include a summary of the changes -->

  ## Checklist

  - [ ] Run `pnpm test`
  - [ ] Run `pnpm lint`
  ```
- Include a clear summary of the changes at the top of the pull request description
- Reference any related issues using the format `#issue-number` 

## Dependencies and Testing
- Inject dependencies through a deps object parameter for testability
- Example:
  ```typescript
  export const functionName = async (
    param1: Type1,
    param2: Type2,
    deps = {
      defaultFunction1,
      defaultFunction2,
    }
  ) => {
    // Use deps.defaultFunction1() instead of direct call
  };
  ```
- Mock dependencies by passing test doubles through deps object
- Use vi.mock() only when dependency injection is not feasible

## Generate Comprehensive Output
- Include all content without abbreviation, unless specified otherwise
- Optimize for handling large codebases while maintaining output quality
</file>

<file path="LICENSE">
Copyright <2025> <Happy Vertical Corporation>

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
</file>

<file path="pnpm-workspace.yaml">
packages:
  - "packages/*"
</file>

<file path="vitest.setup.ts">
import { promises as fs } from 'fs';
import path from 'path';
import { tmpdir } from 'os';

export const TMP_DIR = path.resolve(`${tmpdir()}/.have-sdk/tests`);

export async function setup() {
  try {
    await fs.mkdir(TMP_DIR, { recursive: true });
    console.log('Test setup complete');
  } catch (error) {
    console.error('Error in test setup', error);
  } finally {
    // cleanup
  }
}
</file>

<file path=".claude/commands/backlog.md">
---
name: backlog
description: Process all issues in "Backlog" status assigned to me
usage: /backlog [notes]
---

# Backlog Lane Command

Processes all issues in the "Backlog" status that are assigned to the current user.

## Usage
```
/backlog
/backlog "all in the same pr"
```

## Description
This command:
1. Finds all issues in "Backlog" status assigned to you
2. Runs `/issue` command on each one
3. Applies Definition of Ready validation
4. Moves ready issues to "To Do" status
5. Adds missing DoR elements for incomplete issues

## Behavior
- Reviews and completes Definition of Ready criteria
- Adds acceptance criteria if missing
- Creates implementation gameplans
- Provides estimates
- Groups related issues for efficient processing

## Notes Parameter
Optional notes guide how to process the issues:
- "all in the same pr" - Plan to implement multiple issues together
- "quick DoR" - Add minimal viable DoR elements
- "detailed planning" - Create comprehensive implementation plans
- "prioritize by effort" - Process smallest issues first

## Example
```
/backlog "group by component"
```
</file>

<file path=".claude/commands/deploy.md">
---
name: deploy
description: Process all issues in "Deploying" status assigned to me
usage: /deploy [notes]
---

# Deploying Lane Command

Processes all issues in the "Deploying" status that are assigned to the current user.

## Usage
```
/deploy
/deploy "merge and deploy"
```

## Description
This command:
1. Finds all issues in "Deploying" status assigned to you
2. Runs `/issue` command on each one
3. Validates deployment readiness
4. Merges approved PRs
5. Moves to "Done" status after successful deployment

## Behavior
- Final validation before merge
- Merges approved PRs
- Monitors deployment process
- Updates status to "Deployed"
- Handles any deployment issues

## Notes Parameter
Optional notes guide how to process the issues:
- "merge and deploy" - Execute the deployment process
- "validate deployment" - Check deployment success
- "rollback if needed" - Monitor for issues requiring rollback
- "batch deploy" - Deploy multiple changes together

## Example
```
/deploy "deploy to production"
```
</file>

<file path=".claude/commands/develop.md">
---
name: develop
description: Process all issues in "Developing" status assigned to me
usage: /develop [notes]
---

# Develop Lane Command

Processes all issues in the "Developing" status that are assigned to the current user.

## Usage
```
/develop
/develop "create PRs when ready"
```

## Description
This command:
1. Finds all issues in "Developing" status assigned to you
2. Runs `/issue` command on each one
3. Checks implementation progress
4. Creates PRs when work is complete
5. Moves to "Quality Assurance" when PR is opened

## Behavior
- Reviews current implementation status
- Checks for completion of acceptance criteria
- Creates PRs with proper linking
- Handles any implementation feedback
- Updates status appropriately

## Notes Parameter
Optional notes guide how to process the issues:
- "create PRs when ready" - Open PRs for completed work
- "review feedback" - Focus on addressing review comments
- "continue implementation" - Push forward with active development
- "check blockers" - Review for any impediments

## Example
```
/develop "finalize and create PRs"
```
</file>

<file path=".claude/commands/fresh.md">
---
name: fresh
description: Process all issues in "Fresh" status assigned to me
usage: /fresh [notes]
---

# Fresh Lane Command

Processes all issues in the "Fresh" status that are assigned to the current user.

## Usage
```
/fresh
/fresh "triage and prioritize"
```

## Description
This command:
1. Finds all issues in "Fresh" status assigned to you
2. Runs `/issue` command on each one
3. Performs initial triage and assessment
4. Moves valid issues to appropriate next status (Backlog/Icebox)
5. Closes or requests clarification for invalid issues

## Behavior
- Reviews issue clarity and completeness
- Checks for duplicates
- Assesses project relevance
- Determines initial priority
- Moves to Backlog (default) or Icebox (low priority)

## Notes Parameter
Optional notes guide how to process the issues:
- "quick triage" - Fast assessment, default to backlog
- "deep review" - Thorough analysis with detailed comments
- "close stale" - Be aggressive about closing unclear issues

## Example
```
/fresh "prioritize security issues"
```
</file>

<file path=".claude/commands/icebox.md">
---
name: icebox
description: Process all issues in "Icebox" status assigned to me
usage: /icebox [notes]
---

# Icebox Lane Command

Processes all issues in the "Icebox" status that are assigned to the current user.

## Usage
```
/icebox
/icebox "review for relevance"
```

## Description
This command:
1. Finds all issues in "Icebox" status assigned to you
2. Runs `/issue` command on each one
3. Reviews continued relevance
4. Either promotes to Backlog, keeps in Icebox, or closes

## Behavior
- Checks if issue is still relevant to project goals
- Reviews for changed priorities
- Assesses if blockers have been resolved
- Updates or closes stale issues

## Notes Parameter
Optional notes guide how to process the issues:
- "promote ready" - Look for issues ready to move to backlog
- "close stale" - Close issues older than 6 months
- "update context" - Add fresh analysis to old issues

## Example
```
/icebox "review Q1 priorities"
```
</file>

<file path=".claude/commands/qa.md">
---
name: qa
description: Process all issues in "Quality Assurance" status assigned to me
usage: /qa [notes]
---

# Quality Assurance Lane Command

Processes all issues in the "Quality Assurance" status that are assigned to the current user.

## Usage
```
/qa
/qa "address feedback"
```

## Description
This command:
1. Finds all issues in "Quality Assurance" status assigned to you
2. Runs `/issue` command on each one
3. Checks PR review status and CI results
4. Addresses feedback or moves to next stage
5. Updates to "Ready for Deployment" when approved

## Behavior
- Reviews PR status and feedback
- Implements requested changes
- Validates CI/CD pipeline status
- Ensures all review requirements met
- Moves to next stage when ready

## Notes Parameter
Optional notes guide how to process the issues:
- "address feedback" - Focus on implementing review comments
- "merge when ready" - Advance approved PRs
- "check CI status" - Review failing tests or builds
- "ping reviewers" - Follow up on pending reviews

## Example
```
/qa "implement review suggestions"
```
</file>

<file path=".claude/commands/todo.md">
---
name: todo
description: Process all issues in "To Do" status assigned to me
usage: /todo [notes]
---

# To Do Lane Command

Processes all issues in the "To Do" status that are assigned to the current user.

## Usage
```
/todo
/todo "start highest priority"
```

## Description
This command:
1. Finds all issues in "To Do" status assigned to you
2. Runs `/issue` command on each one
3. Validates Definition of Ready compliance
4. Creates feature branches and starts implementation
5. Moves issues to "Developing" status

## Behavior
- Final DoR validation before development
- Creates appropriately named feature branches
- Validates git state before branch creation
- Begins implementation following gameplan
- Updates status to "Developing"

## Notes Parameter
Optional notes guide how to process the issues:
- "start highest priority" - Begin with most important issue
- "smallest first" - Start with quickest wins
- "related issues together" - Group similar work
- "single branch" - Implement multiple issues in one branch

## Example
```
/todo "start bug fixes first"
```
</file>

<file path=".gitea/workflows/agents/claude/on-assigned-pr.yaml">
name: Process PR Review on Assignment

on:
  pull_request:
    types: [assigned]

jobs:
  process-pr-on-assignment:
    runs-on: ubuntu-latest
    # Only run when the PR is assigned to the designated bot user
    if: gitea.event.pr.assignee.login == 'claude'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Setup Tea CLI and get PR details
        id: pr
        run: |
          # Install gitea-cli if needed
          if ! command -v tea &> /dev/null; then
            curl -sL https://dl.gitea.io/tea/latest/linux/amd64/tea -o tea
            chmod +x tea
            mv tea /usr/local/bin/
          fi
          
          # Configure tea client
          tea login add -u ${{ vars.GITEA_URL }} --token ${{ secrets.GITEA_TOKEN }}
          
          # Get PR details
          REPO_NAME="${{ gitea.repository }}"
          PR_NUM="${{ gitea.event.pull_request.number }}"
          
          # Get branch name
          PR_BRANCH=$(tea pulls view "$REPO_NAME#$PR_NUM" --format '{{ .Head.Ref }}')
          echo "branch=$PR_BRANCH" >> $GITHUB_OUTPUT
          
          # Get PR comments
          PR_COMMENTS=$(tea pulls comments "$REPO_NAME#$PR_NUM" --format '{{ range . }}{{ .Body }}{{ "\n\n" }}{{ end }}')
          echo "comments<<EOF" >> $GITHUB_OUTPUT
          echo "$PR_COMMENTS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          # Get PR review comments
          PR_REVIEW_COMMENTS=$(tea pulls reviews "$REPO_NAME#$PR_NUM" --format '{{ range . }}{{ range .Comments }}{{ .Body }}{{ "\n\n" }}{{ end }}{{ end }}')
          echo "review_comments<<EOF" >> $GITHUB_OUTPUT
          echo "$PR_REVIEW_COMMENTS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
      
      - name: Install Claude Code CLI
        run: npm install -g @anthropic-ai/claude-code
      
      - name: Checkout PR branch
        run: |
          git checkout ${{ steps.pr.outputs.branch }}
      
      - name: Process PR comments with Claude Code
        run: |
          ALL_COMMENTS="${{ steps.pr.outputs.review_comments }}
          ${{ steps.pr.outputs.comments }}"
          # Pass the comments to Claude Code to process and make changes
          claude --print "Review the following PR feedback and make the requested changes: $ALL_COMMENTS"
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      
      - name: Commit and push changes
        run: |
          git config user.name "Gitea Actions Bot"
          git config user.email "actions-bot@example.com"
          git add .
          git commit -m "Address PR review comments triggered by assignment" || echo "No changes to commit"
          git push origin ${{ steps.pr.outputs.branch }}
      
      - name: Add response comment to PR and unassign
        run: |
          REPO_NAME="${{ gitea.repository }}"
          PR_NUM="${{ gitea.event.pull_request.number }}"
          
          # Add comment to PR
          tea pulls comment "$REPO_NAME#$PR_NUM" --body "I've processed all review comments and made the requested changes. Please review the new commits."
          
          # Unassign the bot to indicate work is complete
          tea pulls edit "$REPO_NAME#$PR_NUM" --unassign ${{ gitea.event.assignee.login }}
</file>

<file path=".gitea/workflows/agents/claude/test-issue-trigger.yaml">
name: Test Issue Assignment Trigger
on:
  issues:
    types: [labeled, assigned] # Add 'assigned' to test that trigger too, or keep only 'labeled'
jobs:
  test_job:
    runs-on: ubuntu-latest
    steps:
      - name: Show Event Details
        run: |
          echo "Workflow triggered by event: ${{ gitea.event_name }}"
          echo "Action type: ${{ gitea.event.action }}"
          echo "Issue Number: ${{ gitea.event.issue.number }}"
          echo "Issue Title: ${{ gitea.event.issue.title }}"
          echo "Sender (user performing the action): ${{ gitea.event.sender.login }}"

      - name: Show Label Details (if action is 'labeled')
        if: gitea.event.action == 'labeled'
        run: |
          echo "Label Name: ${{ gitea.event.label.name }}"
          echo "Label Color: ${{ gitea.event.label.color }}"

      - name: Show Assignee Information
        run: |
          echo "Issue's Current Assignee (from issue object): ${{ gitea.event.issue.assignee.login }}"
          echo "Full Issue Assignee Object (from issue object): ${{ toJson(gitea.event.issue.assignee) }}"
          echo "Direct Event Assignee (populated for 'assigned' action): ${{ gitea.event.assignee.login }}"
          echo "Full Direct Event Assignee Object (for 'assigned' action): ${{ toJson(gitea.event.assignee) }}"
</file>

<file path=".github/scripts/validate-security.sh">
#!/bin/bash

# Security validation script for GitHub Actions workflows
# This script validates that required secrets are configured and performs basic security checks

set -e

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to log with colors
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to validate required secrets
validate_secrets() {
    local secrets=("$@")
    local missing_secrets=()
    
    log_info "Validating required secrets..."
    
    for secret in "${secrets[@]}"; do
        if [ -z "${!secret}" ]; then
            missing_secrets+=("$secret")
        fi
    done
    
    if [ ${#missing_secrets[@]} -gt 0 ]; then
        log_error "Missing required secrets: ${missing_secrets[*]}"
        return 1
    fi
    
    log_info "All required secrets are configured"
    return 0
}

# Function to validate GitHub token permissions
validate_github_token() {
    if [ -z "$GITHUB_TOKEN" ]; then
        log_error "GITHUB_TOKEN is not set"
        return 1
    fi
    
    log_info "Validating GitHub token permissions..."
    
    # Test basic GitHub API access
    if ! gh auth status > /dev/null 2>&1; then
        log_error "GitHub token authentication failed"
        return 1
    fi
    
    log_info "GitHub token authentication successful"
    return 0
}

# Function to validate action versions
validate_action_versions() {
    local workflow_file="$1"
    
    if [ ! -f "$workflow_file" ]; then
        log_error "Workflow file not found: $workflow_file"
        return 1
    fi
    
    log_info "Validating action versions in $workflow_file..."
    
    # Check for unpinned versions
    if grep -q "@beta\|@latest\|@main\|@master" "$workflow_file"; then
        log_warn "Found unpinned action versions in $workflow_file"
        grep -n "@beta\|@latest\|@main\|@master" "$workflow_file" || true
    fi
    
    log_info "Action version validation complete"
    return 0
}

# Main validation function
main() {
    local required_secrets=()
    local workflow_files=()
    
    # Parse command line arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            --secrets)
                shift
                while [[ $# -gt 0 && ! "$1" =~ ^-- ]]; do
                    required_secrets+=("$1")
                    shift
                done
                ;;
            --workflows)
                shift
                while [[ $# -gt 0 && ! "$1" =~ ^-- ]]; do
                    workflow_files+=("$1")
                    shift
                done
                ;;
            --help)
                echo "Usage: $0 [OPTIONS]"
                echo ""
                echo "Options:"
                echo "  --secrets SECRET1 SECRET2 ...    List of required secrets to validate"
                echo "  --workflows FILE1 FILE2 ...      List of workflow files to validate"
                echo "  --help                           Show this help message"
                exit 0
                ;;
            *)
                log_error "Unknown option: $1"
                exit 1
                ;;
        esac
    done
    
    local validation_failed=false
    
    # Validate secrets if provided
    if [ ${#required_secrets[@]} -gt 0 ]; then
        if ! validate_secrets "${required_secrets[@]}"; then
            validation_failed=true
        fi
    fi
    
    # Validate GitHub token if GITHUB_TOKEN is set
    if [ -n "$GITHUB_TOKEN" ]; then
        if ! validate_github_token; then
            validation_failed=true
        fi
    fi
    
    # Validate workflow files if provided
    for workflow_file in "${workflow_files[@]}"; do
        if ! validate_action_versions "$workflow_file"; then
            validation_failed=true
        fi
    done
    
    if [ "$validation_failed" = true ]; then
        log_error "Security validation failed"
        exit 1
    fi
    
    log_info "Security validation passed"
    exit 0
}

# Run main function with all arguments
main "$@"
</file>

<file path=".github/config-dependabot.yml">
version: 2
updates:
  # Enable version updates for npm/pnpm
  - package-ecosystem: "npm"
    # Look for `package.json` and `lock` files in the root directory
    directory: "/"
    # Check for updates once a week
    schedule:
      interval: "weekly"
    # Allow up to 10 open pull requests at a time
    open-pull-requests-limit: 10
    # Set a version update strategy
    versioning-strategy: increase
    # Specify package manager
    package-manager: "pnpm"
    # Groups to reduce PR noise
    groups:
      # Group all dev dependencies together
      dev-dependencies:
        patterns:
          - "@biomejs/*"
          - "typescript"
          - "vitest"
          - "lefthook"
          - "standard-version"
          - "@changesets/*"
          - "conventional-changelog-cli"
    
  # Also check for updates in each package directory
  - package-ecosystem: "npm"
    directory: "/packages/utils"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/files"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/spider"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/sql"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/pdf"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/ai"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/smrt"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  - package-ecosystem: "npm"
    directory: "/packages/svelte"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
    versioning-strategy: increase
    package-manager: "pnpm"
    
  # Enable version updates for GitHub Actions
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 5
</file>

<file path=".github/SECURITY.md">
# GitHub Actions Security Configuration

This document explains the security configurations implemented in the GitHub Actions workflows.

## Security Improvements Implemented

### 1. Action Version Pinning

**Problem**: Using unpinned action versions like `@beta`, `@v5`, `@latest` creates supply chain security risks.

**Solution**: All actions are now pinned to specific versions:
- `actions/checkout@v4` - Consistent across all workflows
- `actions/setup-node@v4` - Latest stable version
- `actions/cache@v4` - Latest stable version
- `anthropics/claude-code-action@v1.0.0` - Pinned to stable release
- `alstr/todo-to-issue-action@v5.0.0` - Pinned to specific version
- `dependabot/fetch-metadata@v2.1.0` - Pinned to specific version

### 2. Secret Validation

**Problem**: Workflows would fail silently or with unclear errors if required secrets were missing.

**Solution**: Added validation steps to check for required secrets before proceeding:

```yaml
- name: Validate required secrets
  run: |
    if [ -z "${{ secrets.ANTHROPIC_API_KEY }}" ]; then
      echo "Error: ANTHROPIC_API_KEY secret is not configured"
      exit 1
    fi
    echo "All required secrets are configured"
```

### 3. Secure Token Handling

**Problem**: Writing authentication tokens to files creates security risks.

**Before**:
```yaml
echo "//npm.pkg.github.com/:_authToken=${{ secrets.GITHUB_TOKEN }}" >> .npmrc
```

**After**:
```yaml
echo "@have:registry=https://npm.pkg.github.com" > .npmrc
# Token is passed via environment variables instead
env:
  NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  NPM_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

### 4. Permission Documentation

**Problem**: Unclear why specific permissions are required.

**Solution**: Added clear documentation for each permission:

```yaml
permissions:
  contents: write       # Required for creating git tags/releases and pushing commits
  packages: write       # Required for publishing packages to GitHub Packages
  pages: write          # Required for GitHub Pages deployment (when enabled)
  id-token: write       # Required for GitHub Pages deployment (when enabled)
```

### 5. Enhanced Dependabot Security

**Problem**: Dependabot automatically approved and merged all non-major updates without additional security checks.

**Solution**: 
- Added security validation steps
- Added manual review requirement for major updates
- Enhanced logging for dependency update metadata
- Added security validation comments on PRs

### 6. Reusable Security Validation Script

**Location**: `.github/scripts/validate-security.sh`

**Purpose**: Provides centralized security validation that can be reused across workflows.

**Features**:
- Secret validation
- GitHub token permission validation
- Action version validation
- Colored output for better visibility

**Usage**:
```bash
# Validate specific secrets
./validate-security.sh --secrets GITHUB_TOKEN ANTHROPIC_API_KEY

# Validate workflow files
./validate-security.sh --workflows .github/workflows/claude.yaml

# Combined validation
./validate-security.sh --secrets GITHUB_TOKEN --workflows .github/workflows/claude.yaml
```

## Security Checklist

When adding new workflows, ensure:

- [ ] All actions are pinned to specific versions (no @latest, @beta, @main)
- [ ] Required secrets are validated before use
- [ ] Permissions are documented with clear justification
- [ ] Sensitive data is not written to files
- [ ] Security validation script is used where appropriate
- [ ] Manual review is required for high-impact changes

## Secrets Configuration

The following secrets must be configured in the repository:

| Secret Name | Purpose | Required For |
|-------------|---------|--------------|
| `GITHUB_TOKEN` | GitHub API access (auto-provided) | All workflows |
| `ANTHROPIC_API_KEY` | Claude AI integration | claude.yaml |

## Monitoring and Maintenance

1. **Regular Updates**: Review and update pinned action versions quarterly
2. **Security Audits**: Run security validation script before major releases
3. **Permission Reviews**: Audit workflow permissions annually
4. **Secret Rotation**: Rotate secrets according to security policy

## Incident Response

If a security issue is discovered:

1. **Immediate**: Disable affected workflows
2. **Assessment**: Evaluate scope and impact
3. **Remediation**: Apply fixes using this security framework
4. **Validation**: Run security validation script
5. **Documentation**: Update this document with lessons learned
</file>

<file path=".husky/_/pre-commit">
#!/bin/sh

if [ "$LEFTHOOK_VERBOSE" = "1" -o "$LEFTHOOK_VERBOSE" = "true" ]; then
  set -x
fi

if [ "$LEFTHOOK" = "0" ]; then
  exit 0
fi

call_lefthook()
{
  if test -n "$LEFTHOOK_BIN"
  then
    "$LEFTHOOK_BIN" "$@"
  elif lefthook -h >/dev/null 2>&1
  then
    lefthook "$@"
  else
    dir="$(git rev-parse --show-toplevel)"
    osArch=$(uname | tr '[:upper:]' '[:lower:]')
    cpuArch=$(uname -m | sed 's/aarch64/arm64/;s/x86_64/x64/')
    if test -f "$dir/node_modules/lefthook-${osArch}-${cpuArch}/bin/lefthook"
    then
      "$dir/node_modules/lefthook-${osArch}-${cpuArch}/bin/lefthook" "$@"
    elif test -f "$dir/node_modules/@evilmartians/lefthook/bin/lefthook-${osArch}-${cpuArch}/lefthook"
    then
      "$dir/node_modules/@evilmartians/lefthook/bin/lefthook-${osArch}-${cpuArch}/lefthook" "$@"
    elif test -f "$dir/node_modules/@evilmartians/lefthook-installer/bin/lefthook"
    then
      "$dir/node_modules/@evilmartians/lefthook-installer/bin/lefthook" "$@"
    elif test -f "$dir/node_modules/lefthook/bin/index.js"
    then
      "$dir/node_modules/lefthook/bin/index.js" "$@"
    
    elif go tool lefthook -h >/dev/null 2>&1
    then
      go tool lefthook "$@"
    elif bundle exec lefthook -h >/dev/null 2>&1
    then
      bundle exec lefthook "$@"
    elif yarn lefthook -h >/dev/null 2>&1
    then
      yarn lefthook "$@"
    elif pnpm lefthook -h >/dev/null 2>&1
    then
      pnpm lefthook "$@"
    elif swift package lefthook >/dev/null 2>&1
    then
      swift package --build-path .build/lefthook --disable-sandbox lefthook "$@"
    elif command -v mint >/dev/null 2>&1
    then
      mint run csjones/lefthook-plugin "$@"
    elif uv run lefthook -h >/dev/null 2>&1
    then
      uv run lefthook "$@"
    elif mise exec -- lefthook -h >/dev/null 2>&1
    then
      mise exec -- lefthook "$@"
    elif devbox run lefthook -h >/dev/null 2>&1
    then
      devbox run lefthook "$@"
    else
      echo "Can't find lefthook in PATH"
    fi
  fi
}

call_lefthook run "pre-commit" "$@"
</file>

<file path=".husky/_/prepare-commit-msg">
#!/bin/sh

if [ "$LEFTHOOK_VERBOSE" = "1" -o "$LEFTHOOK_VERBOSE" = "true" ]; then
  set -x
fi

if [ "$LEFTHOOK" = "0" ]; then
  exit 0
fi

call_lefthook()
{
  if test -n "$LEFTHOOK_BIN"
  then
    "$LEFTHOOK_BIN" "$@"
  elif lefthook -h >/dev/null 2>&1
  then
    lefthook "$@"
  else
    dir="$(git rev-parse --show-toplevel)"
    osArch=$(uname | tr '[:upper:]' '[:lower:]')
    cpuArch=$(uname -m | sed 's/aarch64/arm64/;s/x86_64/x64/')
    if test -f "$dir/node_modules/lefthook-${osArch}-${cpuArch}/bin/lefthook"
    then
      "$dir/node_modules/lefthook-${osArch}-${cpuArch}/bin/lefthook" "$@"
    elif test -f "$dir/node_modules/@evilmartians/lefthook/bin/lefthook-${osArch}-${cpuArch}/lefthook"
    then
      "$dir/node_modules/@evilmartians/lefthook/bin/lefthook-${osArch}-${cpuArch}/lefthook" "$@"
    elif test -f "$dir/node_modules/@evilmartians/lefthook-installer/bin/lefthook"
    then
      "$dir/node_modules/@evilmartians/lefthook-installer/bin/lefthook" "$@"
    elif test -f "$dir/node_modules/lefthook/bin/index.js"
    then
      "$dir/node_modules/lefthook/bin/index.js" "$@"
    
    elif go tool lefthook -h >/dev/null 2>&1
    then
      go tool lefthook "$@"
    elif bundle exec lefthook -h >/dev/null 2>&1
    then
      bundle exec lefthook "$@"
    elif yarn lefthook -h >/dev/null 2>&1
    then
      yarn lefthook "$@"
    elif pnpm lefthook -h >/dev/null 2>&1
    then
      pnpm lefthook "$@"
    elif swift package lefthook >/dev/null 2>&1
    then
      swift package --build-path .build/lefthook --disable-sandbox lefthook "$@"
    elif command -v mint >/dev/null 2>&1
    then
      mint run csjones/lefthook-plugin "$@"
    elif uv run lefthook -h >/dev/null 2>&1
    then
      uv run lefthook "$@"
    elif mise exec -- lefthook -h >/dev/null 2>&1
    then
      mise exec -- lefthook "$@"
    elif devbox run lefthook -h >/dev/null 2>&1
    then
      devbox run lefthook "$@"
    else
      echo "Can't find lefthook in PATH"
    fi
  fi
}

call_lefthook run "prepare-commit-msg" "$@"
</file>

<file path="docs/adr/0001-use-typescript-for-all-packages.md">
# ADR-0001: Use TypeScript for All Packages

## Status

- **Status**: Accepted
- **Date**: 2024-01-01
- **Authors**: HAppy VErtical Development Team
- **Reviewers**: Technical Leadership

## Context

### Background
The HAppy VErtical SDK is a monorepo designed for building vertical AI agents. As the project grows, we need to establish a consistent programming language and type system across all packages to ensure maintainability, developer experience, and code quality.

### Constraints
- Need to support both CommonJS and ESM modules
- Must maintain compatibility with Node.js runtime
- Development team has strong TypeScript experience
- Build process must be efficient for monorepo structure

### Assumptions
- TypeScript will continue to be actively maintained
- The benefits of static typing outweigh the compilation overhead
- Team members are proficient in TypeScript development

## Decision

We will use TypeScript exclusively for all packages in the HAppy VErtical SDK monorepo, with a pure TypeScript implementation to avoid CommonJS vs ESM compatibility issues.

## Rationale

### Options Considered

1. **Pure TypeScript**: All packages written in TypeScript
2. **Mixed JavaScript/TypeScript**: Some packages in JS, some in TS
3. **Pure JavaScript**: All packages in JavaScript with JSDoc typing

### Analysis

#### Option 1: Pure TypeScript
**Pros:**
- Strong static typing improves code quality and catches errors early
- Excellent IDE support with autocompletion and refactoring
- Better maintainability for complex codebases
- Consistent developer experience across all packages
- Built-in support for modern JavaScript features

**Cons:**
- Compilation step adds build complexity
- Potential learning curve for JavaScript-only developers
- Additional tooling configuration required

**Trade-offs:**
- Compilation overhead vs. runtime error prevention
- Initial setup complexity vs. long-term maintainability

#### Option 2: Mixed JavaScript/TypeScript
**Pros:**
- Flexibility to choose appropriate tool for each package
- Gradual migration path possible

**Cons:**
- Inconsistent developer experience
- Complex build configuration
- Potential type safety gaps at package boundaries
- Increased maintenance burden

**Trade-offs:**
- Flexibility vs. consistency and simplicity

#### Option 3: Pure JavaScript
**Pros:**
- No compilation step required
- Simpler build process
- Direct execution in Node.js

**Cons:**
- Lack of static typing leads to runtime errors
- Limited IDE support for large codebases
- JSDoc typing is less robust than TypeScript
- Harder to maintain as codebase grows

**Trade-offs:**
- Simplicity vs. type safety and tooling support

### Decision Criteria
- **Type Safety**: Ability to catch errors at compile time
- **Developer Experience**: IDE support, autocompletion, refactoring
- **Maintainability**: Long-term code quality and team productivity
- **Consistency**: Uniform approach across all packages
- **Ecosystem**: Compatibility with Node.js and npm ecosystem

### Selected Option
We selected **Pure TypeScript** because:
- Static typing significantly improves code quality for a complex monorepo
- Consistent developer experience across all packages
- Excellent tooling support enhances productivity
- The compilation overhead is acceptable given the benefits
- TypeScript's ESM support aligns with our module strategy

## Consequences

### Positive Consequences
- Improved code quality through static type checking
- Better developer experience with IDE support
- Reduced runtime errors and debugging time
- Consistent codebase structure across all packages
- Enhanced refactoring capabilities
- Better documentation through type annotations

### Negative Consequences
- Additional build step required for all packages
- Potential learning curve for JavaScript-only developers
- Increased complexity in build configuration
- TypeScript compiler dependency

### Neutral Consequences
- Need to establish TypeScript configuration standards
- Must maintain tsconfig.json files for each package
- Documentation must reflect TypeScript usage patterns

## Implementation

### Action Items
- [x] Create root tsconfig.json with shared configuration
- [x] Configure individual package tsconfig.json files
- [x] Set up build process for TypeScript compilation
- [x] Establish linting rules for TypeScript
- [x] Update documentation to reflect TypeScript usage
- [x] Configure IDE settings for TypeScript development

### Timeline
- **Phase 1**: Initial TypeScript configuration and build setup
- **Phase 2**: Package-by-package TypeScript adoption
- **Phase 3**: Documentation updates and team training

### Success Metrics
- **Code Quality**: Reduction in runtime type errors
- **Developer Productivity**: Faster development cycles with better IDE support
- **Maintainability**: Easier refactoring and code navigation
- **Build Performance**: Acceptable compilation times for development workflow

### Risks and Mitigation

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|---------|-------------------|
| Build performance issues | Medium | Medium | Optimize tsconfig, use incremental compilation |
| Team learning curve | Low | Low | Provide TypeScript training and documentation |
| Configuration complexity | Medium | Low | Standardize tsconfig patterns, document best practices |

## References

### Related ADRs
- None (first ADR)

### External References
- [TypeScript Documentation](https://www.typescriptlang.org/)
- [Node.js TypeScript Support](https://nodejs.org/api/esm.html#typescript)
- [Monorepo TypeScript Configuration](https://www.typescriptlang.org/docs/handbook/project-references.html)

### Code References
- `tsconfig.json` - Root TypeScript configuration
- `packages/*/tsconfig.json` - Package-specific configurations
- `packages/utils/src/types.ts` - Shared type definitions

---

## Notes

This decision establishes TypeScript as the foundation for all future development in the HAppy VErtical SDK. All new packages must use TypeScript, and existing JavaScript code should be migrated to TypeScript as part of regular maintenance.

---

**ADR Template Version**: 1.0  
**Created**: 2024-01-01  
**Last Updated**: 2024-01-01
</file>

<file path="docs/adr/README.md">
# Architecture Decision Records (ADR)

This directory contains Architecture Decision Records (ADRs) for the HAppy VErtical SDK project.

## What is an ADR?

An Architecture Decision Record (ADR) is a document that captures an important architectural decision made along with its context and consequences. ADRs help teams:

- Track the reasoning behind architectural choices
- Understand the trade-offs considered
- Provide historical context for future decisions
- Facilitate knowledge sharing across the team
- Support onboarding of new team members

## When to Create an ADR

Create an ADR when making decisions that:

- **Introduce new dependencies** (libraries, frameworks, services)
- **Change system architecture** (monolith to microservices, database changes)
- **Affect multiple packages** in the monorepo
- **Impact performance, security, or scalability**
- **Establish coding standards or conventions**
- **Choose between multiple technical approaches**
- **Retire or replace existing technologies**

## ADR Process

1. **Identify the Decision**: Recognize when an architectural decision needs to be made
2. **Research Options**: Investigate different approaches and their trade-offs
3. **Create ADR**: Use the provided template to document the decision
4. **Review**: Get feedback from team members before finalizing
5. **Implement**: Proceed with the implementation
6. **Update**: Modify the ADR if circumstances change

## ADR Naming Convention

ADRs should be named with the format:
```
NNNN-title-of-decision.md
```

Where:
- `NNNN` is a 4-digit sequential number (0001, 0002, etc.)
- `title-of-decision` is a short, descriptive title in kebab-case

Examples:
- `0001-use-typescript-for-all-packages.md`
- `0002-adopt-biome-for-linting-and-formatting.md`
- `0003-implement-monorepo-with-pnpm-workspaces.md`

## Directory Structure

```
docs/adr/
├── README.md           # This file
├── template.md         # ADR template
├── 0001-example.md     # First ADR
├── 0002-example.md     # Second ADR
└── ...
```

## Review Process

Before merging an ADR:

1. **Technical Review**: Ensure the technical analysis is sound
2. **Stakeholder Input**: Get feedback from affected team members
3. **Documentation Review**: Verify the ADR follows the template
4. **Impact Assessment**: Consider consequences and alternatives
5. **Approval**: Get sign-off from technical leads

## Updating ADRs

ADRs represent point-in-time decisions. If circumstances change:

1. **Don't modify the original ADR** - it represents historical context
2. **Create a new ADR** that supersedes the previous one
3. **Reference the original ADR** in the new one
4. **Update the status** of the original ADR to "Superseded"

## Integration with Definition of Done

Per the [Definition of Done](../workflow/DEFINITION_OF_DONE.md), an ADR must be created when:

> An Architecture Decision Record (ADR) has been created in the `/docs/adr` directory if the change introduces a new dependency or makes a significant architectural decision.

This ensures architectural decisions are properly documented and reviewed as part of the development process.

## Template

Use the [ADR template](template.md) to create new ADRs. The template provides a consistent structure for documenting decisions.

## Resources

- [ADR Template](template.md) - Template for creating new ADRs
- [Definition of Done](../workflow/DEFINITION_OF_DONE.md) - When ADRs are required
- [Kanban Workflow](../workflow/KANBAN.md) - Overall development process
</file>

<file path="docs/adr/template.md">
# ADR-NNNN: [Title of Decision]

## Status

- **Status**: [Proposed | Accepted | Rejected | Superseded]
- **Date**: [YYYY-MM-DD]
- **Authors**: [Author Name(s)]
- **Reviewers**: [Reviewer Name(s)]

## Context

### Background
[Describe the current situation and the forces that prompted this decision. What is the problem or opportunity that needs to be addressed?]

### Constraints
[List any constraints that limit the solution space, such as time, budget, technology, regulatory requirements, etc.]

### Assumptions
[Document any assumptions made during the decision process that could affect the outcome.]

## Decision

[State the architectural decision that was made. Be clear and concise about what was decided.]

## Rationale

### Options Considered
[List the alternatives that were evaluated. For each option, provide a brief description.]

1. **Option 1**: [Description]
2. **Option 2**: [Description]
3. **Option 3**: [Description]

### Analysis
[Provide detailed analysis of each option, including pros, cons, and trade-offs.]

#### Option 1: [Name]
**Pros:**
- [Advantage 1]
- [Advantage 2]

**Cons:**
- [Disadvantage 1]
- [Disadvantage 2]

**Trade-offs:**
- [Trade-off consideration]

#### Option 2: [Name]
**Pros:**
- [Advantage 1]
- [Advantage 2]

**Cons:**
- [Disadvantage 1]
- [Disadvantage 2]

**Trade-offs:**
- [Trade-off consideration]

### Decision Criteria
[Explain the criteria used to evaluate the options, such as performance, maintainability, cost, etc.]

### Selected Option
[Explain why the selected option was chosen over the alternatives.]

## Consequences

### Positive Consequences
[List the benefits and positive outcomes expected from this decision.]

- [Benefit 1]
- [Benefit 2]

### Negative Consequences
[List the drawbacks and negative outcomes expected from this decision.]

- [Drawback 1]
- [Drawback 2]

### Neutral Consequences
[List other outcomes that are neither clearly positive nor negative.]

- [Neutral outcome 1]
- [Neutral outcome 2]

## Implementation

### Action Items
[List the specific actions needed to implement this decision.]

- [ ] [Action item 1]
- [ ] [Action item 2]
- [ ] [Action item 3]

### Timeline
[Provide an estimated timeline for implementation.]

- **Phase 1**: [Description and timeline]
- **Phase 2**: [Description and timeline]

### Success Metrics
[Define how success will be measured.]

- [Metric 1]: [Target value]
- [Metric 2]: [Target value]

### Risks and Mitigation
[Identify potential risks and how they will be mitigated.]

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|---------|-------------------|
| [Risk 1] | [Low/Medium/High] | [Low/Medium/High] | [Strategy] |
| [Risk 2] | [Low/Medium/High] | [Low/Medium/High] | [Strategy] |

## References

### Related ADRs
[List any related ADRs that influenced this decision or are affected by it.]

- [ADR-XXXX: Related Decision Title](XXXX-related-decision.md)

### External References
[Include links to external resources, documentation, or research that influenced this decision.]

- [Resource 1](https://example.com)
- [Resource 2](https://example.com)

### Code References
[Reference specific files, packages, or code sections affected by this decision.]

- `packages/example/src/component.ts`
- `docs/workflow/DEFINITION_OF_DONE.md`

---

## Notes

[Any additional notes or considerations that don't fit in the above sections.]

---

**ADR Template Version**: 1.0  
**Created**: [Date]  
**Last Updated**: [Date]
</file>

<file path="docs/workflow/README.md">
# HAppy VErtical Workflow Standards

This directory contains the organization-wide workflow standards for all HAppy VErtical projects. These documents serve as the single source of truth for development processes across the organization.

## Documents

- **[Definition of Ready](./DEFINITION_OF_READY.md)** - Criteria that must be met before work can begin on an issue
- **[Definition of Done](./DEFINITION_OF_DONE.md)** - Checklist for Pull Request completion
- **[Kanban Process](./KANBAN.md)** - Kanban CI/CD workflow with lanes and automation

## How to Reference in Other Repositories

### Option 1: Direct Links (Recommended)
Reference these documents directly from other repositories using GitHub URLs:

```markdown
# In your project's README or CONTRIBUTING.md

## Development Process

We follow the HAppy VErtical organization workflow standards:

- [Definition of Ready](https://github.com/happyvertical/sdk/blob/main/docs/workflow/DEFINITION_OF_READY.md)
- [Definition of Done](https://github.com/happyvertical/sdk/blob/main/docs/workflow/DEFINITION_OF_DONE.md)  
- [Kanban Process](https://github.com/happyvertical/sdk/blob/main/docs/workflow/KANBAN.md)
```

### Option 2: Git Submodule
Add this workflow directory as a submodule:

```bash
git submodule add https://github.com/happyvertical/sdk.git workflow-standards
git submodule init
git submodule update --remote --merge
```

Then reference the files locally:
```markdown
See [workflow-standards/docs/workflow/](./workflow-standards/docs/workflow/) for our development process.
```

### Option 3: Automated Sync
Use GitHub Actions to sync these files to your repository:

```yaml
name: Sync Workflow Standards
on:
  schedule:
    - cron: '0 0 * * 0' # Weekly
  workflow_dispatch:

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Sync workflow docs
        run: |
          mkdir -p .github/workflow-standards
          curl -L https://raw.githubusercontent.com/happyvertical/sdk/main/docs/workflow/DEFINITION_OF_READY.md -o .github/workflow-standards/DEFINITION_OF_READY.md
          curl -L https://raw.githubusercontent.com/happyvertical/sdk/main/docs/workflow/DEFINITION_OF_DONE.md -o .github/workflow-standards/DEFINITION_OF_DONE.md
          curl -L https://raw.githubusercontent.com/happyvertical/sdk/main/docs/workflow/KANBAN.md -o .github/workflow-standards/KANBAN.md
      - uses: peter-evans/create-pull-request@v5
        with:
          title: Update workflow standards
          body: Automated sync of workflow documentation from happyvertical/sdk
```

## Project Setup

To implement these workflows in your project:

### Quick Setup Commands

Run these GitHub CLI commands to set up your project with required labels and project board:

```bash
# Create type labels for issue categorization
gh label create "type:bug" --color "D73A4A" --description "Something isn't working"
gh label create "type:feature" --color "0075CA" --description "New feature or request"
gh label create "type:enhancement" --color "A2EEEF" --description "Improvement to existing functionality"
gh label create "type:tech-debt" --color "FBCA04" --description "Technical debt or refactoring"
gh label create "type:epic" --color "8B5CF6" --description "Large feature that spans multiple issues"

# Create a new project board with GitHub's standard Status field
gh project create --owner "@me" --title "Development Workflow"
```

### Project Board Setup

After creating the project, configure it to use GitHub's built-in Status field:

1. **Configure the project board**:
   - Visit your project in GitHub's web interface
   - Switch to "Board" view
   - The Status field will have default options: Todo, In Progress, Done

2. **Enable built-in automation**:
   - Go to project Settings > Workflows
   - Enable "Auto-add to project" to automatically add new issues with "Todo" status
   - Enable "Auto-archive" to move completed items to archive

3. **Set up status automation** (optional):
   - Configure workflows to automatically update status when PRs are opened/merged
   - Use GitHub's built-in project automation instead of custom GitHub Actions

The native approach provides:
- **Drag & Drop**: Visual updates automatically sync to the Status field
- **Built-in Automation**: No custom workflows needed for basic functionality
- **Status Tracking**: Single source of truth using GitHub's Status field

### Manual Setup Steps

If you prefer manual setup:

1. **Configure Labels**: Use the commands above to create type labels for issue categorization
2. **Setup Project Board**: Create project and configure Board view with Status field
3. **Enable Built-in Automation**: Use GitHub's native project workflows instead of custom actions
4. **Reference in README**: Add links to these standards in your project documentation

**Migration from Custom Labels**: If you have existing `status:*` labels, you can:
- Remove them from issues and rely on the Status field instead
- Delete the custom labels to simplify your workflow
- Use GitHub's built-in automation for status management

## Updates and Changes

Changes to these workflow standards should be:
1. Proposed via Pull Request to the happyvertical/sdk repository
2. Reviewed by team leads across affected projects
3. Documented with rationale in the PR description
4. Communicated to all teams upon merge

## Questions or Suggestions

Open an issue in the [happyvertical/sdk](https://github.com/happyvertical/sdk/issues) repository to discuss improvements or clarifications to these workflow standards.
</file>

<file path="docs/workflow/setup-project-board.sh">
#!/usr/bin/env bash

# HAppy VErtical Project Board Setup Script
# Creates a GitHub Project with board view and workflow columns

set -e

# Check if gh CLI is installed and authenticated
if ! command -v gh &> /dev/null; then
    echo "Error: GitHub CLI (gh) is not installed"
    echo "Install from: https://cli.github.com/"
    exit 1
fi

# Check authentication and project scope
echo "Checking authentication..."
if ! gh auth status 2>/dev/null | grep -q "project"; then
    echo "Error: Missing 'project' scope in GitHub CLI authentication"
    echo "Run: gh auth refresh -s project"
    exit 1
fi

# Get repository owner and name
echo "Getting repository info..."
REPO_INFO=$(gh repo view --json owner,name)
echo "Repo info: $REPO_INFO"
OWNER=$(echo "$REPO_INFO" | jq -r '.owner.login')
REPO_NAME=$(echo "$REPO_INFO" | jq -r '.name')

echo "Setting up project board for $OWNER/$REPO_NAME"

# Get owner ID - try organization first
echo "Getting owner ID for $OWNER..."
OWNER_RESPONSE=$(gh api graphql -f query='
  query($login: String!) {
    organization(login: $login) {
      id
    }
  }' -F login="$OWNER" 2>/dev/null)

OWNER_ID=$(echo "$OWNER_RESPONSE" | jq -r '.data.organization.id')

# If organization lookup failed, try as user
if [ "$OWNER_ID" = "null" ] || [ -z "$OWNER_ID" ]; then
    echo "Organization not found, trying as user..."
    OWNER_RESPONSE=$(gh api graphql -f query='
      query($login: String!) {
        user(login: $login) {
          id
        }
      }' -F login="$OWNER")
    OWNER_ID=$(echo "$OWNER_RESPONSE" | jq -r '.data.user.id')
fi

echo "Owner ID: $OWNER_ID"

if [ "$OWNER_ID" = "null" ]; then
    echo "Error: Could not find owner ID for $OWNER"
    exit 1
fi

# Create the project
echo "Creating project..."
PROJECT_RESPONSE=$(gh api graphql -f query='
  mutation($ownerId: ID!, $title: String!) {
    createProjectV2(input: {
      ownerId: $ownerId
      title: $title
    }) {
      projectV2 {
        id
        title
        number
      }
    }
  }' -F ownerId="$OWNER_ID" -F title="Development Workflow")

PROJECT_ID=$(echo "$PROJECT_RESPONSE" | jq -r '.data.createProjectV2.projectV2.id')
PROJECT_NUMBER=$(echo "$PROJECT_RESPONSE" | jq -r '.data.createProjectV2.projectV2.number')

if [ "$PROJECT_ID" = "null" ]; then
    echo "Error creating project"
    echo "$PROJECT_RESPONSE"
    exit 1
fi

echo "Created project #$PROJECT_NUMBER with ID: $PROJECT_ID"

# Note: GitHub Projects automatically has a Status field when using board view
# We'll focus on configuring the project for board view instead
echo "Configuring project for board view..."

# Get the existing Status field (GitHub creates this automatically)
STATUS_FIELD_RESPONSE=$(gh api graphql -f query='
  query($projectId: ID!) {
    node(id: $projectId) {
      ... on ProjectV2 {
        field(name: "Status") {
          ... on ProjectV2SingleSelectField {
            id
            name
            options {
              id
              name
            }
          }
        }
      }
    }
  }' -F projectId="$PROJECT_ID")

STATUS_FIELD_ID=$(echo "$STATUS_FIELD_RESPONSE" | jq -r '.data.node.field.id')

if [ "$STATUS_FIELD_ID" = "null" ]; then
    # Create a basic Status field if it doesn't exist
    echo "Creating Status field..."
    STATUS_FIELD_RESPONSE=$(gh api graphql -f query='
      mutation($projectId: ID!) {
        createProjectV2Field(input: {
          projectId: $projectId
          dataType: SINGLE_SELECT
          name: "Status"
        }) {
          projectV2Field {
            ... on ProjectV2SingleSelectField {
              id
              name
            }
          }
        }
      }' -F projectId="$PROJECT_ID")
    
    STATUS_FIELD_ID=$(echo "$STATUS_FIELD_RESPONSE" | jq -r '.data.createProjectV2Field.projectV2Field.id')
fi

if [ "$STATUS_FIELD_ID" = "null" ]; then
    echo "Error creating Status field"
    echo "$STATUS_FIELD_RESPONSE"
    exit 1
fi

echo "Created Status field with ID: $STATUS_FIELD_ID"

# Get the default view ID
echo "Getting project views..."
VIEWS_RESPONSE=$(gh api graphql -f query='
  query($projectId: ID!) {
    node(id: $projectId) {
      ... on ProjectV2 {
        views(first: 10) {
          nodes {
            id
            name
            layout
          }
        }
      }
    }
  }' -F projectId="$PROJECT_ID")

# Find the first view (usually the default one)
VIEW_ID=$(echo "$VIEWS_RESPONSE" | jq -r '.data.node.views.nodes[0].id')

if [ "$VIEW_ID" = "null" ]; then
    echo "Error: Could not find project view"
    echo "$VIEWS_RESPONSE"
    exit 1
fi

# Configure Status field with workflow options
echo "Configuring Status field with workflow options..."
STATUS_CONFIG_RESPONSE=$(gh api graphql -f query='
mutation {
  updateProjectV2Field(input: {
    fieldId: "'$STATUS_FIELD_ID'"
    singleSelectOptions: [
      {name: "New Issues", color: YELLOW, description: "New issue requiring triage"},
      {name: "Icebox", color: BLUE, description: "Valid but not current priority"},
      {name: "Backlog", color: ORANGE, description: "Prioritized work waiting for development"},
      {name: "To Do", color: GREEN, description: "Ready for development (meets Definition of Ready)"},
      {name: "In Progress", color: YELLOW, description: "Currently being developed"},
      {name: "Review & Testing", color: ORANGE, description: "Under peer review and CI testing"},
      {name: "Ready for Deployment", color: GREEN, description: "Approved and ready for production"},
      {name: "Deployed", color: PURPLE, description: "Live in production"}
    ]
  }) {
    projectV2Field {
      ... on ProjectV2SingleSelectField {
        id
        name
        options {
          id
          name
          color
        }
      }
    }
  }
}')

if echo "$STATUS_CONFIG_RESPONSE" | jq -e '.errors' > /dev/null; then
    echo "Warning: Could not configure Status field options"
    echo "$STATUS_CONFIG_RESPONSE"
else
    echo "✅ Status field configured with workflow columns"
fi

echo ""
echo "Board view setup:"
echo "- Visit the project URL below"
echo "- Switch to Board view (may be automatic with Status field configured)"
echo "- The Status field should now show as columns with your workflow lanes"

# Link the project to the repository
echo "Linking project to repository..."
REPO_ID=$(gh repo view --json id | jq -r '.id')

LINK_RESPONSE=$(gh api graphql -f query='
  mutation($projectId: ID!, $repositoryId: ID!) {
    linkProjectV2ToRepository(input: {
      projectId: $projectId
      repositoryId: $repositoryId
    }) {
      repository {
        name
      }
    }
  }' -F projectId="$PROJECT_ID" -F repositoryId="$REPO_ID")

if echo "$LINK_RESPONSE" | jq -e '.errors' > /dev/null; then
    echo "Warning: Could not link project to repository"
    echo "$LINK_RESPONSE"
else
    echo "Linked project to repository"
fi

echo ""
echo "✅ Project board setup complete!"
echo ""
echo "Project URL: https://github.com/orgs/$OWNER/projects/$PROJECT_NUMBER"
echo ""
echo "The project board is now configured with columns matching the workflow lanes."
echo ""
echo "⚠️  IMPORTANT: GitHub Actions workflow required for label sync"
echo ""
echo "To enable automatic label-to-column synchronization, create this workflow:"
echo "File: .github/workflows/project-automation.yml"
echo ""
cat << 'EOF'
name: Project Board Automation

on:
  issues:
    types: [opened, labeled, unlabeled]
  pull_request:
    types: [opened, labeled, unlabeled]

jobs:
  sync-to-project:
    runs-on: ubuntu-latest
    steps:
      - name: Sync labels to project columns
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const projectNumber = <PROJECT_NUMBER>; // Replace with your project number
            
            // Get the issue or PR
            const item = context.payload.issue || context.payload.pull_request;
            if (!item) return;
            
            // Map status labels to column names
            const labelToColumn = {
              'status:new-issue': 'New Issues',
              'status:icebox': 'Icebox',
              'status:backlog': 'Backlog',
              'status:to-do': 'To Do',
              'status:in-progress': 'In Progress',
              'status:review-testing': 'Review & Testing',
              'status:ready-for-deployment': 'Ready for Deployment',
              'status:deployed': 'Deployed'
            };
            
            // Find the current status label
            const statusLabel = item.labels.find(label => label.name.startsWith('status:'));
            if (!statusLabel) return;
            
            const columnName = labelToColumn[statusLabel.name];
            if (!columnName) return;
            
            // Update the project item
            // This requires GraphQL API calls to:
            // 1. Find the project
            // 2. Find the item in the project
            // 3. Update the Status field
            console.log(`Would move item to column: ${columnName}`);
EOF
echo ""
echo "Replace <PROJECT_NUMBER> with: $PROJECT_NUMBER"
echo ""
echo "For a complete implementation, see:"
echo "https://docs.github.com/en/issues/planning-and-tracking-with-projects/automating-your-project"
</file>

<file path="packages/ai/src/index.ts">
/**
 * @have/ai - A standardized interface for AI model interactions
 * 
 * This package provides a unified interface for interacting with various AI models.
 * Currently supports OpenAI's models through their API.
 * 
 * Key components:
 * - AIClient - Base class and factory for creating AI service clients
 * - AIMessage - Represents messages in AI conversations
 * - AIThread - Manages conversation threads with AI models
 */

export * from './client.js';
export * from './message.js';
export * from './thread.js';
</file>

<file path="packages/ai/src/message.ts">
import { AIThread } from './thread.js';

/**
 * Options for creating AI messages
 */
export interface AIMessageOptions {
  /**
   * Role of the message sender
   */
  role?: 'user' | 'assistant' | 'system';
  
  /**
   * Format for the AI response
   */
  responseFormat?: { type: 'text' | 'json_object' };
}

/**
 * Represents a message in an AI conversation
 */
export class AIMessage {
  /**
   * Original options used to create this message
   */
  protected options;
  
  /**
   * Name of the message sender
   */
  public name: string;
  
  /**
   * Content of the message
   */
  public content: string;
  
  /**
   * Role of the message sender in the conversation
   */
  public role: 'user' | 'assistant' | 'system';

  /**
   * Creates a new AI message
   * 
   * @param options - Message configuration
   * @param options.role - Role of the message sender
   * @param options.content - Content of the message
   * @param options.name - Name of the message sender
   */
  constructor(options: {
    role: 'user' | 'assistant' | 'system';
    content: string;
    name: string;
  }) {
    this.options = options;
    this.role = options.role;
    this.content = options.content;
    this.name = options.name;
  }

  /**
   * Factory method to create a new AI message
   * 
   * @param options - Message configuration
   * @param options.thread - Thread this message belongs to
   * @param options.role - Role of the message sender
   * @param options.content - Content of the message
   * @param options.name - Name of the message sender
   * @returns Promise resolving to a new AIMessage instance
   */
  static async create(options: {
    thread: AIThread;
    role: 'user' | 'assistant' | 'system';
    content: string;
    name: string;
  }) {
    return new AIMessage(options);
  }
}
</file>

<file path="packages/ai/src/thread.ts">
import { AIClient, type AIClientOptions } from './client.js';
import { AIMessage } from './message.js';
import OpenAI from 'openai';

/**
 * Options for creating an AI conversation thread
 */
export interface AIThreadOptions {
  /**
   * Options for the AI client to use in this thread
   */
  ai: AIClientOptions;
}

/**
 * Represents a conversation thread with an AI model
 * Manages messages, references, and conversation state
 */
export class AIThread {
  /**
   * AI client instance for this thread
   */
  protected ai!: AIClient;
  
  /**
   * Options used to configure this thread
   */
  protected options: AIThreadOptions;
  
  /**
   * Messages in this conversation thread
   */
  private messages: AIMessage[] = [];
  
  /**
   * Reference materials to include in the conversation context
   */
  private references: { [name: string]: string } = {};

  /**
   * Creates a new AI thread
   * 
   * @param options - Thread configuration options
   */
  constructor(options: AIThreadOptions) {
    this.options = options;
  }

  /**
   * Factory method to create and initialize a new AI thread
   * 
   * @param options - Thread configuration options
   * @returns Promise resolving to an initialized AIThread
   */
  static async create(options: AIThreadOptions) {
    const thread = new AIThread(options);
    await thread.initialize();
    return thread; // No need to add system message here, do it in addSystem
  }

  /**
   * Initializes the AI client for this thread
   */
  public async initialize() {
    this.ai = await AIClient.create(this.options.ai);
  }

  /**
   * Adds a system message to the conversation
   * 
   * @param prompt - System message content
   * @returns Promise resolving to the created AIMessage
   */
  public async addSystem(prompt: string) {
    const message = await AIMessage.create({
      thread: this,
      role: 'system',
      name: 'system',
      content: prompt,
    });

    this.messages.push(message);
    return message;
  }

  /**
   * Adds a message to the conversation
   * 
   * @param options - Message options
   * @param options.role - Role of the message sender
   * @param options.name - Optional name of the message sender
   * @param options.content - Content of the message
   * @returns Promise resolving to the created AIMessage
   */
  public async add(options: {
    role: 'user' | 'assistant' | 'system';
    name?: string;
    content: string;
  }) {
    const message = await AIMessage.create({
      thread: this,
      role: options.role,
      name: options.name || options.role, // Default name to role if not provided
      content: options.content,
    });

    this.messages.push(message);
    return message;
  }

  /**
   * Gets all messages in this thread
   * 
   * @returns Array of AIMessage objects
   */
  public get(): AIMessage[] {
    return this.messages;
  }

  /**
   * Adds a reference to be included in the conversation context
   * 
   * @param name - Name of the reference
   * @param body - Content of the reference
   */
  public addReference(name: string, body: string): void {
    this.references[name] = body;
  }

  /**
   * Assembles the conversation history for sending to the AI
   * Properly orders system message, references, and conversation messages
   * 
   * @returns Array of message parameters formatted for the OpenAI API
   */
  public assembleHistory(): OpenAI.Chat.ChatCompletionMessageParam[] {
    const history: OpenAI.Chat.ChatCompletionMessageParam[] = [];

    // Add system message first
    const systemMessage = this.messages.find((m) => m.role === 'system');
    if (systemMessage) {
      history.push({
        role: systemMessage.role,
        content: systemMessage.content,
      });
    }

    // Add references as user messages (before other user/assistant messages)
    for (const name in this.references) {
      history.push({
        role: 'user',
        content: `Reference - ${name}:\n${this.references[name]}`,
      });
    }

    // Add other messages
    this.messages
      .filter((m) => m.role !== 'system')
      .forEach((message) => {
        history.push({ role: message.role, content: message.content });
      });

    return history;
  }

  /**
   * Sends a prompt to the AI and gets a response
   * 
   * @param prompt - Prompt message to send
   * @param options - Options for the AI response
   * @param options.responseFormat - Format for the AI to respond with
   * @returns Promise resolving to the AI response
   */
  public async do(
    prompt: string,
    options: {
      responseFormat?: 'html' | 'text' | 'json';
    } = {
      responseFormat: 'text',
    },
  ) {
    const { responseFormat } = options;
    const history = this.assembleHistory();

    // Get completion from AI with assembled history
    const response = await this.ai.textCompletion(prompt, {
      history,
      responseFormat: {
        type: responseFormat === 'json' ? 'json_object' : 'text',
      },
    });
    return response;
  }
}
</file>

<file path="packages/ai/README.md">
# @have/ai

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

A standardized interface for AI model interactions in the HAVE SDK.

## Overview

The `@have/ai` package provides a consistent interface for interacting with various AI models, making it easy to switch between providers without changing your application code. Currently supports OpenAI, with plans to expand to other providers.

## Features

- Unified interface for multiple AI providers
- Type-safe API with TypeScript
- Simple, promise-based interactions
- Configurable request options
- Streaming responses support
- Error handling and retry logic

## Installation

```bash
# Install with npm
npm install @have/ai

# Or with yarn
yarn add @have/ai

# Or with pnpm
pnpm add @have/ai
```

## Usage

### Basic Usage

```typescript
import { OpenAIModel } from '@have/ai';

// Initialize with your API key
const model = new OpenAIModel({
  apiKey: process.env.OPENAI_API_KEY,
  model: 'gpt-4-turbo',
});

// Simple completion
const response = await model.complete('Tell me about TypeScript');
console.log(response);

// Chat completion
const chatResponse = await model.chat([
  { role: 'system', content: 'You are a helpful assistant.' },
  { role: 'user', content: 'What are the benefits of TypeScript?' },
]);
console.log(chatResponse);
```

### Streaming Responses

```typescript
import { OpenAIModel } from '@have/ai';

const model = new OpenAIModel({
  apiKey: process.env.OPENAI_API_KEY,
  model: 'gpt-4-turbo',
});

// Stream the response
const stream = await model.streamChat([
  { role: 'system', content: 'You are a helpful assistant.' },
  { role: 'user', content: 'Write a short poem about coding.' },
]);

for await (const chunk of stream) {
  process.stdout.write(chunk.content);
}
```

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_ai.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/ai/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="packages/files/src/fetch.ts">
import { writeFile } from 'node:fs/promises';

/**
 * Rate limiter for controlling fetch request frequency by domain
 */
class RateLimiter {
  /**
   * Map of domains to their rate limit configurations
   */
  private domains: Map<
    string,
    {
      lastRequest: number;
      limit: number;
      interval: number;
      queue: number;
    }
  > = new Map();

  /**
   * Default maximum number of requests per interval
   */
  private defaultLimit = 6;
  
  /**
   * Default interval in milliseconds
   */
  private defaultInterval = 500;

  /**
   * Creates a new RateLimiter with default settings
   */
  constructor() {
    // Initialize with default settings
    this.domains.set('default', {
      lastRequest: 0,
      limit: this.defaultLimit,
      interval: this.defaultInterval,
      queue: 0,
    });
  }

  /**
   * Extracts the domain from a URL
   * 
   * @param url - URL to extract domain from
   * @returns Domain string or 'default' if the URL is invalid
   */
  private getDomain(url: string): string {
    try {
      return new URL(url).hostname;
    } catch {
      return 'default';
    }
  }

  /**
   * Waits until the next request can be made according to rate limits
   * 
   * @param url - URL to check rate limits for
   * @returns Promise that resolves when the request can proceed
   */
  async waitForNext(url: string): Promise<void> {
    const domain = this.getDomain(url);
    const now = Date.now();

    const domainConfig =
      this.domains.get(domain) || this.domains.get('default')!;

    // Wait if we're over the limit
    if (domainConfig.queue >= domainConfig.limit) {
      const timeToWait = Math.max(
        0,
        domainConfig.lastRequest + domainConfig.interval - now,
      );
      if (timeToWait > 0) {
        await new Promise((resolve) => setTimeout(resolve, timeToWait));
      }
      domainConfig.queue = 0;
    }

    domainConfig.lastRequest = now;
    domainConfig.queue++;
  }

  /**
   * Sets rate limit for a specific domain
   * 
   * @param domain - Domain to set limits for
   * @param limit - Maximum number of requests per interval
   * @param interval - Interval in milliseconds
   */
  setDomainLimit(domain: string, limit: number, interval: number) {
    this.domains.set(domain, {
      lastRequest: 0,
      limit,
      interval,
      queue: 0,
    });
  }

  /**
   * Gets rate limit configuration for a domain
   * 
   * @param domain - Domain to get limits for
   * @returns Rate limit configuration
   */
  getDomainLimit(domain: string) {
    return this.domains.get(domain) || this.domains.get('default')!;
  }
}

// Create singleton instance
const rateLimiter = new RateLimiter();

/**
 * Sets rate limit for a specific domain
 * 
 * @param domain - Domain to set limits for
 * @param limit - Maximum number of requests per interval
 * @param interval - Interval in milliseconds
 */
export async function addRateLimit(
  domain: string,
  limit: number,
  interval: number,
) {
  rateLimiter.setDomainLimit(domain, limit, interval);
}

/**
 * Gets rate limit configuration for a domain
 * 
 * @param domain - Domain to get limits for
 * @returns Rate limit configuration object with limit and interval properties
 */
export async function getRateLimit(
  domain: string,
): Promise<{ limit: number; interval: number }> {
  const config = rateLimiter.getDomainLimit(domain);
  return {
    limit: config.limit,
    interval: config.interval,
  };
}

/**
 * Performs a fetch request with rate limiting
 * 
 * @param url - URL to fetch
 * @param options - Fetch options
 * @returns Promise resolving to a Response object
 */
async function rateLimitedFetch(
  url: string,
  options?: RequestInit,
): Promise<Response> {
  await rateLimiter.waitForNext(url);
  return fetch(url, options);
}

/**
 * Fetches a URL and returns the response as text
 * 
 * @param url - URL to fetch
 * @returns Promise resolving to the response body as a string
 */
export async function fetchText(url: string): Promise<string> {
  const response = await rateLimitedFetch(url);
  return response.text();
}

/**
 * Fetches a URL and returns the response as parsed JSON
 * 
 * @param url - URL to fetch
 * @returns Promise resolving to the parsed JSON response
 */
export async function fetchJSON(url: string): Promise<any> {
  const response = await rateLimitedFetch(url);
  return response.json();
}

/**
 * Fetches a URL and returns the response as a Buffer
 * 
 * @param url - URL to fetch
 * @returns Promise resolving to the response body as a Buffer
 */
export async function fetchBuffer(url: string): Promise<Buffer> {
  const response = await rateLimitedFetch(url);
  return Buffer.from(await response.arrayBuffer());
}

/**
 * Fetches a URL and saves the response to a file
 * 
 * @param url - URL to fetch
 * @param filepath - Path to save the file to
 * @returns Promise that resolves when the file is saved
 */
export async function fetchToFile(
  url: string,
  filepath: string,
): Promise<void> {
  const response = await rateLimitedFetch(url);
  const buffer = await response.arrayBuffer();
  await writeFile(filepath, Buffer.from(buffer));
}
</file>

<file path="packages/files/src/filesystem-local.ts">
import { FilesystemAdapter, FilesystemAdapterOptions } from './filesystem.js';
import { getMimeType } from './index.js';

/**
 * Adapter for interacting with the local filesystem
 */
export class LocalFilesystemAdapter extends FilesystemAdapter {
  /**
   * Cache directory path
   */
  protected cacheDir: string;
  
  /**
   * Type identifier for this adapter
   */
  public type: string;
  
  /**
   * Configuration options
   */
  protected options: FilesystemAdapterOptions;
  
  /**
   * Creates a new LocalFilesystemAdapter instance
   * 
   * @param options - Configuration options
   */
  constructor(options: FilesystemAdapterOptions) {
    super(options);
    this.options = options;
    this.type = options.type || 'local';
    this.cacheDir = options.cacheDir || '.cache';
  }

  /**
   * Creates a LocalFilesystemAdapter from a URL
   * 
   * @param url - URL to create adapter from
   * @returns Promise resolving to a LocalFilesystemAdapter
   */
  static async createFromUrl(url: string) {}

  /**
   * Factory method to create a LocalFilesystemAdapter
   * 
   * @param options - Configuration options
   * @returns Promise resolving to an initialized LocalFilesystemAdapter
   */
  static async create(options: FilesystemAdapterOptions) {
    const fs = new LocalFilesystemAdapter(options);
    return fs;
  }

  /**
   * Checks if a file or directory exists in the local filesystem
   * 
   * @param path - Path to check
   * @returns Promise resolving to boolean indicating existence
   */
  async exists(path: string) {
    return false;
  }

  /**
   * Reads a file's contents from the local filesystem
   * 
   * @param path - Path to the file
   * @returns Promise resolving to the file contents as a string
   */
  async read(path: string) {
    return '';
  }

  /**
   * Writes content to a file in the local filesystem
   * 
   * @param path - Path to the file
   * @param content - Content to write
   * @returns Promise that resolves when the write is complete
   */
  async write(path: string, content: string) {
    return;
  }

  /**
   * Deletes a file or directory from the local filesystem
   * 
   * @param path - Path to delete
   * @returns Promise that resolves when the deletion is complete
   */
  async delete(path: string) {
    return;
  }

  /**
   * Lists files in a directory in the local filesystem
   * 
   * @param path - Directory path to list
   * @returns Promise resolving to an array of file names
   */
  async list(path: string) {
    return [];
  }

  /**
   * Gets the MIME type for a file based on its extension
   * 
   * @param path - Path to the file
   * @returns Promise resolving to the MIME type string
   */
  async mimeType(path: string) {
    const extension = path.slice(((path.lastIndexOf('.') - 1) >>> 0) + 2);
    return getMimeType(`.${extension}`);
  }
}
</file>

<file path="packages/files/src/filesystem.ts">
import path from 'path';
import os from 'os';
import { mkdir } from 'fs/promises';
import { getCached, setCached } from './index.js';

/**
 * Interface defining the required methods for a filesystem adapter
 */
export interface FilesystemAdapterInterface {
  /**
   * Checks if a file or directory exists
   * 
   * @param path - Path to check
   * @returns Promise resolving to boolean indicating existence
   */
  exists(path: string): Promise<boolean>;
  
  /**
   * Reads a file's contents
   * 
   * @param path - Path to the file
   * @returns Promise resolving to the file contents as a string
   */
  read(path: string): Promise<string>;
  
  /**
   * Writes content to a file
   * 
   * @param path - Path to the file
   * @param content - Content to write
   * @returns Promise that resolves when the write is complete
   */
  write(path: string, content: string): Promise<void>;
  
  /**
   * Deletes a file or directory
   * 
   * @param path - Path to delete
   * @returns Promise that resolves when the deletion is complete
   */
  delete(path: string): Promise<void>;
  
  /**
   * Lists files in a directory
   * 
   * @param path - Directory path to list
   * @returns Promise resolving to an array of file names
   */
  list(path: string): Promise<string[]>;
  
  /**
   * Gets the MIME type for a file
   * 
   * @param path - Path to the file
   * @returns Promise resolving to the MIME type string
   */
  mimeType(path: string): Promise<string>;
}

/**
 * Configuration options for filesystem adapters
 */
export interface FilesystemAdapterOptions {
  /**
   * Type of filesystem adapter
   */
  type?: string;
  
  /**
   * Directory to use for caching
   */
  cacheDir?: string;
}

/**
 * Base class for filesystem adapters providing common functionality
 */
export class FilesystemAdapter {
  /**
   * Configuration options
   */
  protected options: FilesystemAdapterOptions;
  
  /**
   * Cache directory path
   */
  protected cacheDir: string;

  /**
   * Creates a new FilesystemAdapter instance
   * 
   * @param options - Configuration options
   */
  constructor(options: FilesystemAdapterOptions) {
    this.options = options;
    this.cacheDir =
      options.cacheDir || path.join(os.tmpdir(), 'have-sdk', '.cache');
  }

  /**
   * Factory method to create and initialize a FilesystemAdapter
   * 
   * @param options - Configuration options
   * @returns Promise resolving to an initialized FilesystemAdapter
   */
  static async create<T extends FilesystemAdapterOptions>(
    options: T,
  ): Promise<FilesystemAdapter> {
    const fs = new FilesystemAdapter(options);
    await fs.initialize();
    return fs;
  }

  /**
   * Initializes the adapter by creating the cache directory
   */
  protected async initialize() {
    await mkdir(this.cacheDir, { recursive: true });
  }

  /**
   * Downloads a file from a URL
   * 
   * @param url - URL to download from
   * @param options - Download options
   * @param options.force - Whether to force download even if cached
   * @returns Promise resolving to the path of the downloaded file
   */
  async download(
    url: string,
    options: {
      force: boolean;
    } = {
      force: false,
    },
  ): Promise<string> {
    return '';
  }

  /**
   * Checks if a file or directory exists
   * 
   * @param path - Path to check
   * @returns Promise resolving to boolean indicating existence
   */
  async exists(path: string): Promise<boolean> {
    // Dummy implementation
    return false;
  }

  /**
   * Reads a file's contents
   * 
   * @param path - Path to the file
   * @returns Promise resolving to the file contents as a string
   */
  async read(path: string): Promise<string> {
    // Dummy implementation
    return '';
  }

  /**
   * Writes content to a file
   * 
   * @param path - Path to the file
   * @param content - Content to write
   * @returns Promise that resolves when the write is complete
   */
  async write(path: string, content: string): Promise<void> {
    // Dummy implementation
  }

  /**
   * Deletes a file or directory
   * 
   * @param path - Path to delete
   * @returns Promise that resolves when the deletion is complete
   */
  async delete(path: string): Promise<void> {
    // Dummy implementation
  }

  /**
   * Lists files in a directory
   * 
   * @param path - Directory path to list
   * @returns Promise resolving to an array of file names
   */
  async list(path: string): Promise<string[]> {
    // Dummy implementation
    return [];
  }

  /**
   * Gets data from cache if available and not expired
   * 
   * @param file - Cache file identifier
   * @param expiry - Cache expiry time in milliseconds
   * @returns Promise resolving to the cached data or undefined if not found/expired
   */
  async getCached(file: string, expiry: number = 300000) {
    return getCached(file, expiry);
  }

  /**
   * Sets data in cache
   * 
   * @param file - Cache file identifier
   * @param data - Data to cache
   * @returns Promise that resolves when the data is cached
   */
  async setCached(file: string, data: string) {
    return setCached(file, data);
  }
}
</file>

<file path="packages/files/CLAUDE.md">
# @have/files: File System Interface Package

## Purpose and Responsibilities

The `@have/files` package provides a standardized interface for file system operations, supporting both local and potentially remote file systems. Its core responsibilities include:

- Reading and writing files with consistent APIs
- Managing directory creation and navigation
- Handling file paths across different platforms
- Supporting temporary file operations
- Providing utilities for common file operations

This package abstracts away the complexities of different file systems, allowing other packages to work with files in a consistent way regardless of the underlying storage.

## Key APIs

### Basic File Operations

```typescript
import { readFile, writeFile, fileExists } from '@have/files';

// Read a file
const content = await readFile('/path/to/file.txt');

// Write a file (creates directories if needed)
await writeFile('/path/to/new/file.txt', 'File content');

// Check if a file exists
const exists = await fileExists('/path/to/file.txt');
```

### Directory Operations

```typescript
import { 
  createDirectory, 
  ensureDirectory, 
  listDirectory 
} from '@have/files';

// Create a directory
await createDirectory('/path/to/new/dir');

// Ensure a directory exists (creates if it doesn't)
await ensureDirectory('/path/to/another/dir');

// List directory contents
const files = await listDirectory('/path/to/dir');
```

### Temporary Files

```typescript
import { 
  getTempDirectory, 
  createTempFile, 
  createTempDirectory 
} from '@have/files';

// Get path to a temporary directory
const tempDir = getTempDirectory('my-app');

// Create a temporary file
const { path, cleanup } = await createTempFile({ 
  prefix: 'data-', 
  extension: '.json',
  content: '{"key": "value"}'
});
// Use the file...
// Then clean it up
await cleanup();

// Create a temporary directory
const { path, cleanup } = await createTempDirectory('temp-data');
// Use the directory...
// Then clean it up
await cleanup();
```

### Path Utilities

```typescript
import { 
  resolvePath, 
  getExtension, 
  getFilename, 
  getDirectory 
} from '@have/files';

// Resolve a path (handles relative paths)
const absolutePath = resolvePath('~/documents/file.txt');

// Get file extension
const ext = getExtension('/path/to/document.pdf'); // => 'pdf'

// Get filename
const filename = getFilename('/path/to/document.pdf'); // => 'document.pdf'

// Get directory path
const dir = getDirectory('/path/to/document.pdf'); // => '/path/to'
```

### Content Handling

```typescript
import { 
  readJson, 
  writeJson, 
  readLines,
  appendFile
} from '@have/files';

// Read JSON file
const data = await readJson('/path/to/config.json');

// Write JSON file
await writeJson('/path/to/data.json', { key: 'value' });

// Read file line by line
const lines = await readLines('/path/to/log.txt');

// Append to file
await appendFile('/path/to/log.txt', 'New log entry\n');
```

## Dependencies

The package has minimal dependencies:

- `@have/utils`: For utility functions like path handling and temporary directory management

No external file system libraries are used, leveraging Node.js's built-in `fs/promises` module for most operations.

## Development Guidelines

### Error Handling

File operations can fail for various reasons (permissions, disk space, etc.). Handle these cases appropriately:

```typescript
try {
  await writeFile('/path/to/file.txt', 'content');
} catch (error) {
  if (error.code === 'ENOENT') {
    // Handle "no such file or directory" error
  } else if (error.code === 'EACCES') {
    // Handle permission error
  } else {
    // Handle other errors
  }
}
```

### Path Normalization

Always normalize paths to ensure consistency across different platforms:

```typescript
// Using built-in functions
const normalizedPath = resolvePath('~/documents/file.txt');
```

### Testing

The package includes tests for verifying file operations:

```bash
pnpm test        # Run tests once
pnpm test:watch  # Run tests in watch mode
```

Tests use temporary directories to avoid affecting the real file system.

### Building

Build the package with:

```bash
pnpm build       # Build once
pnpm build:watch # Build in watch mode
```

### Best Practices

- Always clean up temporary files and directories
- Use async/await with file operations to avoid blocking the event loop
- Prefer streaming for large files to manage memory usage
- Handle path separators carefully for cross-platform compatibility
- Use appropriate file permissions when creating files
- Consider file locking for operations that need exclusive access
- Dont include branding in commit messages

This package provides the foundation for file operations across the HAVE SDK, ensuring consistent behavior regardless of the environment.
</file>

<file path="packages/files/README.md">
# @have/files

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

File system interface abstraction layer for the HAVE SDK.

## Overview

The `@have/files` package provides a unified interface for working with file systems, supporting both local and remote file operations. It simplifies common file operations with a consistent API regardless of the underlying storage system.

## Features

- Unified API for local and remote file systems
- Promise-based interface for all operations
- Support for reading, writing, and manipulating files and directories
- Stream support for efficient handling of large files
- Metadata and attribute handling
- Extensible adapter system

## Installation

```bash
# Install with npm
npm install @have/files

# Or with yarn
yarn add @have/files

# Or with pnpm
pnpm add @have/files
```

## Usage

### Local File System

```typescript
import { LocalFileSystem } from '@have/files';

// Create a file system instance
const fs = new LocalFileSystem();

// Read a file
const content = await fs.readFile('/path/to/file.txt');
console.log(content);

// Write a file
await fs.writeFile('/path/to/output.txt', 'Hello, world!');

// Check if a file exists
const exists = await fs.exists('/path/to/file.txt');
console.log(`File exists: ${exists}`);

// List files in a directory
const files = await fs.readDir('/path/to/directory');
console.log(files);
```

### Remote File System (example with S3)

```typescript
import { S3FileSystem } from '@have/files';

// Create an S3 file system instance
const s3fs = new S3FileSystem({
  bucket: 'my-bucket',
  region: 'us-west-2',
  // Credentials are loaded from environment or AWS configuration
});

// Read a file from S3
const content = await s3fs.readFile('path/to/file.txt');
console.log(content);

// Write a file to S3
await s3fs.writeFile('path/to/output.txt', 'Hello, world!');

// List files in an S3 directory
const files = await s3fs.readDir('path/to/directory');
console.log(files);
```

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_files.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/files/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="packages/pdf/src/modules.d.ts">
/**
 * Type declarations for the scribe.js-ocr library
 * 
 * This library provides OCR (Optical Character Recognition) functionality
 * to extract text from images and PDF files.
 */
declare module 'scribe.js-ocr';

/**
 * Type declarations for the PDF.js library (legacy build)
 * 
 * This library provides PDF parsing and rendering functionality.
 * The legacy build is used for compatibility with older environments.
 */
declare module 'pdfjs-dist/legacy/build/pdf.mjs';
</file>

<file path="packages/pdf/README.md">
# @have/pdf

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

PDF processing utilities for the HAVE SDK.

## Overview

The `@have/pdf` package provides tools for parsing, processing, and extracting data from PDF documents. It offers a simple and consistent API for working with PDF files in both browser and Node.js environments.

## Features

- PDF text extraction with layout preservation
- Metadata extraction (title, author, creation date, etc.)
- Page splitting and merging
- Image extraction
- Form field handling
- Structured data extraction
- Table detection and parsing
- Conversion to other formats (HTML, Markdown, etc.)

## Installation

```bash
# Install with npm
npm install @have/pdf

# Or with yarn
yarn add @have/pdf

# Or with pnpm
pnpm add @have/pdf
```

## Usage

### Basic Text Extraction

```typescript
import { PDFDocument } from '@have/pdf';

// Load a PDF from a file path (Node.js)
const doc = await PDFDocument.fromFile('/path/to/document.pdf');

// Or from a URL
const doc2 = await PDFDocument.fromURL('https://example.com/document.pdf');

// Extract text from the entire document
const text = await doc.extractText();
console.log(text);

// Extract text from specific pages
const page1Text = await doc.extractText({ pageNumbers: [0] }); // 0-indexed
console.log(page1Text);
```

### Metadata Extraction

```typescript
import { PDFDocument } from '@have/pdf';

const doc = await PDFDocument.fromFile('/path/to/document.pdf');

// Get document metadata
const metadata = await doc.getMetadata();
console.log(`Title: ${metadata.title}`);
console.log(`Author: ${metadata.author}`);
console.log(`Creation Date: ${metadata.creationDate}`);
console.log(`Page Count: ${metadata.pageCount}`);
```

### Converting PDF to Markdown

```typescript
import { PDFDocument } from '@have/pdf';

const doc = await PDFDocument.fromFile('/path/to/document.pdf');

// Convert to markdown
const markdown = await doc.toMarkdown();
console.log(markdown);

// Save to a file
import { writeFile } from 'fs/promises';
await writeFile('output.md', markdown);
```

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_pdf.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/pdf/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="packages/smrt/src/class.ts">
import { getDatabase } from '@have/sql';
import type { DatabaseOptions, DatabaseInterface } from '@have/sql';
import { FilesystemAdapter } from '@have/files';
import type { FilesystemAdapterOptions } from '@have/files';
import type { AIClientOptions } from '@have/ai';
import { AIClient } from '@have/ai';

/**
 * Configuration options for the BaseClass
 */
export interface BaseClassOptions {
  /**
   * Optional custom class name override
   */
  _className?: string;
  
  /**
   * Database configuration options
   */
  db?: DatabaseOptions;
  
  /**
   * Filesystem adapter configuration options
   */
  fs?: FilesystemAdapterOptions;
  
  /**
   * AI client configuration options
   */
  ai?: AIClientOptions;
}

/**
 * Foundation class providing core functionality for the SMRT framework
 * 
 * BaseClass provides unified access to database, filesystem, and AI client
 * interfaces. It serves as the foundation for all other classes in the
 * SMRT framework.
 */
export class BaseClass<T extends BaseClassOptions = BaseClassOptions> {
  /**
   * AI client instance for interacting with AI models
   */
  protected _ai!: AIClient;
  
  /**
   * Filesystem adapter for file operations
   */
  protected _fs!: FilesystemAdapter;
  
  /**
   * Database interface for data persistence
   */
  protected _db!: DatabaseInterface;
  
  /**
   * Class name used for identification
   */
  protected _className!: string;
  
  /**
   * Configuration options provided to the class
   */
  protected options: T;

  /**
   * Creates a new BaseClass instance
   * 
   * @param options - Configuration options for database, filesystem, and AI clients
   */
  constructor(options: T) {
    this.options = options;
    this._className = this.constructor.name;
  }

  /**
   * Initializes database, filesystem, and AI client connections
   * 
   * This method sets up all required services based on the provided options.
   * It should be called before using any of the service interfaces.
   * 
   * @returns Promise that resolves when initialization is complete
   */
  protected async initialize(): Promise<void> {
    if (this.options.db) {
      this._db = await getDatabase(this.options.db);
    }
    if (this.options.fs) {
      this._fs = await FilesystemAdapter.create(this.options.fs);
    }
    if (this.options.ai) {
      this._ai = await AIClient.create(this.options.ai);
    }
  }

  /**
   * Gets the filesystem adapter instance
   */
  get fs() {
    return this._fs;
  }

  /**
   * Gets the database interface instance
   */
  get db() {
    return this._db;
  }

  /**
   * Gets the AI client instance
   */
  get ai() {
    return this._ai;
  }
}
</file>

<file path="packages/smrt/src/collection.ts">
import type { BaseClassOptions } from './class.js';
import { BaseClass } from './class.js';
import {
  fieldsFromClass,
  tableNameFromClass,
  generateSchema,
  formatDataJs,
  formatDataSql,
} from './utils.js';
import { syncSchema, buildWhere } from '@have/sql';
import { BaseObject } from './object.js';

/**
 * Configuration options for BaseCollection
 */
export interface BaseCollectionOptions extends BaseClassOptions {}

/**
 * Collection interface for managing sets of BaseObjects
 * 
 * BaseCollection provides methods for querying, creating, and managing
 * collections of persistent objects. It handles database setup, schema
 * generation, and provides a fluent interface for querying objects.
 */
export class BaseCollection<
  ModelType extends BaseObject<any>,
  T extends BaseCollectionOptions = BaseCollectionOptions,
> extends BaseClass<T> {
  /**
   * Promise tracking the database setup operation
   */
  protected _db_setup_promise: Promise<void> | null = null;
  
  /**
   * Gets the class constructor for items in this collection
   */
  protected get _itemClass(): (new (options: any) => ModelType) & {
    create(options: any): ModelType | Promise<ModelType>;
  } {
    const constructor = this.constructor as {
      readonly _itemClass?: (new (options: any) => ModelType) & {
        create(options: any): ModelType | Promise<ModelType>;
      };
    };
    if (!constructor._itemClass) {
      // todo: sort out why Meetings._itemClass is undefined
      throw new Error(
        `Collection ${this.constructor.name} must define static _itemClass`,
      );
      //   console.warn(
      //     `Collection ${this.constructor.name} must define static _itemClass`,
      //   );
      // }
    }
    return constructor._itemClass;
  }

  /**
   * Static reference to the item class constructor
   */
  static readonly _itemClass: any;
  
  /**
   * Database table name for this collection
   */
  public _tableName!: string;

  /**
   * Valid SQL operators that can be used in where conditions.
   * Keys are the operators as they appear in the query object,
   * values are their SQL equivalents.
   */
  private readonly VALID_OPERATORS = {
    '=': '=',
    '>': '>',
    '>=': '>=',
    '<': '<',
    '<=': '<=',
    '!=': '!=',
    like: 'LIKE',
    in: 'IN',
    // Add more operators as needed
  } as const;

  /**
   * Creates a new BaseCollection instance
   * 
   * @param options - Configuration options
   */
  constructor(options: T) {
    super(options);
  }

  /**
   * Initializes the collection, setting up database tables
   * 
   * @returns Promise that resolves when initialization is complete
   */
  public async initialize() {
    await super.initialize();
    if (this.options.db) {
      await this.setupDb();
    }
  } 

  /**
   * Retrieves a single object from the collection by ID, slug, or custom filter
   * 
   * @param filter - String ID/slug or object with filter conditions
   * @returns Promise resolving to the object or null if not found
   */
  public async get(filter: string | Record<string, any>) {
    const where =
      typeof filter === 'string'
        ? /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(
            filter,
          )
          ? { id: filter }
          : { slug: filter, context: '' }
        : filter;

    // let sql = `SELECT * FROM ${this.tableName}`;
    const { sql: whereSql, values: whereValues } = buildWhere(where);

    const { rows } = await this.db.query(
      `SELECT * FROM ${this.tableName} ${whereSql}`,
      whereValues,
    );
    if (!rows?.[0]) {
      return null;
    }

    return this.create(formatDataJs(rows[0]));
  }

  /**
   * Lists records from the collection with flexible filtering options
   *
   * @param options - Query options object
   * @param options.where - Record of conditions to filter results. Each key can include an operator
   *                      separated by a space (e.g., 'price >', 'name like'). Default operator is '='.
   * @param options.offset - Number of records to skip
   * @param options.limit - Maximum number of records to return
   * @param options.orderBy - Field(s) to order results by, with optional direction
   *
   * @example
   * ```typescript
   * // Find active products priced between $100-$200
   * await collection.list({
   *   where: {
   *     'price >': 100,
   *     'price <=': 200,
   *     'status': 'active',              // equals operator is default
   *     'category in': ['A', 'B', 'C'],  // IN operator for arrays
   *     'name like': '%shirt%',          // LIKE for pattern matching
   *     'deleted_at !=': null            // exclude deleted items
   *   },
   *   limit: 10,
   *   offset: 0
   * });
   *
   * // Find users matching pattern but not in specific roles
   * await users.list({
   *   where: {
   *     'email like': '%@company.com',
   *     'active': true,
   *     'role in': ['guest', 'blocked'],
   *     'last_login <': lastMonth
   *   }
   * });
   * ```
   *
   * @returns Promise resolving to an array of model instances
   */
  public async list(options: {
    where?: Record<string, any>;
    offset?: number;
    limit?: number;
    orderBy?: string | string[];
  }) {
    const { where, offset, limit, orderBy } = options;
    const { sql: whereSql, values: whereValues } = buildWhere(where || {});

    let orderBySql = '';
    if (orderBy) {
      orderBySql = ' ORDER BY ';
      const orderByItems = Array.isArray(orderBy) ? orderBy : [orderBy];

      orderBySql += orderByItems
        .map((item) => {
          const [field, direction = 'ASC'] = item.split(' ');

          // Validate field name
          if (!/^[a-zA-Z0-9_]+$/.test(field)) {
            throw new Error(`Invalid field name for ordering: ${field}`);
          }

          // Validate direction
          const normalizedDirection = direction.toUpperCase();
          if (normalizedDirection !== 'ASC' && normalizedDirection !== 'DESC') {
            throw new Error(
              `Invalid sort direction: ${direction}. Must be ASC or DESC.`,
            );
          }

          return `${field} ${normalizedDirection}`;
        })
        .join(', ');
    }

    let limitOffsetSql = '';
    const limitOffsetValues = [];

    if (limit !== undefined) {
      limitOffsetSql += ' LIMIT ?';
      limitOffsetValues.push(limit);
    }

    if (offset !== undefined) {
      limitOffsetSql += ' OFFSET ?';
      limitOffsetValues.push(offset);
    }

    const result = await this.db.query(
      `SELECT * FROM ${this.tableName} ${whereSql} ${orderBySql} ${limitOffsetSql}`,
      [...whereValues, ...limitOffsetValues],
    );
    return Promise.all(
      result.rows.map((item: object) => this.create(formatDataJs(item))),
    );
  }

  /**
   * Creates a new instance of the collection's item class
   * 
   * @param options - Options for creating the item
   * @returns New item instance
   */
  public create(options: any) {
    const params = {
      ai: this.options.ai,
      db: this.options.db,
      ...options,
    };
    return this._itemClass.create(params);
  }

  /**
   * Gets an existing item or creates a new one if it doesn't exist
   * 
   * @param data - Object data to find or create
   * @param defaults - Default values to use if creating a new object
   * @returns Promise resolving to the existing or new object
   */
  public async getOrUpsert(data: any, defaults: any = {}) {
    data = formatDataSql(data);
    let where: any = {};
    if (data.id) {
      where = { id: data.id };
    } else if (data.slug) {
      where = { slug: data.slug, context: data.context || '' };
    } else {
      where = data;
    }
    const existing = await this.get(where);
    if (existing) {
      const diff = this.getDiff(existing, data);
      if (diff) {
        Object.assign(existing, diff);
        await existing.save();
        return existing;
      }
      return existing;
    }
    const upsertData = { ...defaults, ...data };
    const upserted = await this.create(upsertData);
    await upserted.save();
    return upserted;
  }

  /**
   * Gets differences between an existing object and new data
   * 
   * @param existing - Existing object
   * @param data - New data
   * @returns Object containing only the changed fields
   */
  getDiff(
    existing: Record<string, any>,
    data: Record<string, any>,
  ): Record<string, any> {
    const fields = this._itemClass.prototype.getFields();
    return Object.keys(data).reduce(
      (acc, key) => {
        if (fields[key] && existing[key] !== data[key]) {
          acc[key] = data[key];
        }
        return acc;
      },
      {} as Record<string, any>,
    );
  }

  /**
   * Sets up the database schema for this collection
   * 
   * @returns Promise that resolves when setup is complete
   */
  async setupDb() {
    if (this._db_setup_promise) {
      return this._db_setup_promise;
    }

    this._db_setup_promise = (async () => {
      try {
        const schema = this.generateSchema();
        await syncSchema({ db: this.db, schema });
        await this.setupTriggers();
      } catch (error) {
        this._db_setup_promise = null; // Allow retry on failure
        throw error;
      }
    })();

    return this._db_setup_promise;
  }

  /**
   * Gets field definitions for the collection's item class
   * 
   * @returns Object containing field definitions
   */
  getFields() {
    return fieldsFromClass(this._itemClass);
  }

  /**
   * Generates database schema for the collection's item class
   * 
   * @returns Schema object for database setup
   */
  generateSchema() {
    // Use the imported generateSchema function with the item class
    return generateSchema(this._itemClass);
  }

  /**
   * Sets up database triggers for automatically updating timestamps
   * 
   * @returns Promise that resolves when triggers are set up
   */
  async setupTriggers() {
    const triggers = [
      `${this.tableName}_set_created_at`,
      `${this.tableName}_set_updated_at`,
    ];

    for (const trigger of triggers) {
      const exists = await this.db
        .pluck`SELECT name FROM sqlite_master WHERE type='trigger' AND name=${trigger}`;
      if (!exists) {
        if (trigger === `${this.tableName}_set_created_at`) {
          const createTriggerSQL = `
            CREATE TRIGGER ${trigger}
            AFTER INSERT ON ${this.tableName}
            BEGIN
              UPDATE ${this.tableName} 
              SET created_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP 
              WHERE id = NEW.id;
            END;
          `;
          await this.db.query(createTriggerSQL);
        } else if (trigger === `${this.tableName}_set_updated_at`) {
          const createTriggerSQL = `
            CREATE TRIGGER ${trigger}
            AFTER UPDATE ON ${this.tableName}
            BEGIN
              UPDATE ${this.tableName} 
              SET updated_at = CURRENT_TIMESTAMP 
              WHERE id = NEW.id;
            END;
          `;
          await this.db.query(createTriggerSQL);
        }
      }
    }
  }

  /**
   * Gets the database table name for this collection
   */
  get tableName() {
    if (!this._tableName) {
      this._tableName = tableNameFromClass(this.constructor);
    }
    return this._tableName;
  }

  /**
   * Generates a table name from the collection class name
   * 
   * @returns Generated table name
   */
  generateTableName() {
    // Convert camelCase/PascalCase to snake_case and pluralize
    const tableName = this._className
      // Insert underscore between lower & upper case letters
      .replace(/([a-z])([A-Z])/g, '$1_$2')
      // Convert to lowercase
      .toLowerCase()
      // Handle basic pluralization rules
      .replace(/([^s])$/, '$1s')
      // Handle special cases ending in 'y'
      .replace(/y$/, 'ies');

    return tableName;
  }

  /**
   * Counts records in the collection matching the given filters
   * 
   * Accepts the same where conditions as list() but ignores limit/offset/orderBy.
   * 
   * @param options - Query options object
   * @param options.where - Record of conditions to filter results
   * @returns Promise resolving to the total count of matching records
   */
  public async count(options: { where?: Record<string, any> } = {}) {
    const { where } = options;
    const { sql: whereSql, values: whereValues } = buildWhere(where || {});

    const result = await this.db.query(
      `SELECT COUNT(*) as count FROM ${this.tableName} ${whereSql}`,
      whereValues
    );
    
    return parseInt(result.rows[0].count, 10);
  }
}
</file>

<file path="packages/smrt/src/content.ts">
import type { BaseObjectOptions } from './object.js';
import { BaseObject } from './object.js';

/**
 * Options for Content initialization
 */
export interface ContentOptions extends BaseObjectOptions {
  /**
   * Content type classification
   */
  type?: string | null;
  
  /**
   * Reference to file storage key
   */
  fileKey?: string | null;
  
  /**
   * Author of the content
   */
  author?: string | null;
  
  /**
   * Content title
   */
  title?: string | null;
  
  /**
   * Short description or summary
   */
  description?: string | null;
  
  /**
   * Main content body text
   */
  body?: string | null;
  
  /**
   * Date when content was published
   */
  publish_date?: Date | null;
  
  /**
   * URL source of the content
   */
  url?: string | null;
  
  /**
   * Original source identifier
   */
  source?: string | null;
  
  /**
   * Publication status
   */
  status?: 'published' | 'draft' | 'archived' | 'deleted' | null;
  
  /**
   * Content state flag
   */
  state?: 'deprecated' | 'active' | 'highlighted' | null;
}

/**
 * Structured content object with metadata and body text
 * 
 * Content represents any text-based content with metadata such as
 * title, author, description, and publishing information. It supports
 * referencing related content objects.
 */
export class Content extends BaseObject<ContentOptions> {
  /**
   * Configuration options
   */
  protected options: ContentOptions;
  
  /**
   * Array of referenced content objects
   */
  protected references: Content[] = [];
  
  /**
   * Content type classification
   */
  public type?: string | null;
  
  /**
   * Reference to file storage key
   */
  public fileKey: string | null | undefined;
  
  /**
   * Author of the content
   */
  public author!: string | null | undefined;
  
  /**
   * Content title
   */
  public title!: string | null | undefined;
  
  /**
   * Short description or summary
   */
  public description!: string | null | undefined;
  
  /**
   * Main content body text
   */
  public body!: string | null | undefined;
  
  /**
   * Date when content was published
   */
  public publish_date!: Date | null | undefined;
  
  /**
   * URL source of the content
   */
  public url!: string | null | undefined;
  
  /**
   * Original source identifier
   */
  public source!: string | null | undefined;
  
  /**
   * Publication status
   */
  public status!: 'published' | 'draft' | 'archived' | 'deleted' | null;
  
  /**
   * Content state flag
   */
  public state!: 'deprecated' | 'active' | 'highlighted' | null;
  
  /**
   * Creates a new Content instance
   * 
   * @param options - Content configuration options
   */
  constructor(options: ContentOptions) {
    super(options);
    this.options = options;
    this.type = options.type || null;
    this.name = options.title || '';
    this.author = options.author || null;
    this.title = options.title || '';
    this.description = options.description || null;
    this.body = options.body || '';
    this.publish_date = options.publish_date || null;
    this.fileKey = options.fileKey || null;
    this.url = options.url || null;
    this.source = options.source || null;
    this.status = options.status || 'draft';
    this.state = options.state || 'active';
  }

  /**
   * Creates and initializes a Content instance
   * 
   * @param options - Content configuration options
   * @returns Promise resolving to the initialized Content instance
   */
  static async create(options: ContentOptions) {
    if (!options.db) {
      options.db = {
        url: process.env.CONTENT_DB_URL || process.env.KISSD_DB_URL,
      };
    }
    const content = new Content(options);
    await content.initialize();
    return content;
  }

  /**
   * Initializes this content object
   * 
   * @returns Promise that resolves when initialization is complete
   */
  async initialize() {
    await super.initialize();
  }

  /**
   * Loads referenced content objects
   * 
   * @returns Promise that resolves when references are loaded
   */
  public async loadReferences() {}

  /**
   * Adds a reference to another content object
   * 
   * @param content - Content object or URL to reference
   * @returns Promise that resolves when the reference is added
   */
  public async addReference(content: Content | string) {
    if (typeof content === 'string') {
      content = await Content.create({
        db: this.options.db,
        ai: this.options.ai,
        url: content,
      });
    }
    this.references.push(content);
  }

  /**
   * Gets all referenced content objects
   * 
   * @returns Promise resolving to an array of referenced Content objects
   */
  public async getReferences() {
    return this.references;
  }

  /**
   * Converts this content object to a plain JSON object
   * 
   * @returns JSON representation of this content
   */
  public toJSON() {
    return {
      id: this.id || '',
      slug: this.slug || '',
      context: this.context || '',
      type: this.type,
      fileKey: this.fileKey || '',
      author: this.author || '',
      title: this.title || '',
      description: this.description || '',
      body: this.body || '',
      publish_date: this.publish_date || '',
      url: this.url || '',
      source: this.source || '',
      status: this.status || 'draft',
      state: this.state || 'active',
    };
  }
}
</file>

<file path="packages/smrt/src/contents.ts">
import type { AIClientOptions } from '@have/ai';
import type { BaseCollectionOptions } from './collection.js';
import YAML from 'yaml';
import path from 'path';
import { writeFile } from 'node:fs/promises';
import { ensureDirectoryExists } from '@have/files';
import { makeSlug } from '@have/utils';
import { BaseCollection } from './collection.js';
import { Content } from './content.js';
import { Document } from './document.js';

/**
 * Configuration options for Contents collection
 */
export interface ContentsOptions extends BaseCollectionOptions {
  /**
   * AI client configuration options
   */
  ai?: AIClientOptions;
  
  /**
   * Directory to store content files
   */
  contentDir?: string;
}

/**
 * Collection for managing Content objects
 * 
 * The Contents collection provides functionality for managing and manipulating
 * collections of Content objects, including saving to the filesystem and
 * mirroring content from remote URLs.
 */
export class Contents extends BaseCollection<Content> {
  /**
   * Class constructor for collection items
   */
  static _itemClass = Content;
  
  /**
   * Configuration options
   */
  public options: ContentsOptions = {} as ContentsOptions;
  
  /**
   * Sample content for reference
   */
  private exampleContent!: Content;
  
  /**
   * Map of loaded content objects
   */
  private loaded: Map<string, Content>;
  
  /**
   * Directory to store content files
   */
  public contentDir?: string;

  /**
   * Creates and initializes a Contents collection
   * 
   * @param options - Configuration options
   * @returns Promise resolving to the initialized Contents collection
   */
  static async create(options: ContentsOptions): Promise<Contents> {
    const contents = new Contents(options);
    await contents.initialize();
    return contents;
  }

  /**
   * Creates a new Contents collection
   * 
   * @param options - Configuration options
   */
  constructor(options: ContentsOptions) {
    super(options);
    this.options = options; //needed cause redeclare above i think ?
    this.loaded = new Map();
  }

  /**
   * Gets the database interface
   * 
   * @returns Database interface
   */
  getDb() {
    return this._db;
  }

  /**
   * Initializes the collection
   * 
   * @returns Promise that resolves when initialization is complete
   */
  public async initialize(): Promise<void> {
    await super.initialize();
  }

  /**
   * Mirrors content from a remote URL
   * 
   * Downloads and stores content from a remote URL, extracting text
   * and saving it as a Content object.
   * 
   * @param options - Mirror options
   * @param options.url - URL to mirror
   * @param options.mirrorDir - Directory for caching mirrored files
   * @param options.context - Context for the mirrored content
   * @returns Promise resolving to the mirrored Content object
   * @throws Error if URL is invalid or missing
   */
  public async mirror(options: {
    url: string;
    mirrorDir?: string;
    context?: string;
  }) {
    if (!options.url) {
      throw new Error('No URL provided');
    }
    let url: URL;
    try {
      // const url = new URL(options.url);
      // const existing = await this.db
      //   .oO`SELECT * FROM contents WHERE url = ${options.url}`;
      url = new URL(options.url); // validate url
    } catch (error) {
      console.error(error);
      throw new Error(`Invalid URL provided: ${options.url}`);
    }
    const existing = await this.get({ url: options.url });
    if (existing) {
      return existing;
    }

    const doc = await Document.create({
      cacheDir: options?.mirrorDir,
      url: options.url,
    });

    const filename = url.pathname.split('/').pop();
    const nameWithoutExtension = filename?.replace(/\.[^/.]+$/, '');
    const title = nameWithoutExtension?.replace(/[-_]/g, ' ');
    const slug = makeSlug(title as string);
    const body = await doc.getText();
    if (body) {
      const content = await Content.create({
        db: this.options.db,
        ai: this.options.ai,
        url: options.url,
        type: 'mirror',
        title,
        slug,
        context: options.context || '',
        body,
      });

      await content.save();
      return content;
    }
  }

  /**
   * Writes a Content object to the filesystem as a markdown file
   * 
   * @param options - Options for writing the content file
   * @param options.content - Content object to write
   * @param options.contentDir - Directory to write the file to
   * @returns Promise that resolves when the file is written
   * @throws Error if contentDir is not provided
   */
  public async writeContentFile(options: {
    content: Content;
    contentDir: string;
  }) {
    const { content, contentDir } = options;
    if (!contentDir) {
      throw new Error('No content dir provided');
    }

    const { body } = content;
    const frontMatter = {
      title: content.title,
      slug: content.slug,
      context: content.context,
      author: content.author,
      publish_date: content.publish_date,
    };

    let output = '';
    if (frontMatter && Object.keys(frontMatter).length > 0) {
      output += '---\n';
      output += YAML.stringify(frontMatter);
      output += '---\n';
    }

    // Format body as markdown if it's plain text
    let formattedBody = body || '';
    if (body && !this.isMarkdown(body)) {
      formattedBody = this.formatAsMarkdown(body);
    }
    output += formattedBody;

    const pathParts = [
      contentDir,
      content.context || '', // if empty, use empty string
      content.slug,
      'index.md',
    ].filter(Boolean); // remove empty strings

    const outputFile = path.join(...(pathParts as string[]));
    await ensureDirectoryExists(path.dirname(outputFile));
    await writeFile(outputFile, output);
  }

  /**
   * Checks if text appears to be in markdown format
   * 
   * @param text - Text to check
   * @returns Boolean indicating if the text contains markdown syntax
   */
  private isMarkdown(text: string): boolean {
    // Basic check for common markdown indicators
    const markdownIndicators = [
      /^#\s/m, // Headers
      /\*\*.+\*\*/, // Bold
      /\*.+\*/, // Italic
      /\[.+\]\(.+\)/, // Links
      /^\s*[-*+]\s/m, // Lists
      /^\s*\d+\.\s/m, // Numbered lists
      /```[\s\S]*```/, // Code blocks
      /^\s*>/m, // Blockquotes
    ];

    return markdownIndicators.some((indicator) => indicator.test(text));
  }

  /**
   * Formats plain text as simple markdown
   * 
   * @param text - Plain text to format
   * @returns Text formatted as basic markdown
   */
  private formatAsMarkdown(text: string): string {
    // Basic formatting of plain text to markdown
    return text
      .split(/\n\n+/)
      .map((paragraph) => paragraph.trim())
      .filter(Boolean)
      .join('\n\n');
  }

  /**
   * Synchronizes content to the filesystem
   * 
   * Writes all article-type Content objects to the filesystem
   * as markdown files.
   * 
   * @param options - Sync options
   * @param options.contentDir - Directory to write content files to
   * @returns Promise that resolves when synchronization is complete
   */
  public async syncContentDir(options: { contentDir?: string }) {
    const contentFilter = {
      type: 'article',
    };

    const contents = await this.list({ where: contentFilter });
    for (const content of contents) {
      await this.writeContentFile({
        content,
        contentDir: options.contentDir || this.options.contentDir || '',
      });
    }
  }

  // public async list(options: {
  //   where?: object;
  //   filter?: object;
  //   offset?: number;
  //   limit?: number;
  // }): Promise<Content[]> {
  //   const { where, filter, offset, limit } = options;

  //   const replacements: any[] = [];
  //   let currIndex = 1;

  //   let whereSql = '';
  //   if (where) {
  //     whereSql = 'where ';
  //     for (const [key, value] of Object.entries(where)) {
  //       whereSql += ` AND ${key} = $${currIndex++}`;
  //       replacements.push(value);
  //     }
  //   }

  //   let whereNotSql = '';
  //   if (filter) {
  //     if (whereSql) {
  //       whereNotSql = ' and ';
  //     } else {
  //       whereNotSql += ' where ';
  //     }
  //     for (const [key, value] of Object.entries(filter)) {
  //       whereNotSql += `${key} != $${currIndex++}`;
  //       replacements.push(value);
  //     }
  //   }

  //   const { rows } = await this._db.query(
  //     `SELECT * FROM contents ${whereSql} ${whereNotSql} LIMIT ${limit} OFFSET ${offset}`,
  //     replacements,
  //   );

  //   return Promise.all(rows.map((row: any) => this.create(row)));
  // }
}
</file>

<file path="packages/smrt/src/document.ts">
import os from 'os';
import path from 'path';
import { URL } from 'url';
import { FilesystemAdapter } from '@have/files';
import { downloadFileWithCache } from '@have/files';
import { extractTextFromPDF } from '@have/pdf';
import { getCached, setCached, getMimeType } from '@have/files';
import { makeSlug } from '@have/utils';

/**
 * Configuration options for Document
 */
export interface DocumentOptions {
  /**
   * Filesystem adapter for file operations
   */
  fs?: FilesystemAdapter;
  
  /**
   * Directory to use for caching files
   */
  cacheDir?: string;
  
  /**
   * URL or path to the document
   */
  url: string;
  
  /**
   * Local file path override
   */
  localPath?: string;
  
  /**
   * Document MIME type
   */
  type?: string | undefined | null;
}

/**
 * Handler for document files with text extraction capabilities
 * 
 * Document provides functionality for working with document files (like PDFs)
 * including downloading, caching, and extracting text content.
 */
export class Document {
  /**
   * Flag indicating if document is from a remote source
   */
  protected isRemote: boolean;
  
  /**
   * Configuration options
   */
  protected options: DocumentOptions;
  
  /**
   * Local file path where document is stored
   */
  protected localPath: string;
  
  /**
   * Directory used for caching files
   */
  protected cacheDir: string;
  
  /**
   * Document URL
   */
  public url: URL;
  
  /**
   * Document MIME type
   */
  public type: string | undefined | null;
  
  /**
   * Creates a new Document instance
   * 
   * @param options - Document configuration options
   */
  constructor(options: DocumentOptions) {
    this.options = options;
    this.url = new URL(options.url);

    this.type = options.type || getMimeType(this.url.toString());
    this.cacheDir =
      options.cacheDir || path.resolve(os.tmpdir(), '.cache', 'have-sdk');

    if (this.url.protocol.startsWith('file')) {
      this.localPath = this.url.pathname;
      this.isRemote = false;
    } else {
      this.localPath = path.join(
        this.cacheDir,
        makeSlug(this.url.hostname),
        this.url.pathname,
      );
      this.isRemote = true;
    }
  }

  /**
   * Creates and initializes a Document instance
   * 
   * @param options - Document configuration options
   * @returns Promise resolving to the initialized Document
   */
  static async create(options: DocumentOptions) {
    const document = new Document(options);
    await document.initialize();
    return document;
  }

  /**
   * Initializes the document, downloading it if it's remote
   * 
   * @returns Promise that resolves when initialization is complete
   */
  async initialize() {
    if (this.isRemote) {
      //todo: should be getCached?
      await downloadFileWithCache(this.url.toString(), this.localPath);
    }
  }

  /**
   * Extracts text content from the document
   * 
   * Currently supports PDF documents with planned support for other types.
   * Uses caching to avoid repeatedly processing the same document.
   * 
   * @returns Promise resolving to the extracted text content
   * @throws Error if the document type is not supported
   */
  async getText() {
    const cached = await getCached(this.localPath + '.extracted_text');
    if (cached) {
      return cached;
    }

    let extracted: string | null = '';
    switch (this.type) {
      case 'application/pdf':
        extracted = await extractTextFromPDF(this.localPath);
        break;
      case 'text':
      case 'json':
      default:
        throw new Error(
          'Getting text from ${this.type} types not yet implemented. I should check to see if its a text file here',
        );
    }
    if (extracted) {
      await setCached(this.localPath + '.extracted_text', extracted);
    }
    return extracted;
  }
}

export default {
  Document,
};
</file>

<file path="packages/smrt/src/fields.ts">
/*
this should work, but I want to get an initial commit in before refactoring



class Person extends BaseObject {
  name = new BaseText();
  age = new BaseReal();
  created_at = new BaseDate();
}

const person = new Person();
person.name = 'John';  // Works!
person.age = 30;      // Works!
person.created_at = new Date();  // Works!

console.log(`Hello ${person.name}`);  // "Hello John"


*/

/**
 * Base field class for object property typing
 * 
 * Provides a proxy-based approach to represent strongly-typed fields
 * in database objects.
 */
export class Field<T> {
  /**
   * The underlying field value
   */
  protected _value: T | null = null;

  /**
   * Creates a new Field instance
   * 
   * @param value - Initial field value
   * @returns Proxy-wrapped field instance
   */
  constructor(value: T | null = null) {
    this._value = value;
    return new Proxy(this, {
      set(target: any, prop: string, value: any) {
        if (prop === '_value') {
          target._value = value;
          return true;
        }
        target[prop] = value;
        return true;
      },
    });
  }

  /**
   * Gets the field value
   */
  get value(): T | null {
    return this._value;
  }

  /**
   * Converts the field value to a string
   * 
   * @returns String representation of the field value
   */
  toString() {
    return this._value?.toString() ?? '';
  }
}

/**
 * Text field type for string values
 */
export class TextField extends Field<string> {
  /**
   * SQL data type for this field
   */
  static readonly type = 'TEXT';
}

/**
 * Decimal field type for numeric values
 */
export class DecimalField extends Field<number> {
  /**
   * SQL data type for this field
   */
  static readonly type = 'REAL';
}

/**
 * Date field type for timestamp values
 */
export class DateField extends Field<Date> {
  /**
   * SQL data type for this field (stored as text in ISO format)
   */
  static readonly type = 'TEXT';
}
</file>

<file path="packages/smrt/src/human.ts">
/**
 * Configuration options for Human
 */
export interface HumanOptions {
  /**
   * Human's name
   */
  name?: string;
}

/**
 * Simple user/person representation
 * 
 * Human provides a lightweight representation of a person/user
 * with basic identification properties.
 */
export class Human {
  /**
   * Human's name
   */
  public name: string;
  
  /**
   * URL-friendly identifier
   */
  private _slug?: string;

  /**
   * Creates a new Human instance
   * 
   * @param options - Human configuration options
   */
  constructor(options: HumanOptions) {
    this.name = options.name || '';
  }

  /**
   * Initializes this Human object
   * 
   * @returns Promise resolving to this Human
   */
  protected async initialize(): Promise<Human> {
    // Perform any async initialization here
    return this;
  }

  /**
   * Creates and initializes a Human instance
   * 
   * @param options - Human configuration options
   * @returns Promise resolving to the initialized Human
   */
  static async create(options: HumanOptions): Promise<Human> {
    const person = new Human(options);
    return await person.initialize();
  }

  /**
   * Gets the URL-friendly slug for this human
   * 
   * @returns The slug string
   */
  get slug(): string {
    if (!this._slug) {
      // Implement slug generation logic directly here
      this._slug = ''; // TODO: Add actual slug generation
    }
    return this._slug;
  }
}
</file>

<file path="packages/smrt/src/index.ts">
/**
 * @have/smrt - Core library for building AI agents
 * 
 * This package provides standardized collections and objects for building 
 * vertical AI agents. It brings together multiple HAVE SDK packages 
 * (ai, files, pdf, spider, sql) to provide a unified interface.
 * 
 * Key components:
 * - BaseClass: Foundation class providing access to database, filesystem, and AI
 * - BaseObject: Persistent object with unique identifiers and database storage
 * - BaseCollection: Collection interface for managing sets of BaseObjects
 * - Content: Structured content object with metadata and body text
 * - Document: Handler for document files with text extraction capabilities
 * - Human: Simple user/person representation
 */

export * from './human.js';
export * from './document.js';
export * from './content.js';
export * from './contents.js';
export * from './class.js';
export * from './object.js';
export * from './collection.js';
export * from './pleb.js';
</file>

<file path="packages/smrt/src/object.ts">
import type { AIMessageOptions } from '@have/ai';
import type { BaseClassOptions } from './class.js';

import {
  fieldsFromClass,
  tableNameFromClass,
  setupTableFromClass,
} from '@have/smrt/utils';
import { escapeSqlValue } from '@have/sql';

import { BaseClass } from './class.js';
import { BaseCollection } from './collection.js';

/**
 * Options for BaseObject initialization
 */
export interface BaseObjectOptions extends BaseClassOptions {
  /**
   * Unique identifier for the object
   */
  id?: string;
  
  /**
   * Human-readable name for the object
   */
  name?: string;
  
  /**
   * URL-friendly identifier
   */
  slug?: string;
  
  /**
   * Optional context to scope the slug (could be a path, domain, etc.)
   */
  context?: string;
  
  /**
   * Creation timestamp
   */
  created_at?: Date;
  
  /**
   * Last update timestamp
   */
  updated_at?: Date;
}

/**
 * Base persistent object with unique identifiers and database storage
 * 
 * BaseObject provides functionality for creating, loading, and saving objects
 * to a database. It supports identification via unique IDs and URL-friendly
 * slugs, with optional context scoping.
 */
export class BaseObject<
  T extends BaseObjectOptions = BaseObjectOptions,
> extends BaseClass<T> {
  /**
   * Reference to the collection this object belongs to
   */
  public _collection!: BaseCollection<BaseObject<T>>;
  
  /**
   * Database table name for this object
   */
  public _tableName!: string;
  
  /**
   * Unique identifier for the object
   */
  protected _id: string | null | undefined;
  
  /**
   * URL-friendly identifier
   */
  protected _slug: string | null | undefined;
  
  /**
   * Optional context to scope the slug
   */
  protected _context: string | null | undefined;

  /**
   * Human-readable name, primarily for display purposes
   */
  public name: string | null | undefined;
  
  /**
   * Creation timestamp
   */
  public created_at: Date | null | undefined;
  
  /**
   * Last update timestamp
   */
  public updated_at: Date | null | undefined;

  /**
   * Creates a new BaseObject instance
   * 
   * @param options - Configuration options including identifiers and metadata
   * @throws Error if options is null
   */
  constructor(options: T) {
    super(options);
    if (options === null) {
      throw new Error('options cant be null');
    }
    this._id = options.id || null;
    this._slug = options.slug || null;
    this._context = options.context || '';
    this.name = options.name || null;
    this.created_at = options.created_at || null;
    this.updated_at = options.updated_at || null;
  }

  /**
   * Gets the unique identifier for this object
   */
  get id(): string | null | undefined {
    return this._id;
  }

  /**
   * Sets the unique identifier for this object
   * 
   * @param value - The ID to set
   * @throws Error if the value is invalid
   */
  set id(value: string | null | undefined) {
    if (!value || value === 'undefined' || value === 'null') {
      throw new Error(`id is required, ${value} given`);
    }
    this._id = value;
  }

  /**
   * Gets the URL-friendly slug for this object
   */
  get slug(): string | null | undefined {
    return this._slug;
  }

  /**
   * Sets the URL-friendly slug for this object
   * 
   * @param value - The slug to set
   * @throws Error if the value is invalid
   */
  set slug(value: string | null | undefined) {
    if (!value || value === 'undefined' || value === 'null') {
      throw new Error(`slug is invalid, ${value} given`);
    }

    this._slug = value;
  }

  /**
   * Gets the context that scopes this object's slug
   */
  get context(): string {
    return this._context || '';
  }

  /**
   * Sets the context that scopes this object's slug
   * 
   * @param value - The context to set
   * @throws Error if the value is invalid
   */
  set context(value: string | null | undefined) {
    if (value !== '' && !value) {
      throw new Error(`context is invalid, ${value} given`);
    }
    this._context = value;
  }

  /**
   * Initializes this object, setting up database tables and loading data if identifiers are provided
   * 
   * @returns Promise that resolves when initialization is complete
   */
  protected async initialize(): Promise<void> {
    await super.initialize();
    if (this.options.db) {
      await setupTableFromClass(this.db, this.constructor);
      await this.db.query(`
        CREATE UNIQUE INDEX IF NOT EXISTS idx_${this.tableName}_slug_context 
        ON ${this.tableName}(slug, context);
      `);
    }

    if (this.options.id) {
      await this.loadFromId();
    } else if (this.options.slug) {
      await this.loadFromSlug();
    }
  }

  /**
   * Loads data from a database row into this object's properties
   * 
   * @param data - Database row data
   */
  loadDataFromDb(data: any) {
    const fields = this.getFields();
    for (const field in fields) {
      if (fields.hasOwnProperty(field)) {
        this[field as keyof this] = data[field];
      }
    }
  }

  /**
   * Gets all property descriptors from this object's prototype
   * 
   * @returns Object containing all property descriptors
   */
  allDescriptors() {
    const proto = Object.getPrototypeOf(this);
    const descriptors = Object.getOwnPropertyDescriptors(proto);
    return descriptors;
  }

  /**
   * Gets the database table name for this object
   */
  get tableName() {
    if (!this._tableName) {
      this._tableName = tableNameFromClass(this.constructor);
    }
    return this._tableName;
  }

  /**
   * Gets field definitions and current values for this object
   * 
   * @returns Object containing field definitions with current values
   */
  getFields() {
    // Get the static fields definition from the class
    const fields = fieldsFromClass(
      this.constructor as new (...args: any[]) => any,
    );

    // Add current instance values to the fields
    for (const key in fields) {
      fields[key].value = this[key as keyof this];
    }

    return fields;
  }

  /**
   * Generates an SQL UPSERT statement for saving this object to the database
   * 
   * @returns SQL statement for inserting or updating this object
   */
  generateUpsertStatement() {
    const fields = this.getFields();
    const columns = ['id', 'slug', 'context'];
    const id = escapeSqlValue(this.id) || '';
    const slug = escapeSqlValue(this.slug);
    const context = escapeSqlValue(this.context || '');
    const values = [id, slug, context];
    const updates = [`slug = ${slug}`, `context = ${context}`];

    for (const [key, field] of Object.entries(fields)) {
      if (key === 'slug' || key === 'context') continue;
      columns.push(key);
      const value =
        typeof field.value === 'boolean' ? (field.value ? 1 : 0) : field.value;

      const escapedValue = escapeSqlValue(value);

      values.push(escapedValue);
      updates.push(`${key} = ${escapedValue}`);
    }

    // Use UPSERT syntax with explicit ON CONFLICT handling
    const sql = `
      INSERT INTO ${this.tableName} (${columns.join(', ')})
      VALUES (${values.join(', ')})
      ON CONFLICT(slug, context) 
      WHERE slug = ${slug} AND context = ${context}
      DO UPDATE SET
        ${updates.join(',\n        ')}
      WHERE ${this.tableName}.slug = ${slug} AND ${this.tableName}.context = ${context};
    `;

    return sql;
  }

  /**
   * Gets or generates a unique ID for this object
   * 
   * @returns Promise resolving to the object's ID
   */
  async getId() {
    // lookup by slug and context
    const saved = await this.db
      .pluck`SELECT id FROM ${this.tableName} WHERE slug = ${this.slug} AND context = ${this.context} LIMIT 1`;
    if (saved) {
      this.id = saved;
    }

    if (!this.id) {
      this.id = crypto.randomUUID();
    }
    return this.id;
  }

  /**
   * Gets or generates a slug for this object based on its name
   * 
   * @returns Promise resolving to the object's slug
   */
  async getSlug() {
    if (!this.slug && this.name) {
      // Generate slug from name if not set
      this.slug = this.name
        .toLowerCase()
        .replace(/[^a-z0-9]+/g, '-')
        .replace(/(^-|-$)/g, '');
    }

    // check for existing slug and make unique?
    return this.slug;
  }

  /**
   * Gets the ID of this object if it's already saved in the database
   * 
   * @returns Promise resolving to the saved ID or null if not saved
   */
  async getSavedId() {
    const { pluck } = this.db;
    const saved =
      await pluck`SELECT id FROM ${this.tableName} WHERE id = ${this.id} OR slug = ${this.slug} LIMIT 1`;
    return saved;
  }

  /**
   * Checks if this object is already saved in the database
   * 
   * @returns Promise resolving to true if saved, false otherwise
   */
  async isSaved() {
    const saved = await this.getSavedId();
    return !!saved;
  }

  /**
   * Saves this object to the database
   * 
   * @returns Promise resolving to this object
   */
  async save() {
    if (!this.id) {
      this.id = crypto.randomUUID();
    }

    if (!this.slug) {
      this.slug = await this.getSlug();
    }

    // Update the updated_at timestamp
    this.updated_at = new Date();

    if (!this.created_at) {
      this.created_at = new Date();
    }

    await setupTableFromClass(this.options.db, this.constructor);

    const sql = this.generateUpsertStatement();
    await this.db.query(sql);

    return this;
  }

  /**
   * Loads this object's data from the database using its ID
   * 
   * @returns Promise that resolves when loading is complete
   */
  public async loadFromId() {
    const {
      rows: [existing],
    } = await this.db.query(`SELECT * FROM ${this.tableName} WHERE id = ?`, [
      this.options.id,
    ]);
    if (existing) {
      this.loadDataFromDb(existing);
    }
  }

  /**
   * Loads this object's data from the database using its slug and context
   * 
   * @returns Promise that resolves when loading is complete
   */
  public async loadFromSlug() {
    const {
      rows: [existing],
    } = await this.db.query(
      `SELECT * FROM ${this.tableName} WHERE slug = ? AND context = ?`,
      [this.options.slug, this.options.context || ''],
    );
    if (existing) {
      this.loadDataFromDb(existing);
    }
  }

  /**
   * Evaluates whether this object meets given criteria using AI
   * 
   * @param criteria - Criteria to evaluate against
   * @param options - AI message options
   * @returns Promise resolving to true if criteria are met, false otherwise
   * @throws Error if the AI response is invalid
   */
  public async is(criteria: string, options: AIMessageOptions = {}) {
    const prompt = `--- Beginning of criteria ---\n${criteria}\n--- End of criteria ---\nDoes the content meet all the given criteria? Reply with a json object with a single boolean 'result' property`;
    const message = await this.ai.message(prompt, {
      ...(options as any),
      responseFormat: { type: 'json_object' },
    });
    try {
      const { result } = JSON.parse(message);
      if (result === true || result === false) {
        return result;
      }
    } catch (e) {
      throw new Error(`Unexpected answer: ${message}`);
    }
  }

  /**
   * Performs actions on this object based on instructions using AI
   * 
   * @param instructions - Instructions for the AI to follow
   * @param options - AI message options
   * @returns Promise resolving to the AI response
   */
  public async do(instructions: string, options: AIMessageOptions = {}) {
    const prompt = `--- Beginning of instructions ---\n${instructions}\n--- End of instructions ---\nBased on the content body, please follow the instructions and provide a response. Never make use of codeblocks.`;
    const result = await this.ai.message(prompt, options);
    return result;
  }
}

// async function ensureTriggersExist(db: any, tableName: string) {
//   const triggers = [
//     `${tableName}_set_created_at`,
//     `${tableName}_set_updated_at`,
//   ];

//   for (const trigger of triggers) {
//     const exists = await db.get(
//       `SELECT name FROM sqlite_master WHERE type='trigger' AND name=?`,
//       [trigger],
//     );

//     if (!exists) {
//       if (trigger === `${tableName}_set_created_at`) {
//         await db.exec(`
//           CREATE TRIGGER ${trigger}
//           AFTER INSERT ON ${tableName}
//           BEGIN
//             UPDATE ${tableName}
//             SET created_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
//             WHERE id = NEW.id;
//           END;
//         `);
//       } else if (trigger === `${tableName}_set_updated_at`) {
//         await db.exec(`
//           CREATE TRIGGER ${trigger}
//           AFTER UPDATE ON ${tableName}
//           BEGIN
//             UPDATE ${tableName}
//             SET updated_at = CURRENT_TIMESTAMP
//             WHERE id = NEW.id;
//           END;
//         `);
//       }
//     }
//   }
// }
</file>

<file path="packages/smrt/src/utils.ts">
import { syncSchema } from '@have/sql';
import yaml from 'yaml';
import { Content } from './content.js';

/**
 * Checks if a field name indicates a date field based on naming conventions
 * 
 * @param key - Field name to check
 * @returns Boolean indicating if the field is likely a date field
 */
export function isDateField(key: string) {
  return key.endsWith('_date') || key.endsWith('_at') || key === 'date';
}

/**
 * Converts a date string to a Date object
 * 
 * @param date - Date as string or Date object
 * @returns Date object
 */
export function dateAsString(date: Date | string) {
  if (typeof date === 'string') {
    return new Date(date);
  }
  return date;
}

/**
 * Converts a Date object to an ISO string
 * 
 * @param date - Date as Date object or string
 * @returns ISO date string or the original string
 */
export function dateAsObject(date: Date | string) {
  if (date instanceof Date) {
    return date.toISOString();
  }
  return date;
}

/**
 * Extracts field definitions from a class
 * 
 * @param ClassType - Class constructor to extract fields from
 * @param values - Optional values to set for the fields
 * @returns Object containing field definitions
 */
export function fieldsFromClass(
  ClassType: new (...args: any[]) => any,
  values?: Record<string, any>,
) {
  const fields: Record<string, any> = {};
  // just for introspection, dont need real creds
  const instance = new ClassType({
    ai: {
      type: 'openai',
      apiKey: 'sk-proj-1234567890',
    },
    db: {
      url: 'file:/tmp/dummy.db',
    },
  });

  // Get descriptors from the instance and all ancestors
  const descriptors = new Map<string, PropertyDescriptor>();

  // Start with the instance
  Object.entries(Object.getOwnPropertyDescriptors(instance)).forEach(
    ([key, descriptor]) => {
      descriptors.set(key, descriptor);
    },
  );

  // Walk up the prototype chain
  let proto = Object.getPrototypeOf(instance);
  while (proto && proto !== Object.prototype) {
    Object.entries(Object.getOwnPropertyDescriptors(proto)).forEach(
      ([key, descriptor]) => {
        // Only add if we haven't seen this property before
        if (!descriptors.has(key)) {
          descriptors.set(key, descriptor);
        }
      },
    );
    proto = Object.getPrototypeOf(proto);
  }

  // Process all collected descriptors
  for (const [key, descriptor] of descriptors) {
    // Skip methods, getters/setters, and internal properties
    if (
      typeof descriptor.value === 'function' ||
      descriptor.get ||
      descriptor.set ||
      key.startsWith('_') ||
      key.startsWith('#') ||
      key === 'constructor'
    ) {
      continue;
    }

    // If it's a data property with a defined type
    if (descriptor.value !== undefined) {
      let type: string | undefined;

      // Check the property definition
      const defaultValue = descriptor.value;
      if (defaultValue instanceof Date || isDateField(key)) {
        type = 'DATETIME';
      } else if (typeof defaultValue === 'string') {
        type = 'TEXT';
      } else if (typeof defaultValue === 'number') {
        type = 'INTEGER';
      } else if (defaultValue === null) {
        type = 'TEXT';
      }

      if (type) {
        fields[key] = {
          name: key,
          type,
          ...(values && key in values
            ? {
                value: values[key],
              }
            : {}),
        };
      }
    }
  }
  return fields;
}

/**
 * Generates a database schema SQL statement for a class
 * 
 * @param ClassType - Class constructor to generate schema for
 * @returns SQL schema creation statement
 */
export function generateSchema(ClassType: new (...args: any[]) => any) {
  const tableName = tableNameFromClass(ClassType);
  const fields = fieldsFromClass(ClassType);
  let schema = `CREATE TABLE IF NOT EXISTS ${tableName} (\n`;

  // Add id field first (always required)
  schema += '  id TEXT PRIMARY KEY,\n';

  // Add slug and context fields
  schema += '  slug TEXT NOT NULL,\n';
  schema += "  context TEXT NOT NULL DEFAULT '',\n";

  // Add other fields
  for (const [key, field] of Object.entries(fields)) {
    if (key === 'id' || key === 'slug' || key === 'context') continue;
    schema += `  ${key} ${field.type},\n`;
  }

  // Add composite unique constraint for slug and context
  schema += '  UNIQUE(slug, context),\n';

  schema = schema.slice(0, -2); // Remove trailing comma and newline
  schema += '\n);';

  schema += `\nCREATE INDEX IF NOT EXISTS ${tableName}_id_idx ON ${tableName} (id);`;
  schema += `\nCREATE INDEX IF NOT EXISTS ${tableName}_slug_context_idx ON ${tableName} (slug, context);`;
  return schema;
}

/**
 * Generates a table name from a class constructor
 * 
 * @param ClassType - Class constructor or function
 * @returns Pluralized snake_case table name
 */
export function tableNameFromClass(
  // eslint-disable-next-line @typescript-eslint/no-unsafe-function-type
  ClassType: Function | (new (...args: any[]) => any),
) {
  return (
    ClassType.name
      // Insert underscore between lower & upper case letters
      .replace(/([a-z])([A-Z])/g, '$1_$2')
      // Convert to lowercase
      .toLowerCase()
      // Handle basic pluralization rules
      .replace(/([^s])$/, '$1s')
      // Handle special cases ending in 'y'
      .replace(/y$/, 'ies')
  );
}

// export function escapeSqlValue(value: any): string {
//   if (value === null) {
//     return 'NULL';
//   }
//   if (value instanceof Date) {
//     return `'${value.toISOString()}'`;
//   }
//   if (typeof value === 'number') {
//     return value.toString();
//   }
//   if (typeof value === 'boolean') {
//     return value ? '1' : '0';
//   }
//   // Escape single quotes and wrap in quotes
//   return `'${String(value).replace(/'/g, "''")}'`;
// }

// function validateColumnName(column: string): string {
//   // Only allow alphanumeric characters, underscores, and dots (for table.column notation)
//   if (!/^[a-zA-Z0-9_.]+$/.test(column)) {
//     throw new Error(`Invalid column name: ${column}`);
//   }
//   return column;
// }

// export function addWhere({
//   sql,
//   replacements = [],
//   where = {},
//   required = true,
// }: {
//   sql: string;
//   replacements?: any[];
//   where?: object;
//   required?: boolean;
// }): { sql: string; replacements: any[] } {
//   const wheres = [];
//   for (const [key, value] of Object.entries(where)) {
//     const safeColumnName = validateColumnName(key);
//     wheres.push(`${safeColumnName} = $${replacements.length + 1}`);
//     replacements.push(value);
//   }

//   if (wheres.length > 0) {
//     sql += ` WHERE ${wheres.join(' AND ')}`;
//   } else if (required) {
//     throw new Error('WHERE clause is required but no conditions were provided');
//   }

//   return { sql, replacements };
// }

/**
 * Converts a class name to a table name with pluralization
 * 
 * @param className - Name of the class
 * @returns Pluralized snake_case table name
 */
export function classnameToTablename(className: string) {
  // Convert camelCase/PascalCase to snake_case and pluralize
  const tableName = className
    // Insert underscore between lower & upper case letters
    .replace(/([a-z])([A-Z])/g, '$1_$2')
    // Convert to lowercase
    .toLowerCase()
    // Handle basic pluralization rules
    .replace(/([^s])$/, '$1s')
    // Handle special cases ending in 'y'
    .replace(/y$/, 'ies');

  return tableName;
}

/**
 * Cache of table setup promises to avoid duplicate setup operations
 */
const _setup_table_from_class_promises: Record<string, Promise<void> | null> =
  {};

/**
 * Sets up database tables for a class
 * 
 * @param db - Database connection
 * @param ClassType - Class constructor to create tables for
 * @returns Promise that resolves when setup is complete
 */
export async function setupTableFromClass(db: any, ClassType: any) {
  const tableName = classnameToTablename(ClassType.name);

  if (_setup_table_from_class_promises[tableName] !== undefined || null) {
    return _setup_table_from_class_promises[tableName];
  }

  _setup_table_from_class_promises[tableName] = (async () => {
    try {
      const schema = generateSchema(ClassType);
      await syncSchema({ db, schema });
      await setupTriggers(db, tableName);
    } catch (error) {
      _setup_table_from_class_promises[tableName] = null; // Allow retry on failure
      throw error;
    }
  })();

  return _setup_table_from_class_promises[tableName];
}

/**
 * Sets up database triggers for automatic timestamp updates
 * 
 * @param db - Database connection
 * @param tableName - Name of the table to set up triggers for
 * @returns Promise that resolves when triggers are set up
 */
export async function setupTriggers(db: any, tableName: string) {
  const triggers = [
    `${tableName}_set_created_at`,
    `${tableName}_set_updated_at`,
  ];

  for (const trigger of triggers) {
    const exists =
      await db.pluck`SELECT name FROM sqlite_master WHERE type='trigger' AND name=${trigger}`;
    if (!exists) {
      if (trigger === `${tableName}_set_created_at`) {
        const createTriggerSQL = `
          CREATE TRIGGER ${trigger}
          AFTER INSERT ON ${tableName}
          BEGIN
            UPDATE ${tableName} 
            SET created_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP 
            WHERE id = NEW.id;
          END;
        `;
        await db.query(createTriggerSQL);
      } else if (trigger === `${tableName}_set_updated_at`) {
        const createTriggerSQL = `
          CREATE TRIGGER ${trigger}
          AFTER UPDATE ON ${tableName}
          BEGIN
            UPDATE ${tableName} 
            SET updated_at = CURRENT_TIMESTAMP 
            WHERE id = NEW.id;
          END;
        `;
        await db.query(createTriggerSQL);
      }
    }
  }
}

/**
 * Converts a Content object to a string with YAML frontmatter
 * 
 * @param content - Content object to convert
 * @returns String with YAML frontmatter and body content
 */
export function contentToString(content: Content) {
  const { body, ...frontmatter } = content;
  const separator = '---';
  const frontmatterYAML = yaml.stringify(frontmatter);
  return `${separator}\n${frontmatterYAML}\n${separator}\n${body}`;
}

/**
 * Converts a string with YAML frontmatter to a Content object
 * 
 * @param data - String with YAML frontmatter and body content
 * @returns Object with parsed frontmatter and body content
 */
export function stringToContent(data: string) {
  const separator = '---';
  const frontmatterStart = data.indexOf(separator);

  let frontmatter = {};
  let body = data;

  if (frontmatterStart !== -1) {
    const frontmatterEnd = data.indexOf(
      separator,
      frontmatterStart + separator.length,
    );

    if (frontmatterEnd !== -1) {
      const frontmatterYAML = data
        .substring(frontmatterStart + separator.length, frontmatterEnd)
        .trim();
      frontmatter = yaml.parse(frontmatterYAML) || {}; // Handle potential YAML parsing errors
      body = data.substring(frontmatterEnd + separator.length).trim();
    }
  }

  return formatDataJs({
    ...frontmatter,
    body,
  });
}

/**
 * Formats data for JavaScript by converting date strings to Date objects
 * 
 * @param data - Object with data to format
 * @returns Object with properly typed values for JavaScript
 */
export function formatDataJs(data: Record<string, any>) {
  const normalizedData: Record<string, any> = {};
  for (const [key, value] of Object.entries(data)) {
    if (value instanceof Date) {
      normalizedData[key] = value;
    } else if (isDateField(key) && typeof value === 'string') {
      normalizedData[key] = new Date(value);
    } else {
      normalizedData[key] = value;
    }
  }
  return normalizedData;
}

/**
 * Formats data for SQL by converting Date objects to ISO strings
 * 
 * @param data - Object with data to format
 * @returns Object with properly formatted values for SQL
 */
export function formatDataSql(data: Record<string, any>) {
  const normalizedData: Record<string, any> = {};
  for (const [key, value] of Object.entries(data)) {
    if (value instanceof Date) {
      normalizedData[key] = value.toISOString(); // Postgres accepts ISO format with timezone
    } else {
      normalizedData[key] = value;
    }
  }
  return normalizedData;
}
</file>

<file path="packages/smrt/README.md">
# @have/smrt

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Core library for building AI agents with standardized collections and objects in the HAVE SDK.

## Overview

The `@have/smrt` package provides the foundation for building vertical AI agents. It integrates all other HAVE SDK packages into a cohesive framework with standardized collections, object models, and persistence capabilities.

## Features

- Agent framework with built-in AI model integration
- Object-relational mapping with automatic database persistence
- Standardized collections with CRUD operations
- Context-aware object identification (by id, slug, name, or title)
- Built-in support for database operations, file handling, and web interactions
- Extensible plugin architecture
- Efficient resource management

## Installation

```bash
# Install with npm
npm install @have/smrt

# Or with yarn
yarn add @have/smrt

# Or with pnpm
pnpm add @have/smrt
```

## Usage

### Creating an Agent

```typescript
import { Agent } from '@have/smrt';
import { OpenAIModel } from '@have/ai';

// Create a new agent
const agent = new Agent({
  model: new OpenAIModel({ 
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4-turbo'
  }),
  tools: [
    // Add tools as needed
  ]
});

// Run a task with the agent
const result = await agent.run('Research the latest developments in quantum computing');
console.log(result);
```

### Working with Smart Objects

```typescript
import { SmartObject, initializeDatabase } from '@have/smrt';

// Initialize the database
await initializeDatabase({
  file: 'my-database.sqlite', // Use SQLite
  // Or for PostgreSQL:
  // host: 'localhost',
  // port: 5432,
  // database: 'my_database',
  // user: 'username',
  // password: 'password'
});

// Define a smart object class
class Article extends SmartObject {
  title: string;
  content: string;
  author: string;
  publishedDate: Date;
  
  constructor(data: Partial<Article>) {
    super();
    Object.assign(this, data);
  }
}

// Create and save an article
const article = new Article({
  title: 'Understanding TypeScript',
  content: 'TypeScript is a superset of JavaScript that adds...',
  author: 'Jane Doe',
  publishedDate: new Date()
});

await article.save();

// Retrieve an article by title
const retrievedArticle = await Article.findByTitle('Understanding TypeScript');
console.log(retrievedArticle);

// Or by id
const articleById = await Article.findById(article.id);
console.log(articleById);
```

### Using Collections

```typescript
import { Collection } from '@have/smrt';
import { Article } from './article';

// Create a collection of articles
const articles = new Collection<Article>('articles');

// Add filtering capabilities
const recentArticles = await articles.find({
  author: 'Jane Doe',
  publishedDate: { $gt: new Date('2023-01-01') }
});

// Sort articles
const sortedArticles = await articles.find({}, {
  sort: { publishedDate: 'desc' },
  limit: 10
});

console.log(sortedArticles);
```

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_smrt.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/smrt/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="packages/spider/README.md">
# @have/spider

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Web crawling and content parsing tools for the HAVE SDK.

## Overview

The `@have/spider` package provides tools for crawling websites, extracting structured data, and parsing web content. It offers a simple and consistent API for web scraping tasks with built-in rate limiting, caching, and error handling.

## Features

- Web page crawling with customizable depth and breadth
- Content extraction with semantic understanding
- HTML parsing and DOM manipulation
- Table and list detection and extraction
- Automatic handling of pagination
- Rate limiting and respectful crawling
- Browser automation capabilities
- Response caching
- URL normalization and filtering

## Installation

```bash
# Install with npm
npm install @have/spider

# Or with yarn
yarn add @have/spider

# Or with pnpm
pnpm add @have/spider
```

## Usage

### Basic Web Scraping

```typescript
import { Spider } from '@have/spider';

// Create a new spider instance
const spider = new Spider();

// Fetch and parse a web page
const page = await spider.fetch('https://example.com');
console.log(page.title);
console.log(page.text);

// Extract structured content
const content = page.extract({
  title: '.page-title',
  description: 'meta[name="description"]',
  articles: {
    selector: 'article',
    multiple: true,
    extract: {
      title: 'h2',
      summary: '.summary',
      link: {
        selector: 'a.read-more',
        attr: 'href'
      }
    }
  }
});

console.log(content);
```

### Crawling Multiple Pages

```typescript
import { Spider } from '@have/spider';

// Create a spider with configuration
const spider = new Spider({
  maxConcurrent: 5,
  rateLimit: 1000, // 1 request per second
  userAgent: 'MyBot/1.0',
  timeout: 30000
});

// Crawl multiple pages starting from a URL
const results = await spider.crawl('https://example.com', {
  maxDepth: 2,
  followLinks: true,
  patterns: {
    allow: [/^https:\/\/example\.com\/blog\//],
    disallow: [/\/tag\//, /\/category\//]
  },
  extract: {
    title: 'h1',
    content: '.article-content',
    date: {
      selector: '.published-date',
      transform: (text) => new Date(text)
    }
  }
});

console.log(`Crawled ${results.length} pages`);
console.log(results);
```

### Using Headless Browser

```typescript
import { BrowserSpider } from '@have/spider';

// Create a spider with browser automation
const spider = await BrowserSpider.create({
  headless: true,
  // Additional browser options
});

// Navigate and interact with a page
await spider.goto('https://example.com/login');
await spider.type('#username', 'myusername');
await spider.type('#password', 'mypassword');
await spider.click('#login-button');
await spider.waitForNavigation();

// Extract content after interaction
const content = await spider.extract({
  title: 'h1',
  userInfo: '.user-profile',
  dashboard: {
    selector: '.dashboard-stats',
    extract: {
      visits: '.stat-visits',
      conversions: '.stat-conversions'
    }
  }
});

console.log(content);

// Close the browser when done
await spider.close();
```

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_spider.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/spider/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="packages/sql/src/index.ts">
import type { DatabaseInterface } from "./types.js";
import type { PostgresOptions } from "./postgres.js";
import type { SqliteOptions } from "./sqlite.js";

/**
 * Union type of options for creating different database types
 */
type GetDatabaseOptions =
  | (PostgresOptions & { type?: "postgres" })
  | (SqliteOptions & { type?: "sqlite" });

/**
 * Creates a database connection based on the provided options
 * 
 * @param options - Configuration options for the database connection
 * @returns Promise resolving to a DatabaseInterface implementation
 * @throws Error if the database type is invalid
 */
export async function getDatabase(
  options: GetDatabaseOptions = {},
): Promise<DatabaseInterface> {
  // if no type but url starts with file:, set to sqlite
  if (
    !options.type &&
    (options.url?.startsWith("file:") || options.url === ":memory:")
  ) {
    options.type = "sqlite";
  }

  if (options.type === "postgres") {
    const postgres = await import("./postgres.js");
    return postgres.getDatabase(options as PostgresOptions);
  } else if (options.type === "sqlite") {
    const sqlite = await import("./sqlite.js");
    return sqlite.getDatabase(options as SqliteOptions);
  } else {
    throw new Error("Invalid database type");
  }
}

/**
 * Validates if a table name consists only of alphanumeric characters and underscores
 * 
 * @param name - Table name to validate
 * @returns Boolean indicating if the name is valid
 */
function isValidTableName(name: string): boolean {
  // Simple regex to allow only alphanumeric characters and underscores
  return /^[a-zA-Z0-9_]+$/.test(name);
}

/**
 * Synchronizes a SQL schema definition with a database
 * Creates tables if they don't exist and adds missing columns to existing tables
 * 
 * @param options - Object containing database and schema
 * @param options.db - Database interface to use
 * @param options.schema - SQL schema definition
 * @throws Error if db or schema are missing or if table name is invalid
 */
export async function syncSchema(options: {
  db: DatabaseInterface;
  schema: string;
}) {
  const { db, schema } = options;
  if (!db || !schema) {
    throw new Error("db and schema are required");
  }
  const commands = schema
    .trim()
    .split(";")
    .filter((command) => command.trim() !== "");

  for (const command of commands) {
    const createTableRegex =
      /CREATE TABLE (IF NOT EXISTS )?(\w+) \(([\s\S]+)\)/i;
    const match = command.match(createTableRegex);

    if (match) {
      const tableName = match[2];
      const columns = match[3].trim().split(",\n");

      if (!isValidTableName(tableName)) {
        throw new Error("Invalid table name");
      }
      const tableExists =
        !!(await db.pluck`SELECT name FROM sqlite_master WHERE type='table' AND name=${tableName}`);
      if (!tableExists) {
        await db.query(command);
      } else {
        // 3. Check for column differences
        for (const column of columns) {
          const columnDef = column.trim();
          const [, columnName, columnType] =
            columnDef.match(/(\w+) (\w+)(.*)/) || [];

          if (columnName && columnType) {
            try {
              // Check if the column exists and has the correct type
              const columnInfo = await db.oO`
                SELECT *
                FROM pragma_table_info(${tableName})
                WHERE name = ${columnName}
              `;

              if (!columnInfo || columnInfo.length === 0) {
                // Column doesn't exist or has an incorrect type, apply changes
                const alterCommand = `ALTER TABLE ${tableName} ADD COLUMN ${columnDef};`;
                await db.query(alterCommand);
              }
            } catch (error) {
              // Column doesn't exist or has an incorrect type, apply changes
              const alterCommand = `ALTER TABLE ${tableName} ADD COLUMN ${columnDef};`;
              await db.query(alterCommand);
            }
          }
        }
      }
    }
  }
}

/**
 * Checks if a table exists in the database
 * 
 * @param db - Database interface to use
 * @param tableName - Name of the table to check
 * @returns Promise resolving to boolean indicating if the table exists
 */
export async function tableExists(db: DatabaseInterface, tableName: string) {
  const tableExists =
    await db.pluck`SELECT name FROM sqlite_master WHERE type='table' AND name='${tableName}'`;
  return !!tableExists;
}

/**
 * Escapes and formats a value for use in SQL queries
 * 
 * @param value - Value to escape
 * @returns String representation of the value safe for SQL use
 */
export function escapeSqlValue(value: any): string {
  if (value === null) {
    return "NULL";
  }
  if (value instanceof Date) {
    return `'${value.toISOString()}'`;
  }
  if (typeof value === "number") {
    return value.toString();
  }
  if (typeof value === "boolean") {
    return value ? "1" : "0";
  }
  // Escape single quotes and wrap in quotes
  return `'${String(value).replace(/'/g, "''")}'`;
}

/**
 * Validates a column name for use in SQL queries
 * 
 * @param column - Column name to validate
 * @returns The validated column name
 * @throws Error if the column name contains invalid characters
 */
export function validateColumnName(column: string): string {
  // Only allow alphanumeric characters, underscores, and dots (for table.column notation)
  if (!/^[a-zA-Z0-9_.]+$/.test(column)) {
    throw new Error(`Invalid column name: ${column}`);
  }
  return column;
}

/**
 * Map of valid SQL operators for use in WHERE clauses
 */
const VALID_OPERATORS = {
  "=": "=",
  ">": ">",
  ">=": ">=",
  "<": "<",
  "<=": "<=",
  "!=": "!=",
  like: "LIKE",
  in: "IN",
} as const;

/**
 * Builds a SQL WHERE clause with parameterized values and flexible operators
 *
 * @param where - Record of conditions with optional operators in keys
 * @param startIndex - Starting index for parameter numbering (default: 1)
 * @returns Object containing the SQL clause and array of values
 *
 * @example Basic Usage:
 * ```typescript
 * buildWhere({
 *   'status': 'active',           // equals operator is default
 *   'price >': 100,              // greater than
 *   'stock <=': 5,               // less than or equal
 *   'category in': ['A', 'B'],   // IN clause for arrays
 *   'name like': '%shirt%'       // LIKE for pattern matching
 * });
 * ```
 *
 * @example NULL Handling:
 * ```typescript
 * buildWhere({
 *   'deleted_at': null,          // becomes "deleted_at IS NULL"
 *   'updated_at !=': null,       // becomes "updated_at IS NOT NULL"
 *   'status': 'active'           // regular comparison
 * });
 * ```
 *
 * @example Common Patterns:
 * ```typescript
 * // Price range
 * buildWhere({
 *   'price >=': 10,
 *   'price <': 100
 * });
 *
 * // Date filtering
 * buildWhere({
 *   'created_at >': startDate,
 *   'created_at <=': endDate,
 *   'deleted_at': null
 * });
 *
 * // Search with LIKE
 * buildWhere({
 *   'title like': '%search%',
 *   'description like': '%search%',
 *   'status': 'published'
 * });
 *
 * // Multiple values with IN
 * buildWhere({
 *   'role in': ['admin', 'editor'],
 *   'active': true,
 *   'last_login !=': null
 * });
 * ```
 *
 * The function handles:
 * - Standard comparisons (=, >, >=, <, <=, !=)
 * - NULL checks (IS NULL, IS NOT NULL)
 * - IN clauses for arrays
 * - LIKE for pattern matching
 * - Multiple conditions combined with AND
 */
export const buildWhere = (where: Record<string, any>, startIndex = 1) => {
  let sql = "";
  const values: any[] = [];
  let currIndex = startIndex;

  if (where && Object.keys(where).length > 0) {
    sql = "WHERE ";
    for (const [fullKey, value] of Object.entries(where)) {
      const [field, operator = "="] = fullKey.split(" ");
      const sqlOperator =
        VALID_OPERATORS[operator as keyof typeof VALID_OPERATORS] || "=";

      if (sql !== "WHERE ") {
        sql += " AND ";
      }

      if (value === null) {
        sql += `${field} IS ${sqlOperator === "=" ? "NULL" : "NOT NULL"}`;
      } else {
        sql += `${field} ${sqlOperator} $${currIndex++}`;
        values.push(value);
      }
    }
  }

  return { sql, values };
};

export * from "./types.js";

export default { getDatabase, syncSchema, tableExists, buildWhere };
</file>

<file path="packages/sql/src/postgres.ts">
import { Pool, PoolClient, QueryResult } from "pg";

/**
 * Configuration options for PostgreSQL database connections
 */
export interface PostgresOptions {
  /**
   * Connection URL for PostgreSQL
   */
  url?: string;
  
  /**
   * Database name
   */
  database?: string;
  
  /**
   * Database server hostname
   */
  host?: string;
  
  /**
   * Username for authentication
   */
  user?: string;
  
  /**
   * Password for authentication
   */
  password?: string;
  
  /**
   * Port number for the PostgreSQL server
   */
  port?: number;
}

/**
 * Result of a database operation that modifies data
 */
interface QueryResponse {
  /**
   * Type of operation performed (e.g., "insert", "update", "delete")
   */
  operation: string;
  
  /**
   * Number of rows affected by the operation
   */
  affected: number;
}

/**
 * Interface for table-specific operations
 */
interface TableMethods {
  /**
   * Inserts one or more records into the table
   * 
   * @param data - Single record or array of records to insert
   * @returns Promise resolving to operation result
   */
  insert: (
    data: Record<string, any> | Record<string, any>[],
  ) => Promise<QueryResponse>;
  
  /**
   * Retrieves a single record from the table matching the where criteria
   * 
   * @param data - Criteria to match records
   * @returns Promise resolving to query result
   */
  get: (data: Record<string, any>) => Promise<QueryResult>;
  
  /**
   * Retrieves multiple records from the table matching the where criteria
   * 
   * @param data - Criteria to match records
   * @returns Promise resolving to array of records
   */
  list: (data: Record<string, any>) => Promise<any[]>;
}

/**
 * Creates a PostgreSQL database adapter
 * 
 * @param options - PostgreSQL connection options
 * @returns Database interface for PostgreSQL
 */
export function getDatabase(options: PostgresOptions = {}) {
  const {
    url = process.env.SQLOO_URL,
    database = process.env.SQLOO_DATABASE,
    host = process.env.SQLOO_HOST || "localhost",
    user = process.env.SQLOO_USER,
    password = process.env.SQLOO_PASSWORD,
    port = Number(process.env.SQLOO_PORT) || 5432,
  } = options;

  // Create a connection pool
  const client = new Pool(
    url
      ? { connectionString: url }
      : {
          host,
          user,
          password,
          port,
          database,
        },
  );

  /**
   * Inserts data into a table and returns the operation result
   * 
   * @param table - Table name
   * @param data - Single record or array of records to insert
   * @returns Promise resolving to operation result
   */
  const insert = async (
    table: string,
    data: Record<string, any> | Record<string, any>[],
  ): Promise<QueryResponse> => {
    // If data is an array, we need to handle multiple rows
    if (Array.isArray(data)) {
      const keys = Object.keys(data[0]);
      const placeholders = data
        .map(
          (_, i) =>
            `(${keys.map((_, j) => `$${i * keys.length + j + 1}`).join(", ")})`,
        )
        .join(", ");
      const query = `INSERT INTO ${table} (${keys.join(
        ", ",
      )}) VALUES ${placeholders}`;
      const values = data.reduce(
        (acc, row) => acc.concat(Object.values(row)),
        [],
      );
      const result = await client.query(query, values);
      return { operation: "insert", affected: result.rowCount ?? 0 };
    } else {
      // If data is an object, we handle a single row
      const keys = Object.keys(data);
      const values = Object.values(data);
      const placeholders = keys.map((_, i) => `$${i + 1}`).join(", ");
      const query = `INSERT INTO ${table} (${keys.join(
        ", ",
      )}) VALUES (${placeholders})`;
      const result = await client.query(query, values);
      return { operation: "insert", affected: result.rowCount ?? 0 };
    }
  };

  /**
   * Retrieves a single record matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records
   * @returns Promise resolving to query result
   */
  const get = async (
    table: string,
    where: Record<string, any>,
  ): Promise<QueryResult> => {
    const keys = Object.keys(where);
    const values = Object.values(where);
    const whereClause = keys
      .map((key, i) => `${key} = $${i + 1}`)
      .join(" AND ");
    const query = `SELECT * FROM ${table} WHERE ${whereClause}`;
    return client.query(query, values);
  };

  /**
   * Retrieves multiple records matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records
   * @returns Promise resolving to array of records
   */
  const list = async (
    table: string,
    where: Record<string, any>,
  ): Promise<any[]> => {
    const keys = Object.keys(where);
    const values = Object.values(where);
    const whereClause = keys
      .map((key, i) => `${key} = $${i + 1}`)
      .join(" AND ");
    const query = `SELECT * FROM ${table} WHERE ${whereClause}`;
    const result = await client.query(query, values);
    return result.rows;
  };

  /**
   * Updates records matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records to update
   * @param data - New data to set
   * @returns Promise resolving to operation result
   */
  const update = async (
    table: string,
    where: Record<string, any>,
    data: Record<string, any>,
  ): Promise<QueryResponse> => {
    const keys = Object.keys(data);
    const values = Object.values(data);
    const setClause = keys.map((key, i) => `${key} = $${i + 1}`).join(", ");
    const whereKeys = Object.keys(where);
    const whereValues = Object.values(where);
    const whereClause = whereKeys
      .map((key, i) => `${key} = $${i + 1 + values.length}`)
      .join(" AND ");

    const sql = `UPDATE ${table} SET ${setClause} WHERE ${whereClause}`;
    const result = await client.query(sql, [...values, ...whereValues]);
    return { operation: "update", affected: result.rowCount ?? 0 };
  };

  /**
   * Gets a record matching the where criteria or inserts it if not found
   * 
   * @param table - Table name
   * @param where - Criteria to match existing record
   * @returns Promise resolving to the query result or insert result
   */
  const getOrInsert = async (
    table: string,
    where: Record<string, any>,
  ): Promise<QueryResult | QueryResponse> => {
    const result = await get(table, where);
    if (result) return result;
    return insert(table, where);
  };

  /**
   * Creates a table-specific interface for simplified table operations
   * 
   * @param tableName - Table name
   * @returns TableMethods interface for the specified table
   */
  const table = (tableName: string): TableMethods => {
    return {
      insert: (data) => insert(tableName, data),
      get: (data) => get(tableName, data),
      list: (data) => list(tableName, data),
    };
  };

  /**
   * Template and values extracted from a tagged template literal
   */
  interface SqlTemplate {
    /**
     * SQL query with parameter placeholders
     */
    sql: string;
    
    /**
     * Values to use as parameters
     */
    values: any[];
  }

  /**
   * Parses a tagged template literal into a SQL query and values
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Object with SQL query and values array
   */
  const parseTemplate = (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): SqlTemplate => {
    let sql = strings[0];
    const values = [];
    for (let i = 0; i < vars.length; i++) {
      values.push(vars[i]);
      sql += "$" + (i + 1) + strings[i + 1];
    }
    return { sql, values };
  };

  /**
   * Executes a SQL query using template literals and returns a single value
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to a single value (first column of first row)
   */
  const pluck = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<any> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    const result = await client.query(sql, values);
    return result.rows[0][0];
  };

  /**
   * Executes a SQL query using template literals and returns a single row
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to a single result record or null
   */
  const single = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<Record<string, any> | null> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    const result = await client.query(sql, values);
    return result.rows[0];
  };

  /**
   * Executes a SQL query using template literals and returns multiple rows
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to array of result records
   */
  const many = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<Record<string, any>[]> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    const { rows } = await client.query(sql, values);
    return rows;
  };

  /**
   * Executes a SQL query using template literals without returning results
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise that resolves when the query completes
   */
  const execute = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<void> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    await client.query(sql, values);
  };

  /**
   * Executes a raw SQL query with parameterized values
   * 
   * @param sql - SQL query string
   * @param values - Variables to use as parameters
   * @returns Promise resolving to query result with rows and count
   */
  const query = async (
    sql: string,
    values: any[],
  ): Promise<{ rows: Record<string, any>[]; rowCount: number }> => {
    const result = await client.query(sql, values);
    return {
      rows: result.rows,
      rowCount: result.rowCount ?? 0,
    };
  };

  /**
   * Checks if a table exists in the database
   * 
   * @param tableName - Name of the table to check
   * @returns Promise resolving to boolean indicating if the table exists
   */
  const tableExists = async (tableName: string): Promise<boolean> => {
    const result = await client.query(
      `SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = $1)`,
      [tableName],
    );
    return result.rows[0].exists;
  };
  
  // Shorthand aliases for query methods
  const oo = many;   // (o)bjective-(o)bjects: returns multiple rows
  const oO = single; // (o)bjective-(O)bject: returns a single row
  const ox = pluck;  // (o)bjective-(x): returns a single value
  const xx = execute; // (x)ecute-(x)ecute: executes without returning

  return {
    client,
    insert,
    update,
    get,
    getOrInsert,
    list,
    table,
    many,
    single,
    pluck,
    execute,
    query,
    oo,
    oO,
    ox,
    xx,
    tableExists,
  };
}
</file>

<file path="packages/sql/src/types.ts">
/**
 * Common database connection options
 */
export interface DatabaseOptions {
  /**
   * Database connection URL
   */
  url?: string;
  
  /**
   * Authentication token for the database connection
   */
  authToken?: string;
}

/**
 * Result of a database operation that modifies data
 */
export interface QueryResult {
  /**
   * Type of operation performed (e.g., "insert", "update", "delete")
   */
  operation: string;
  
  /**
   * Number of rows affected by the operation
   */
  affected: number;
}

/**
 * Common interface for database adapters
 * Provides a unified API for different database backends
 */
export interface DatabaseInterface {
  /**
   * Underlying database client instance
   */
  client: any;
  
  /**
   * Inserts one or more records into a table
   * 
   * @param table - Table name
   * @param data - Single record or array of records to insert
   * @returns Promise resolving to operation result
   */
  insert: (
    table: string,
    data: Record<string, any> | Record<string, any>[],
  ) => Promise<QueryResult>;
  
  /**
   * Retrieves a single record matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records
   * @returns Promise resolving to matching record or null if not found
   */
  get: (
    table: string,
    where: Record<string, any>,
  ) => Promise<Record<string, any> | null>;
  
  /**
   * Retrieves multiple records matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records
   * @returns Promise resolving to array of matching records
   */
  list: (
    table: string,
    where: Record<string, any>,
  ) => Promise<Record<string, any>[]>;
  
  /**
   * Updates records matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records to update
   * @param data - New data to set
   * @returns Promise resolving to operation result
   */
  update: (
    table: string,
    where: Record<string, any>,
    data: Record<string, any>,
  ) => Promise<QueryResult>;
  
  /**
   * Gets a record matching the where criteria or inserts it if not found
   * 
   * @param table - Table name
   * @param where - Criteria to match existing record
   * @param data - Data to insert if no record found
   * @returns Promise resolving to the record (either retrieved or newly inserted)
   */
  getOrInsert: (
    table: string,
    where: Record<string, any>,
    data: Record<string, any>,
  ) => Promise<Record<string, any>>;
  
  /**
   * Creates a table-specific interface for simplified table operations
   * 
   * @param table - Table name
   * @returns TableInterface for the specified table
   */
  table: (table: string) => TableInterface;
  
  /**
   * Checks if a table exists in the database
   * 
   * @param table - Table name
   * @returns Promise resolving to boolean indicating existence
   */
  tableExists: (table: string) => Promise<boolean>;
  
  /**
   * Executes a SQL query using template literals and returns multiple rows
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to array of result records
   */
  many: (
    strings: TemplateStringsArray,
    ...vars: any[]
  ) => Promise<Record<string, any>[]>;
  
  /**
   * Executes a SQL query using template literals and returns a single row
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to a single result record or null
   */
  single: (
    strings: TemplateStringsArray,
    ...vars: any[]
  ) => Promise<Record<string, any> | null>;
  
  /**
   * Executes a SQL query using template literals and returns a single value
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to a single value (first column of first row)
   */
  pluck: (strings: TemplateStringsArray, ...vars: any[]) => Promise<any>;
  
  /**
   * Executes a SQL query using template literals without returning results
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise that resolves when the query completes
   */
  execute: (strings: TemplateStringsArray, ...vars: any[]) => Promise<void>;
  
  /**
   * Alias for many() - Executes a SQL query and returns multiple rows
   */
  oo: (
    strings: TemplateStringsArray,
    ...vars: any[]
  ) => Promise<Record<string, any>[]>;
  
  /**
   * Alias for single() - Executes a SQL query and returns a single row
   */
  oO: (
    strings: TemplateStringsArray,
    ...vars: any[]
  ) => Promise<Record<string, any> | null>;
  
  /**
   * Alias for pluck() - Executes a SQL query and returns a single value
   */
  ox: (strings: TemplateStringsArray, ...vars: any[]) => Promise<any>;
  
  /**
   * Alias for execute() - Executes a SQL query without returning results
   */
  xx: (strings: TemplateStringsArray, ...vars: any[]) => Promise<void>;
  
  /**
   * Executes a raw SQL query with parameterized values
   * 
   * @param str - SQL query string
   * @param vars - Variables to use as parameters
   * @returns Promise resolving to query result with rows and count
   */
  query: (
    str: string,
    ...vars: any[]
  ) => Promise<{ rows: Record<string, any>[]; rowCount: number }>;
}

/**
 * Simplified interface for table-specific operations
 */
export interface TableInterface {
  /**
   * Inserts one or more records into the table
   * 
   * @param data - Single record or array of records to insert
   * @returns Promise resolving to operation result
   */
  insert: (
    data: Record<string, any> | Record<string, any>[],
  ) => Promise<QueryResult>;
  
  /**
   * Retrieves a single record from the table matching the where criteria
   * 
   * @param where - Criteria to match records
   * @returns Promise resolving to matching record or null if not found
   */
  get: (where: Record<string, any>) => Promise<Record<string, any> | null>;
  
  /**
   * Retrieves multiple records from the table matching the where criteria
   * 
   * @param where - Criteria to match records
   * @returns Promise resolving to array of matching records
   */
  list: (where: Record<string, any>) => Promise<Record<string, any>[]>;
}
</file>

<file path="packages/sql/README.md">
# @have/sql

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Database interaction library with support for SQLite and PostgreSQL in the HAVE SDK.

## Overview

The `@have/sql` package provides a simple and consistent interface for interacting with SQL databases. It supports both SQLite and PostgreSQL with the same API, making it easy to develop locally with SQLite and deploy to production with PostgreSQL.

## Features

- Unified API for SQLite and PostgreSQL
- Template literal query interface with automatic parameterization
- Type-safe query results
- Simple CRUD operations with minimal boilerplate
- Connection pooling and efficient resource management
- Transaction support
- Migration utilities
- No ORM overhead, just raw SQL with safety features

## Installation

```bash
# Install with npm
npm install @have/sql

# Or with yarn
yarn add @have/sql

# Or with pnpm
pnpm add @have/sql
```

## Usage

### Connecting to a Database

```typescript
import { getDatabase } from '@have/sql';

// Connect to SQLite
const sqliteDb = await getDatabase({
  file: ':memory:', // In-memory database
  // Or use a file path:
  // file: './my-database.sqlite',
});

// Connect to PostgreSQL
const pgDb = await getDatabase({
  host: 'localhost',
  port: 5432,
  database: 'my_database',
  user: 'username',
  password: 'password',
});
```

### Executing Queries

The package provides several template literal functions for different query types:

- `oo` - Returns all rows from a query
- `oO` - Returns a single row
- `ox` - Returns a single value (first column of first row)
- `xx` - Executes a statement (no return value)

```typescript
// Fetch all posts
const { oo } = db;
const posts = await oo`
  SELECT * FROM posts
  WHERE published = true
  ORDER BY created_at DESC
`;
console.log(posts);

// Fetch a single post
const { oO } = db;
const post = await oO`
  SELECT * FROM posts
  WHERE id = ${postId}
`;
console.log(post);

// Get a count
const { ox } = db;
const count = await ox`
  SELECT COUNT(*) FROM posts
  WHERE author = ${authorName}
`;
console.log(`Found ${count} posts by ${authorName}`);

// Execute a statement
const { xx } = db;
await xx`
  DELETE FROM posts
  WHERE id = ${postId}
`;
```

### Using CRUD Helper Functions

```typescript
// Insert data
const newPost = await db.insert('posts', {
  title: 'Hello World',
  content: 'This is my first post',
  author: 'Jane Doe',
  created_at: new Date()
});

// Get a record by criteria
const post = await db.get('posts', { id: 123 });

// List records with filters
const recentPosts = await db.list('posts', {
  author: 'Jane Doe',
  published: true
});

// Update records
await db.update('posts', 
  { id: 123 }, // where
  { title: 'Updated Title' } // set
);

// Create a table-specific helper
const postsTable = db.table('posts');
const post = await postsTable.get({ id: 123 });
const newPost = await postsTable.insert({
  title: 'Another Post',
  content: 'More content here',
  author: 'John Smith'
});
```

### Using Transactions

```typescript
// Start a transaction
await db.transaction(async (tx) => {
  // Use transaction object like the db object
  await tx.xx`
    INSERT INTO categories (name) 
    VALUES (${categoryName})
  `;
  
  const categoryId = await tx.ox`
    SELECT id FROM categories 
    WHERE name = ${categoryName}
  `;
  
  await tx.xx`
    INSERT INTO posts (title, category_id)
    VALUES (${title}, ${categoryId})
  `;
  
  // Transaction automatically commits if no errors
  // Or rolls back if any error is thrown
});
```

## Important Notes

- Always use parameterized queries with the template literal functions
- Don't use variables for table or column names - only for values
- Don't accept unsanitized user input for table or column names
- Keep raw SQL as ANSI-compatible as possible for database portability
- Move complex operations to per-database-adapter functions

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_sql.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/sql/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="packages/utils/README.md">
# @have/utils

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Shared utility functions for the HAVE SDK.

## Overview

The `@have/utils` package provides common utility functions and helpers used across the HAVE SDK packages. It includes tools for working with arrays, objects, strings, errors, validation, and more.

## Features

- Type utilities and type guards
- String manipulation and formatting
- Array and object helpers
- Error handling and custom error classes
- Validation utilities
- Async helpers and concurrency tools
- Date and time utilities
- Logging utilities
- Random and UUID generation
- Memoization and caching utilities

## Installation

```bash
# Install with npm
npm install @have/utils

# Or with yarn
yarn add @have/utils

# Or with pnpm
pnpm add @have/utils
```

## Usage

### Type Guards and Checking

```typescript
import { isString, isNumber, isArray, isObject, isNullOrUndefined } from '@have/utils';

// Check types
isString('hello'); // true
isNumber(123); // true
isArray([1, 2, 3]); // true
isObject({ foo: 'bar' }); // true
isNullOrUndefined(null); // true
isNullOrUndefined(undefined); // true
```

### Object Manipulation

```typescript
import { 
  deepMerge, 
  pick, 
  omit, 
  flatten, 
  unflatten 
} from '@have/utils';

// Deep merge objects
const merged = deepMerge({ a: 1, b: { c: 2 } }, { b: { d: 3 }, e: 4 });
// Result: { a: 1, b: { c: 2, d: 3 }, e: 4 }

// Pick specific properties
const picked = pick({ a: 1, b: 2, c: 3 }, ['a', 'c']);
// Result: { a: 1, c: 3 }

// Omit specific properties
const omitted = omit({ a: 1, b: 2, c: 3 }, ['b']);
// Result: { a: 1, c: 3 }

// Flatten nested objects
const flattened = flatten({ a: { b: { c: 1, d: 2 }, e: 3 } });
// Result: { 'a.b.c': 1, 'a.b.d': 2, 'a.e': 3 }

// Unflatten objects
const unflattened = unflatten({ 'a.b.c': 1, 'a.b.d': 2, 'a.e': 3 });
// Result: { a: { b: { c: 1, d: 2 }, e: 3 } }
```

### String Utilities

```typescript
import { 
  camelCase, 
  snakeCase, 
  kebabCase, 
  pascalCase,
  slugify,
  truncate 
} from '@have/utils';

// Convert between case styles
camelCase('hello-world'); // 'helloWorld'
snakeCase('helloWorld'); // 'hello_world'
kebabCase('helloWorld'); // 'hello-world'
pascalCase('hello-world'); // 'HelloWorld'

// Create URL-friendly slugs
slugify('Hello World!'); // 'hello-world'

// Truncate text
truncate('This is a long text that will be truncated', 10); // 'This is a...'
```

### Async Utilities

```typescript
import { 
  retry, 
  timeout, 
  debounce, 
  throttle, 
  parallel 
} from '@have/utils';

// Retry a function
const result = await retry(
  async () => fetch('https://example.com/api'),
  { 
    attempts: 3, 
    delay: 1000,
    backoff: 'exponential' 
  }
);

// Add timeout to a promise
const data = await timeout(
  fetch('https://example.com/api').then(res => res.json()),
  5000 // 5 seconds
);

// Debounce a function
const debouncedSave = debounce(saveData, 500);
debouncedSave(); // Will execute after 500ms of inactivity

// Throttle a function
const throttledScroll = throttle(handleScroll, 100);
window.addEventListener('scroll', throttledScroll);

// Run multiple async tasks in parallel with concurrency limit
const results = await parallel(
  [task1, task2, task3, task4, task5],
  { concurrency: 2 } // Run 2 tasks at a time
);
```

## API Reference

See the [API documentation](https://happyvertical.github.io/sdk/modules/_have_utils.html) for detailed information on all available methods and options.

## License

This package is part of the HAVE SDK and is licensed under the MIT License - see the [LICENSE](../../LICENSE) file for details.
</file>

<file path="packages/utils/vitest.config.ts">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    include: ['src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
  },
});
</file>

<file path="scripts/validate-changeset-config.js">
#!/usr/bin/env node

/**
 * Changeset configuration validation script for pre-commit hooks
 * Validates changeset configuration for proper monorepo publishing setup
 */

import { readFileSync, existsSync, readdirSync } from 'fs';
import { resolve, join } from 'path';

/**
 * Validate changeset configuration file
 * @returns {Array} Array of validation errors
 */
function validateChangesetConfig() {
  const errors = [];
  const configPath = '.changeset/config.json';
  
  if (!existsSync(configPath)) {
    errors.push('Missing .changeset/config.json configuration file');
    return errors;
  }
  
  try {
    const content = readFileSync(configPath, 'utf8');
    const config = JSON.parse(content);
    
    // Required fields validation
    const requiredFields = ['changelog', 'commit', 'access', 'baseBranch'];
    for (const field of requiredFields) {
      if (config[field] === undefined) {
        errors.push(`Missing required field in changeset config: ${field}`);
      }
    }
    
    // Validate access configuration
    if (config.access && !['restricted', 'public'].includes(config.access)) {
      errors.push(`Invalid access value: ${config.access} (should be "restricted" or "public")`);
    }
    
    // Validate base branch
    if (config.baseBranch && config.baseBranch !== 'master' && config.baseBranch !== 'main') {
      errors.push(`Base branch should be "master" or "main" (got: ${config.baseBranch})`);
    }
    
    // Validate updateInternalDependencies
    if (config.updateInternalDependencies && 
        !['patch', 'minor', 'major'].includes(config.updateInternalDependencies)) {
      errors.push(`Invalid updateInternalDependencies: ${config.updateInternalDependencies}`);
    }
    
    // Validate schema if present
    if (config.$schema && !config.$schema.includes('@changesets/config')) {
      errors.push('Invalid schema reference in changeset config');
    }
    
  } catch (parseError) {
    if (parseError instanceof SyntaxError) {
      errors.push(`Invalid JSON in .changeset/config.json: ${parseError.message}`);
    } else {
      errors.push(`Failed to read changeset config: ${parseError.message}`);
    }
  }
  
  return errors;
}

/**
 * Validate workspace packages are properly configured for changesets
 * @returns {Array} Array of validation errors
 */
function validateWorkspacePackages() {
  const errors = [];
  
  try {
    // Read workspace configuration
    const workspaceContent = readFileSync('pnpm-workspace.yaml', 'utf8');
    
    // Get package paths using fs instead of glob
    const packagePaths = [];
    if (existsSync('packages')) {
      const packageDirs = readdirSync('packages', { withFileTypes: true })
        .filter(dirent => dirent.isDirectory())
        .map(dirent => dirent.name);
      
      for (const dir of packageDirs) {
        const pkgPath = join('packages', dir, 'package.json');
        if (existsSync(pkgPath)) {
          packagePaths.push(pkgPath);
        }
      }
    }
    
    // Check each package has proper changeset configuration
    for (const packagePath of packagePaths) {
      try {
        const pkgContent = readFileSync(packagePath, 'utf8');
        const pkg = JSON.parse(pkgContent);
        
        // Check if package is publishable (has name and not private)
        if (pkg.name && !pkg.private) {
          // Validate package name format for publishing
          if (!pkg.name.startsWith('@have/')) {
            errors.push(`Package ${pkg.name} should use @have/ namespace for publishing`);
          }
          
          // Check for required publishing fields
          if (!pkg.version) {
            errors.push(`Publishing package ${pkg.name} missing version field`);
          }
          
          if (!pkg.description) {
            errors.push(`Publishing package ${pkg.name} missing description field`);
          }
          
          // Validate main/exports for publishing
          if (!pkg.main && !pkg.exports) {
            errors.push(`Publishing package ${pkg.name} missing main or exports field`);
          }
        }
        
      } catch (pkgError) {
        errors.push(`Failed to read package.json at ${packagePath}: ${pkgError.message}`);
      }
    }
    
  } catch (workspaceError) {
    errors.push(`Failed to read workspace configuration: ${workspaceError.message}`);
  }
  
  return errors;
}

/**
 * Validate changeset CLI integration
 * @returns {Array} Array of validation errors
 */
function validateChangesetCLI() {
  const errors = [];
  
  try {
    const rootPkgPath = 'package.json';
    if (!existsSync(rootPkgPath)) {
      errors.push('Missing root package.json');
      return errors;
    }
    
    const content = readFileSync(rootPkgPath, 'utf8');
    const pkg = JSON.parse(content);
    
    // Check for changeset in devDependencies
    if (!pkg.devDependencies || !pkg.devDependencies['@changesets/cli']) {
      errors.push('Missing @changesets/cli in devDependencies');
    }
    
    // Check for changeset scripts
    if (!pkg.scripts) {
      errors.push('Missing scripts section in root package.json');
      return errors;
    }
    
    const requiredScripts = {
      'changeset': '@changesets/cli changeset',
      'version-packages': '@changesets/cli version',
      'release': '@changesets/cli publish'
    };
    
    for (const [scriptName, expectedCommand] of Object.entries(requiredScripts)) {
      if (!pkg.scripts[scriptName]) {
        errors.push(`Missing changeset script: ${scriptName}`);
      } else if (!pkg.scripts[scriptName].includes('@changesets/cli')) {
        errors.push(`Script ${scriptName} should use @changesets/cli`);
      }
    }
    
  } catch (error) {
    errors.push(`Failed to validate changeset CLI: ${error.message}`);
  }
  
  return errors;
}

/**
 * Validate gitignore configuration for changesets
 * @returns {Array} Array of validation errors  
 */
function validateGitignore() {
  const errors = [];
  
  try {
    if (!existsSync('.gitignore')) {
      errors.push('Missing .gitignore file');
      return errors;
    }
    
    const content = readFileSync('.gitignore', 'utf8');
    const lines = content.split('\n').map(line => line.trim());
    
    // Check for changeset temp files
    const requiredIgnores = [
      '.changeset/*.md',
      '!.changeset/README.md',
      '!.changeset/config.json'
    ];
    
    for (const ignore of requiredIgnores) {
      if (!lines.includes(ignore)) {
        errors.push(`Missing gitignore entry: ${ignore}`);
      }
    }
    
  } catch (error) {
    errors.push(`Failed to validate gitignore: ${error.message}`);
  }
  
  return errors;
}

/**
 * Main validation function
 */
function main() {
  console.log('🔍 Validating changeset configuration...\n');
  
  let hasErrors = false;
  
  // Validate changeset config file
  const configErrors = validateChangesetConfig();
  if (configErrors.length > 0) {
    hasErrors = true;
    console.error('❌ Changeset configuration errors:');
    for (const error of configErrors) {
      console.error(`  • ${error}`);
    }
    console.error('');
  } else {
    console.log('✅ Changeset configuration is valid');
  }
  
  // Validate workspace packages
  const packageErrors = validateWorkspacePackages();
  if (packageErrors.length > 0) {
    hasErrors = true;
    console.error('❌ Workspace package errors:');
    for (const error of packageErrors) {
      console.error(`  • ${error}`);
    }
    console.error('');
  } else {
    console.log('✅ Workspace packages are properly configured');
  }
  
  // Validate changeset CLI integration
  const cliErrors = validateChangesetCLI();
  if (cliErrors.length > 0) {
    hasErrors = true;
    console.error('❌ Changeset CLI errors:');
    for (const error of cliErrors) {
      console.error(`  • ${error}`);
    }
    console.error('');
  } else {
    console.log('✅ Changeset CLI is properly integrated');
  }
  
  // Validate gitignore
  const gitignoreErrors = validateGitignore();
  if (gitignoreErrors.length > 0) {
    hasErrors = true;
    console.error('❌ Gitignore configuration errors:');
    for (const error of gitignoreErrors) {
      console.error(`  • ${error}`);
    }
    console.error('');
  } else {
    console.log('✅ Gitignore is properly configured for changesets');
  }
  
  if (hasErrors) {
    console.error('🚫 Changeset validation failed!');
    console.error('Please fix the errors above before committing.');
    process.exit(1);
  } else {
    console.log('\n✅ All changeset configuration is valid!');
  }
}

try {
  main();
} catch (error) {
  console.error('💥 Validation script failed:', error.message);
  process.exit(1);
}
</file>

<file path="scripts/validate-package-json.js">
#!/usr/bin/env node

/**
 * Package.json validation script for pre-commit hooks
 * Validates package.json files for required fields, version consistency, and format
 */

import { readFileSync } from 'fs';
import { resolve, dirname, basename } from 'path';

const REQUIRED_FIELDS = [
  'name',
  'version', 
  'description',
  'type',
  'main',
  'scripts'
];

const REQUIRED_SCRIPTS = [
  'build',
  'dev',
  'test'
];

/**
 * Validate a single package.json file
 * @param {string} filePath - Path to package.json file
 * @returns {Array} Array of validation errors
 */
function validatePackageJson(filePath) {
  const errors = [];
  
  try {
    const content = readFileSync(filePath, 'utf8');
    const pkg = JSON.parse(content);
    const packageName = basename(dirname(filePath));
    
    // Check required fields
    for (const field of REQUIRED_FIELDS) {
      if (!pkg[field]) {
        errors.push(`Missing required field: ${field}`);
      }
    }
    
    // Validate package name format
    if (pkg.name && !pkg.name.startsWith('@have/') && packageName !== 'sdk') {
      errors.push(`Package name should start with @have/ (got: ${pkg.name})`);
    }
    
    // Check required scripts
    if (pkg.scripts) {
      for (const script of REQUIRED_SCRIPTS) {
        if (!pkg.scripts[script]) {
          errors.push(`Missing required script: ${script}`);
        }
      }
    }
    
    // Validate version format
    if (pkg.version && !/^\d+\.\d+\.\d+/.test(pkg.version)) {
      errors.push(`Invalid version format: ${pkg.version} (should be semver)`);
    }
    
    // Check for Node.js version consistency
    if (pkg.engines?.node && pkg.engines.node !== '>=22.0.0') {
      errors.push(`Node.js version should be >=22.0.0 (got: ${pkg.engines.node})`);
    }
    
    // Validate workspace dependencies format
    if (pkg.dependencies) {
      for (const [dep, version] of Object.entries(pkg.dependencies)) {
        if (dep.startsWith('@have/') && version !== 'workspace:*') {
          errors.push(`Internal dependency ${dep} should use "workspace:*" (got: ${version})`);
        }
      }
    }
    
    // Check for proper module type
    if (!pkg.type || pkg.type !== 'module') {
      errors.push('Package should specify "type": "module"');
    }
    
  } catch (parseError) {
    if (parseError instanceof SyntaxError) {
      errors.push(`Invalid JSON format: ${parseError.message}`);
    } else {
      errors.push(`Failed to read file: ${parseError.message}`);
    }
  }
  
  return errors;
}

/**
 * Check version consistency across all package.json files
 * @param {Array} packagePaths - Array of package.json file paths
 * @returns {Array} Array of consistency errors
 */
function checkVersionConsistency(packagePaths) {
  const errors = [];
  const versions = new Map();
  
  for (const filePath of packagePaths) {
    try {
      const content = readFileSync(filePath, 'utf8');
      const pkg = JSON.parse(content);
      
      if (pkg.version) {
        const packageName = basename(dirname(filePath));
        versions.set(packageName, pkg.version);
      }
    } catch (error) {
      // Skip files with JSON errors (handled by individual validation)
    }
  }
  
  // Check if all package versions match (for monorepo consistency)
  const versionValues = Array.from(versions.values());
  const uniqueVersions = new Set(versionValues);
  
  if (uniqueVersions.size > 1) {
    errors.push(`Version mismatch across packages: ${Array.from(uniqueVersions).join(', ')}`);
    for (const [pkg, version] of versions) {
      errors.push(`  ${pkg}: ${version}`);
    }
  }
  
  return errors;
}

/**
 * Main validation function
 */
function main() {
  const filePaths = process.argv.slice(2);
  
  if (filePaths.length === 0) {
    console.error('No package.json files provided');
    process.exit(1);
  }
  
  let hasErrors = false;
  
  // Validate each package.json individually
  for (const filePath of filePaths) {
    const errors = validatePackageJson(filePath);
    
    if (errors.length > 0) {
      hasErrors = true;
      console.error(`\n❌ Validation errors in ${filePath}:`);
      for (const error of errors) {
        console.error(`  • ${error}`);
      }
    } else {
      console.log(`✅ ${filePath} is valid`);
    }
  }
  
  // Check version consistency across all files
  const consistencyErrors = checkVersionConsistency(filePaths);
  if (consistencyErrors.length > 0) {
    hasErrors = true;
    console.error('\n❌ Version consistency errors:');
    for (const error of consistencyErrors) {
      console.error(`  • ${error}`);
    }
  }
  
  if (hasErrors) {
    console.error('\n🚫 Package.json validation failed!');
    console.error('Please fix the errors above before committing.');
    process.exit(1);
  } else {
    console.log('\n✅ All package.json files are valid!');
  }
}

main();
</file>

<file path=".gitignore">
# Dependencies
node_modules
packages/*/node_modules
packages/*/dist
.pnpm-store

# Environment variables
.envrc
.env
.env.local
.env.development
.env.production
.env.test

# Build outputs
dist/
build/
docs/manual/

# TypeScript
*.tsbuildinfo
.tscache

# IDE and editors
.idea/
.vscode/
*.swp
*.swo
.DS_Store
Thumbs.db

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# Cache directories
.npm
.eslintcache
.stylelintcache
.next
.nuxt
.cache
.turbo

# Test coverage
coverage/
.nyc_output

# Misc
.DS_Store
*.pem
.vercel

*.traineddata
.svelte-kit
.npmrc.local

**/.claude/settings.local.json
</file>

<file path="biome.json">
{
  "$schema": "https://biomejs.dev/schemas/1.5.3/schema.json",
  "organizeImports": {
    "enabled": true
  },
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true,
      "correctness": {
        "noUnusedVariables": "warn"
      },
      "suspicious": {
        "noExplicitAny": "warn",
        "noPrototypeBuiltins": "warn"
      },
      "style": {
        "noNonNullAssertion": "warn"
      }
    }
  },
  "formatter": {
    "enabled": true,
    "formatWithErrors": false,
    "indentStyle": "space",
    "indentWidth": 2,
    "lineWidth": 80
  },
  "javascript": {
    "formatter": {
      "quoteStyle": "single",
      "trailingCommas": "all",
      "semicolons": "always"
    }
  },
  "files": {
    "include": [
      "packages/**/src/**/*.{ts,tsx,js,jsx,json}",
      "*.{ts,tsx,js,jsx,json}",
      ".claude/**/*.{ts,tsx,js,jsx,json,md}"
    ],
    "ignore": [
      "**/node_modules/**",
      "**/.svelte-kit/**",
      "**/build/**",
      "**/dist/**"
    ]
  }
}
</file>

<file path="setup_dev.sh">
#!/usr/bin/env bash

# Get the directory where the script is located
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

# Unlink all packages globally first
pnpm unlink --global @have/ai @have/files @have/pdf @have/smrt @have/spider @have/sql @have/utils
pnpm unlink --global @happyvertical/praeco

# Link SDK packages
cd "$SCRIPT_DIR/packages/ai" && pnpm unlink && pnpm link --global
cd "$SCRIPT_DIR/packages/files" && pnpm unlink && pnpm link --global
cd "$SCRIPT_DIR/packages/pdf" && pnpm unlink && pnpm link --global
cd "$SCRIPT_DIR/packages/smrt" && pnpm unlink && pnpm link --global
cd "$SCRIPT_DIR/packages/spider" && pnpm unlink && pnpm link --global
cd "$SCRIPT_DIR/packages/sql" && pnpm unlink && pnpm link --global
cd "$SCRIPT_DIR/packages/utils" && pnpm unlink && pnpm link --global

# Link to praeco
cd "$SCRIPT_DIR/../praeco"
pnpm unlink @have/ai @have/files @have/smrt @have/spider @have/sql @have/utils
pnpm link --global @have/ai @have/files @have/smrt @have/spider @have/sql @have/utils
pnpm link --global

# Link to bentleyalberta.com
cd "$SCRIPT_DIR/../bentleyalberta.com"
pnpm unlink @happyvertical/praeco @have/smrt @have/utils
pnpm link --global @happyvertical/praeco @have/smrt @have/utils
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "declaration": true,
    "strict": true,
    "experimentalDecorators": true,
    "emitDecoratorMetadata": true,
    "skipLibCheck": true,
    "lib": ["ES2022"]
  },
  "references": [
    { "path": "./packages/utils" },
    { "path": "./packages/files" },
    { "path": "./packages/spider" },
    { "path": "./packages/sql" },
    { "path": "./packages/pdf" },
    { "path": "./packages/ai" },
    { "path": "./packages/smrt" }
  ],
  "exclude": ["node_modules", "**/dist"]
}
</file>

<file path="vitest.config.ts">
import { defineConfig } from 'vitest/config';
import path from 'path';

export default defineConfig({
  test: {
    include: ['packages/**/src/**/*.{test,spec}.{js,ts}'],
    globals: true,
    environment: 'node',
    globalSetup: path.resolve(__dirname, './vitest.setup.ts'),
    sequence: {
      hooks: 'list',
    },
    poolOptions: {
      threads: {
        singleThread: true,
      },
    },
  },
  // Configure workspace projects
  workspace: [
    'packages/utils',
    'packages/files', 
    'packages/ai',
    'packages/spider',
    'packages/sql',
    'packages/pdf',
    'packages/smrt',
  ],
});
</file>

<file path=".claude/commands/issue.md">
---
name: issue
description: Smart issue management - analyze and advance issues through workflow, or create new issues
usage: /issue <issue_number> OR /issue "<description>"
---

# Issue Workflow Command

Analyzes the current state of a GitHub issue and automatically advances it to the next appropriate workflow stage, or creates a new issue from a description.

## Usage
```
/issue <issue_number>
/issue "<description>"
```

### Create New Issue
When passed a string description instead of a number:
- Analyze the description to understand the request
- Generate appropriate title, body, and labels
- Create the issue using `gh issue create`
- Add to project board with "Todo" status
- Return the new issue number and URL

### Manage Existing Issue
When passed an issue number:

## Behavior by Current Status

**TODO (including new, backlog, ready items):**
- Search for duplicates using title keywords
- Assess clarity and completeness of the issue
- Check Definition of Ready per docs/workflow/DEFINITION_OF_READY.md
- For new/unclear issues: assess validity, request info, or mark ready for development
- For ready issues: perform comprehensive DoR validation:
  1. **User Story Validation**: Check if follows "As a... I want... so that..." format (for features)
  2. **Acceptance Criteria**: Verify specific, testable conditions exist
  3. **Implementation Gameplan**: Ensure technical approach is documented
  4. **Estimation**: Check for size estimate (S/M/L or story points)
  5. **No Blockers**: Verify dependencies are resolved
  6. **Test Scenarios**: Ensure test considerations are outlined
- If Definition of Ready is NOT met: 
  - Add detailed comment explaining specific missing criteria
  - Generate suggestions for missing elements
  - Keep issue in current status
- If Definition of Ready IS met:
  - **Validate Git State**: Check working directory is clean with `git status`
  - **Create feature branch**: Use format `issue-{number}-{short-description}` (max 50 chars)
    - Sanitize description: remove special chars, convert spaces to hyphens
    - Example: `issue-23-claude-commands-validation`
  - **Check branch existence**: Verify branch doesn't already exist with `git branch --list`
  - **Validate status transition**: Ensure move to "In Progress" is valid per workflow
  - Automatically assign to self
  - Update project status to "In Progress"
  - Begin full implementation following the agreed gameplan

**IN PROGRESS:**
- Check for review comments or feedback
- If re-running, implement requested changes
- **Conflict Detection**: Check for other PRs addressing the same issue
- If ready, create PR but keep status as "In Progress" during review
- Continue iterating based on feedback until PR is merged
- **Rollback Instructions**: If implementation fails:
  - Save current work: `git stash`
  - Return to master: `git checkout master`
  - Delete branch if needed: `git branch -D issue-{number}-*`
  - Update issue status back to "To Do" with explanation

**DONE:**
- Check for production issues or follow-up work needed
- Create follow-up issues if bugs are found
- Archive or close if completely stable

## Re-run Behavior
When run again on the same issue, checks for new comments/feedback and acts accordingly:
- Implements requested changes
- Addresses review feedback
- Updates based on new information
- **Status Validation**: Verify current status before attempting any transitions
- **Error Recovery**: If previous run failed, provide clear recovery options

## Validation Safeguards

### Git State Validation
- Always check `git status` before branch operations
- Ensure no uncommitted changes exist
- Verify on correct base branch (master/main)

### Branch Naming Convention
- Format: `issue-{number}-{short-description}`
- Max length: 50 characters
- Sanitize special characters
- Check existence before creation

### Definition of Ready Checklist
Before moving to "In Progress", validate all 7 criteria:
1. ✓ User Story / Problem Statement
2. ✓ Acceptance Criteria
3. ✓ Implementation Gameplan
4. ✓ Design Assets (if applicable)
5. ✓ Estimation
6. ✓ No Blockers
7. ✓ Testing Considerations

### Status Transition Rules
- Only move from "To Do" → "In Progress" if DoR met
- Validate transitions follow workflow rules
- Prevent duplicate status changes
- Log all status changes with reasons

## Examples

### Managing existing issue
```
/issue 1
```
This will analyze issue #1, determine its current project Status (Todo/In Progress/Done), and take the appropriate action to advance it through the simplified workflow.

### Creating new issue
```
/issue "Add dark mode toggle to user settings"
```
This will create a new issue with an appropriate title and body, apply initial labels, and return the issue number.

```
/issue "The login form doesn't work on mobile Safari - users can't submit credentials"
```
This will create a bug report with relevant details and appropriate bug labels.
</file>

<file path=".claude/commands/issues-close.md">
---
name: issues-close
description: Process all issues in "Done" status assigned to me
usage: /issues-close [notes]
---

# Done/Close Lane Command

Processes all issues in the "Done" status that are assigned to the current user.

## Usage
```
/issues-close
/issues-close "monitor for issues"
```

## Description
This command:
1. Finds all issues in "Done" status assigned to you
2. Runs `/issue` command on each one
3. Monitors for production issues
4. Creates follow-up issues if needed
5. Closes stable, completed issues

## Behavior
- Monitors deployed changes for issues
- Validates production stability
- Creates follow-up bug reports if needed
- Closes completed issues
- Updates documentation if needed

## Notes Parameter
Optional notes guide how to process the issues:
- "monitor for issues" - Watch for production problems
- "close stable" - Close issues that are working well
- "create follow-ups" - Generate additional issues from learnings
- "update docs" - Ensure documentation reflects changes

## Example
```
/issues-close "verify production stability"
```
</file>

<file path="docs/workflow/DEFINITION_OF_DONE.md">
# Definition of Done (DoD)

This document outlines the set of criteria that a Pull Request (PR) must satisfy before it can be considered complete and merged into the main branch. This checklist serves as our team's shared agreement to ensure quality, consistency, and completeness for all work.

An issue or story is not "Done" until every applicable item on this list has been checked off.

## General Requirements

- [ ] The code implements all acceptance criteria outlined in the original issue.
- [ ] The solution is successfully deployed to a staging/preview environment and has been verified.
- [ ] The original issue has been updated with a link to the Pull Request.
- [ ] The Pull Request description or commit messages use closing keywords (`closes #123`, `fixes #123`, or `resolves #123`) to automatically close issues when merged.
- [ ] The issue's Status in the GitHub Project has been updated to "Done" upon merge.

## Code Quality & Standards

- [ ] Code adheres to the established style guide (enforced by the linter).
- [ ] There are no new linter warnings or errors introduced.
- [ ] All new code includes clear, concise comments for any complex logic.
- [ ] All secrets, keys, and credentials have been handled securely and are not hard-coded.
- [ ] The code has been self-reviewed by the author before requesting a peer review.

## Testing

- [ ] All new code paths are covered by new or updated unit tests.
- [ ] Relevant integration tests have been added or updated to cover the interaction between components.
- [ ] All existing and new tests are passing in the CI pipeline.
- [ ] The feature has undergone manual testing for any user-facing changes to confirm expected behavior.

## Documentation

- [ ] The project's README.md has been updated if there are changes to the setup, installation, or environment variables.
- [ ] An Architecture Decision Record (ADR) has been created in the `/docs/adr` directory if the change introduces a new dependency or makes a significant architectural decision.
- [ ] User-facing documentation has been updated to reflect any changes in functionality.

## Process & Review

- [ ] The Pull Request has a clear, descriptive title and a body that explains the "what" and "why" of the change.
- [ ] The CI/CD pipeline has completed successfully for the PR branch.
- [ ] The PR has been reviewed and approved by at least one other team member.
- [ ] All review comments have been addressed and resolved.
</file>

<file path="docs/workflow/KANBAN.md">
# Kanban CI/CD Workflow Specification

This document specifies the end-to-end workflow for managing issues within a Kanban system, from initial creation through to deployment. An "issue" represents any single unit of work, such as a feature, bug fix, or enhancement.

The flow is designed to ensure that work is properly vetted, prioritized, and developed with high quality, leveraging automation wherever possible. Each lane in the Kanban board has a distinct purpose and a defined set of actions to guide the process.

## Project Setup and Automation

To implement this workflow effectively in a tool like GitHub, specific setup is required for the repository and its associated project board. This section outlines the necessary labels, board configuration, and automation rules that enable the workflow.

### Label Conventions

A consistent labeling strategy is crucial for categorization.

**Type Labels:**
These labels provide metadata about the nature of the work.

* `type:bug`
* `type:feature`
* `type:enhancement`
* `type:tech-debt`
* `type:epic`

**Status Management:**
Instead of using custom `status:*` labels, this workflow leverages GitHub Projects' built-in **Status field** for tracking issue progress. This provides better integration with GitHub's native automation and eliminates the need for custom label management.

### Project Board Configuration

The project board uses GitHub Projects v2 with the built-in Status field for tracking issue progress.

1. Create a new Project and select the **Board** layout.
2. Configure the board's columns to be grouped by the **Status** field.
3. Configure custom Status options for the 8-stage workflow:
   - **Fresh**: Newly created items requiring triage
   - **Icebox**: Low priority items for future consideration  
   - **Backlog**: Prioritized items waiting for development
   - **To Do**: Items ready for immediate development (meet Definition of Ready)
   - **Developing**: Items currently being actively developed
   - **Quality Assurance**: Items under review and testing
   - **Deploying**: Items approved and currently being deployed
   - **Done**: Completed and deployed items

This comprehensive workflow provides clear visibility into work progress while maintaining proper quality gates.

### Automation Workflows

GitHub Projects v2 provides built-in automation that eliminates the need for custom workflows.

**Built-in Project Automation:**
* **Manual Updates**: Dragging cards between columns automatically updates the Status field
* **Default Status**: Configure automatic assignment of "Todo" status when items are added to the project
* **Status Sync**: The Status field is the single source of truth for issue progress

**Recommended Project Workflows:**
1. **Auto-add items**: Set up the built-in workflow to automatically add new issues to the project with "Todo" status
2. **PR automation**: Configure automatic status changes when PRs are opened/merged

This native approach provides better reliability and requires no custom GitHub Actions maintenance.

## Workflow Stages

The 8-stage workflow provides comprehensive tracking from initial issue creation through final deployment.

### Fresh

The entry point for all work items. Fresh issues require initial triage to determine validity and priority:
- **Validation**: Confirm issue is clear, actionable, and within project scope
- **Duplicate Check**: Ensure no similar issues already exist
- **Initial Classification**: Add appropriate type and priority labels
- **Assignment**: Route to appropriate team member or keep unassigned for later triage

Issues should not remain in "Fresh" long - they move quickly to either Icebox, Backlog, or are closed.

### Icebox

Low priority items that may be addressed in the future but are not currently planned:
- **Future Consideration**: Valid ideas that aren't current priorities
- **Needs More Info**: Issues requiring additional research or clarification
- **Low Priority**: Work that's useful but not urgent
- **Parking Lot**: Items to revisit during planning cycles

Items in Icebox are reviewed periodically (quarterly) to determine if they should move to Backlog or be closed.

### Backlog

Prioritized work that will be addressed but hasn't been refined enough to start development:
- **Prioritized**: Items have been ranked in order of importance
- **Refined**: Issues have sufficient detail for estimation
- **Dependent Work**: Items waiting for other work to complete
- **Ready for Planning**: Will be refined to meet Definition of Ready

The Backlog serves as the funnel for upcoming development work.

### To Do  

Issues that meet the Definition of Ready and are prepared for immediate development:
- **Definition of Ready Complete**: All acceptance criteria, technical requirements, and implementation gameplan are clear
- **No Blockers**: All dependencies resolved, resources available
- **Assigned**: Clear ownership established
- **Estimated**: Effort and complexity understood

Work should only enter "To Do" when a developer can immediately begin implementation.

### Developing

Active development work in progress:
- **Implementation**: Writing code according to acceptance criteria
- **Testing**: Creating and running tests for the functionality  
- **Documentation**: Updating relevant documentation
- **Pull Request Creation**: Opening PRs when ready for review

Issues remain in "Developing" throughout the entire development cycle until work is ready for review.

### Quality Assurance

Work under review and testing before deployment:
- **Code Review**: Peer review of implementation and tests
- **CI/CD Validation**: Automated testing and quality checks
- **Manual Testing**: User acceptance testing when appropriate
- **Documentation Review**: Ensuring all documentation is complete and accurate

Items move through Quality Assurance when all review feedback is addressed and tests pass.

### Deploying

Approved work currently being deployed to production:
- **Review Complete**: All code review feedback addressed
- **Tests Passing**: Full CI/CD pipeline success
- **Documentation Updated**: All relevant docs reflect the changes
- **Deployment Approved**: Ready for production release

This stage allows for batching deployments and final validation before release.

### Done

Completed work that has been successfully deployed to production:
- **Live in Production**: Feature/fix is available to end users
- **Post-Deploy Validation**: Confirming functionality works as expected
- **Monitoring**: Tracking metrics and error rates post-deployment
- **Issue Closure**: Original issue can be closed as complete

Done items serve as a historical record and can be archived periodically to keep the board clean.
</file>

<file path="packages/ai/src/client.ts">
import 'openai/shims/node';
import OpenAI from 'openai';
import { ApiError, ValidationError } from '@have/utils';

import type { AIMessageOptions } from './message.js';

/**
 * Common options for AI client configuration
 */
export interface AIClientOptions {
  /**
   * Type of AI client (e.g., 'openai')
   */
  type?: string;
  
  /**
   * Response format for AI completions
   */
  responseFormat?: string;
  
  /**
   * API key for authentication
   */
  apiKey?: string;
  
  /**
   * Base URL for API requests
   */
  baseUrl?: string;
}

/**
 * Interface defining required methods for AI clients
 */
export interface AIClientInterface {
  /**
   * Configuration options for this client
   */
  options: AIClientOptions;
  
  /**
   * Sends a message to the AI and gets a response
   * 
   * @param text - Message text
   * @param options - Message options
   * @returns Promise resolving to the AI response
   */
  message(text: string, options: AIMessageOptions): Promise<unknown>;
  
  /**
   * Gets a text completion from the AI
   * 
   * @param text - Input text for completion
   * @param options - Completion options
   * @returns Promise resolving to the completion result
   */
  textCompletion(text: string, options: AIMessageOptions): Promise<unknown>;
}

/**
 * Type guard to check if options are for OpenAI client
 * 
 * @param options - Options to check
 * @returns True if options are valid for OpenAI client
 */
function isOpenAIClientOptions(
  options: AIClientOptions,
): options is OpenAIClientOptions {
  return options.type === 'openai' && 'apiKey' in options;
}

/**
 * Options for AI text completion requests
 */
export interface AITextCompletionOptions {
  /**
   * Model identifier to use
   */
  model?: string;
  
  /**
   * Timeout in milliseconds
   */
  timeout?: number;
  
  /**
   * Role of the message sender
   */
  role?: OpenAI.Chat.ChatCompletionRole;
  
  /**
   * Previous messages in the conversation
   */
  history?: OpenAI.Chat.ChatCompletionMessageParam[];
  
  /**
   * Name of the message sender
   */
  name?: string;
  
  /**
   * Penalty for token frequency
   */
  frequencyPenalty?: number;
  
  /**
   * Token bias adjustments
   */
  logitBias?: Record<string, number>;
  
  /**
   * Whether to return log probabilities
   */
  logprobs?: boolean;
  
  /**
   * Number of top log probabilities to return
   */
  topLogprobs?: number;
  
  /**
   * Maximum tokens to generate
   */
  maxTokens?: number;
  
  /**
   * Number of completions to generate
   */
  n?: number;
  
  /**
   * Penalty for token presence
   */
  presencePenalty?: number;
  
  /**
   * Format for the response
   */
  responseFormat?: { type: 'text' | 'json_object' };
  
  /**
   * Random seed for deterministic results
   */
  seed?: number;
  
  /**
   * Sequences that stop generation
   */
  stop?: string | Array<string>;
  
  /**
   * Whether to stream responses
   */
  stream?: boolean;
  
  /**
   * Sampling temperature
   */
  temperature?: number;
  
  /**
   * Top-p sampling parameter
   */
  topProbability?: number;
  
  /**
   * Available tools for the model
   */
  tools?: Array<any>; // todo: figure out generic solution - Array<OpenAI.Chat.ChatCompletionTool>;
  
  /**
   * Tool selection behavior
   */
  toolChoice?:
    | 'none'
    | 'auto'
    | { type: 'function'; function: { name: string } };
  
  /**
   * User identifier
   */
  user?: string;
  
  /**
   * Callback for handling streaming responses
   */
  onProgress?: (partialMessage: string) => void;
}

/**
 * Base class for AI clients
 * Provides a common interface for different AI service providers
 */
export class AIClient {
  /**
   * Configuration options for this client
   */
  public options: AIClientOptions;

  /**
   * Creates a new AIClient
   * 
   * @param options - Client configuration options
   */
  constructor(options: AIClientOptions) {
    this.options = options;
  }

  /**
   * Sends a message to the AI
   * Base implementation returns a placeholder response
   * 
   * @param text - Message text
   * @param options - Message options
   * @returns Promise resolving to a placeholder response
   */
  public async message(
    text: string,
    options: AITextCompletionOptions = { role: 'user' },
  ) {
    return 'not a real ai message, this is the base class!';
  }

  /**
   * Factory method to create appropriate AI client based on options
   * 
   * @param options - Client configuration options
   * @returns Promise resolving to an initialized AI client
   * @throws Error if client type is invalid
   */
  public static async create<T extends AIClientOptions>(
    options: T,
  ): Promise<AIClient | OpenAIClient> {
    if (isOpenAIClientOptions(options)) {
      return OpenAIClient.create(options);
    }
    throw new ValidationError('Invalid client type specified', {
      supportedTypes: ['openai'],
      providedType: (options as any).type,
    });
  }

  /**
   * Gets a text completion from the AI
   * In base class, delegates to message method
   * 
   * @param text - Input text for completion
   * @param options - Completion options
   * @returns Promise resolving to the completion result
   */
  public textCompletion(
    text: string,
    options: AITextCompletionOptions = {
      role: 'user',
    },
  ) {
    return this.message(text, options);
  }
}

/**
 * Creates an OpenAI client instance
 * 
 * @param options - OpenAI configuration options
 * @returns Promise resolving to an OpenAI client
 */
export async function getOpenAI(options: {
  apiKey?: string;
  baseUrl?: string;
}) {
  return new OpenAI({
    apiKey: options.apiKey,
    baseURL: options.baseUrl,
  });
}

/**
 * Options specific to OpenAI text completion requests
 */
export interface OpenAITextCompletionOptions {
  /**
   * Model identifier to use
   */
  model?: string;
  
  /**
   * Timeout in milliseconds
   */
  timeout?: number;
  
  /**
   * Role of the message sender
   */
  role?: OpenAI.Chat.ChatCompletionRole;
  
  /**
   * Previous messages in the conversation
   */
  history?: Array<OpenAI.Chat.ChatCompletionMessageParam>;
  
  /**
   * Name of the message sender
   */
  name?: string;
  
  /**
   * Penalty for token frequency
   */
  frequencyPenalty?: number;
  
  /**
   * Token bias adjustments
   */
  logitBias?: Record<string, number>;
  
  /**
   * Whether to return log probabilities
   */
  logprobs?: boolean;
  
  /**
   * Number of top log probabilities to return
   */
  topLogprobs?: number;
  
  /**
   * Maximum tokens to generate
   */
  maxTokens?: number;
  
  /**
   * Number of completions to generate
   */
  n?: number;
  
  /**
   * Penalty for token presence
   */
  presencePenalty?: number;
  
  /**
   * Format for the response
   */
  responseFormat?: { type: 'text' | 'json_object' };
  
  /**
   * Random seed for deterministic results
   */
  seed?: number;
  
  /**
   * Sequences that stop generation
   */
  stop?: string | Array<string>;
  
  /**
   * Whether to stream responses
   */
  stream?: boolean;
  
  /**
   * Sampling temperature
   */
  temperature?: number;
  
  /**
   * Top-p sampling parameter
   */
  topProbability?: number;
  
  /**
   * Available tools for the model
   */
  tools?: Array<OpenAI.Chat.ChatCompletionTool>;
  
  /**
   * Tool selection behavior
   */
  toolChoice?:
    | 'none'
    | 'auto'
    | { type: 'function'; function: { name: string } };
  
  /**
   * User identifier
   */
  user?: string;
  
  /**
   * Callback for handling streaming responses
   */
  onProgress?: (partialMessage: string) => void;
}

/**
 * Configuration options specific to OpenAI client
 */
export interface OpenAIClientOptions extends AIClientOptions {
  /**
   * OpenAI API key
   */
  apiKey?: string;
  
  /**
   * OpenAI API base URL
   */
  baseUrl?: string;
}

/**
 * Client implementation for the OpenAI API
 */
export class OpenAIClient extends AIClient {
  /**
   * OpenAI client instance
   */
  protected openai!: OpenAI;
  
  /**
   * Configuration options for this client
   */
  public options: OpenAIClientOptions;

  /**
   * Creates a new OpenAIClient
   * 
   * @param options - OpenAI client configuration options
   */
  constructor(options: OpenAIClientOptions) {
    super(options);
    this.options = options;
  }

  /**
   * Sends a message to OpenAI
   * 
   * @param text - Message text
   * @param options - Message options
   * @returns Promise resolving to the OpenAI response
   */
  public async message(
    text: string,
    options: AIMessageOptions = { role: 'user' },
  ) {
    const response = await this.textCompletion(text, options);
    return response;
  }

  /**
   * Factory method to create and initialize an OpenAIClient
   * 
   * @param options - OpenAI client configuration options
   * @returns Promise resolving to an initialized OpenAIClient
   */
  public static async create(
    options: OpenAIClientOptions,
  ): Promise<OpenAIClient> {
    const client = new OpenAIClient(options);
    await client.initialize();
    return client;
  }

  /**
   * Initializes the OpenAI client
   */
  protected async initialize() {
    this.openai = new OpenAI({
      apiKey: this.options.apiKey,
      baseURL: this.options.baseUrl,
    });
  }

  /**
   * Sends a text completion request to the OpenAI API
   *
   * @param message - The message to send
   * @param options - Configuration options for the completion request
   * @returns Promise resolving to the completion text
   * @throws Error if the OpenAI API response is invalid
   */
  public async textCompletion(
    message: string,
    options: OpenAITextCompletionOptions = {},
  ): Promise<string> {
    const {
      model = 'gpt-4o',
      role = 'user',
      history = [],
      name,
      frequencyPenalty: frequency_penalty = 0,
      logitBias: logit_bias,
      logprobs = false,
      topLogprobs: top_logprobs,
      maxTokens: max_tokens,
      n = 1,
      presencePenalty: presence_penalty = 0,
      responseFormat: response_format,
      seed,
      stop,
      stream = false,
      temperature = 1,
      topProbability: top_p = 1,
      tools,
      toolChoice: tool_choice,
      user,
      onProgress,
    } = options;

    const messages = [
      ...history,
      {
        role: role as OpenAI.Chat.ChatCompletionRole,
        content: message,
      } as OpenAI.Chat.ChatCompletionSystemMessageParam,
    ];

    if (onProgress) {
      const stream = await this.openai.chat.completions.create({
        model,
        messages,
        stream: true,
        frequency_penalty,
        logit_bias,
        logprobs,
        top_logprobs,
        max_tokens,
        n,
        presence_penalty,
        response_format,
        seed,
        stop,
        temperature,
        top_p,
        tools,
        tool_choice,
        user,
      });

      let fullContent = '';
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        fullContent += content;
        onProgress(content);
      }

      return fullContent;
    } else {
      const response = await this.openai.chat.completions.create({
        model,
        messages,
        frequency_penalty,
        logit_bias,
        logprobs,
        top_logprobs,
        max_tokens,
        n,
        presence_penalty,
        response_format,
        seed,
        stop,
        stream: false,
        temperature,
        top_p,
        tools,
        tool_choice,
        user,
      });

      const choice = response.choices[0];
      if (!choice || !choice.message || !choice.message.content) {
        throw new ApiError('Invalid response from OpenAI API: Missing content', {
          model,
          responseId: response.id,
          choices: response.choices?.length || 0,
          hasChoice: !!choice,
          hasMessage: !!choice?.message,
          hasContent: !!choice?.message?.content,
        });
      }
      return choice.message.content;
    }
  }
}

/**
 * Options for getting an AI client with type information
 */
type GetAIClientOptions = OpenAIClientOptions & { type?: 'openai' };

/**
 * Factory function to create and initialize an appropriate AI client
 * 
 * @param options - Client configuration options
 * @returns Promise resolving to an initialized AI client
 * @throws Error if client type is invalid
 */
export async function getAIClient(
  options: GetAIClientOptions,
): Promise<AIClient> {
  if (options.type === 'openai') {
    return OpenAIClient.create(options);
  } else {
    throw new ValidationError('Invalid client type specified', {
      supportedTypes: ['openai'],
      providedType: options.type,
    });
  }
}
</file>

<file path="packages/pdf/src/index.ts">
import { extractText, extractImages, getDocumentProxy } from 'unpdf'
import { createWorker } from 'tesseract.js'
import fs from 'fs/promises'

/**
 * Extracts images from all pages of a PDF file
 * 
 * @param pdfPath - Path to the PDF file
 * @returns Promise resolving to an array of image objects or null if extraction fails
 * 
 * @remarks
 * This function uses unpdf's optimized PDF.js build to extract images from all pages.
 * Each image object contains the image data and metadata (width, height, channels).
 * 
 * @example
 * ```typescript
 * const images = await extractImagesFromPDF('/path/to/document.pdf');
 * if (images && images.length > 0) {
 *   console.log(`Found ${images.length} images in the PDF`);
 * }
 * ```
 */
export async function extractImagesFromPDF(
  pdfPath: string,
): Promise<any[] | null> {
  try {
    const buffer = await fs.readFile(pdfPath)
    const pdf = await getDocumentProxy(new Uint8Array(buffer))
    
    // Extract from all pages
    const allImages = []
    for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
      const images = await extractImages(pdf, pageNum)
      allImages.push(...images)
    }
    
    return allImages
  } catch (error) {
    console.error('Error extracting images:', error)
    return null
  }
}

/**
 * Extracts text content from a PDF file
 * 
 * @param pdfPath - Path to the PDF file
 * @returns Promise resolving to the extracted text or null if extraction fails
 * 
 * @remarks
 * This function first attempts to extract text using unpdf's native text extraction.
 * If no text is found (e.g., in the case of scanned PDFs), it falls back to OCR using
 * tesseract.js to extract text from the document images.
 * 
 * The function processes all pages in the PDF and merges the text.
 * 
 * @example
 * ```typescript
 * const text = await extractTextFromPDF('/path/to/document.pdf');
 * if (text) {
 *   console.log(`Extracted ${text.length} characters of text`);
 * }
 * ```
 */
export async function extractTextFromPDF(pdfPath: string): Promise<string | null> {
  try {
    const buffer = await fs.readFile(pdfPath)
    const pdf = await getDocumentProxy(new Uint8Array(buffer))
    const { text } = await extractText(pdf, { mergePages: true })
    
    // If no text was found, try OCR as a fallback
    if (!text?.trim()) {
      const images = await extractImagesFromPDF(pdfPath)
      if (images?.length) {
        return await performOCROnImages(images)
      }
    }
    
    return text || null
  } catch (error) {
    console.error(`Error extracting text from ${pdfPath}:`, error)
    return null
  }
}

/**
 * Performs OCR on image data using Tesseract.js
 * 
 * @param images - Array of image objects from PDF extraction
 * @returns Promise resolving to the OCR text
 * 
 * @remarks
 * This function processes image data through Tesseract.js OCR engine.
 * It's used as a fallback when direct text extraction fails.
 */
async function performOCROnImages(images: any[]): Promise<string> {
  if (!images || images.length === 0) {
    return ''
  }

  const worker = await createWorker()
  let ocrText = ''
  
  try {
    for (const image of images) {
      try {
        // Handle different image data formats from unpdf
        let imageData = image.data || image
        
        // Skip if no valid image data
        if (!imageData) {
          continue
        }
        
        const { data } = await worker.recognize(imageData)
        ocrText += data.text + ' '
      } catch (imageError) {
        console.warn('Failed to process image for OCR:', imageError)
        continue
      }
    }
  } finally {
    await worker.terminate()
  }
  
  return ocrText.trim()
}
</file>

<file path="packages/smrt/package.json">
{
  "name": "@have/smrt",
  "version": "0.0.50",
  "author": "Will Griffin <willgriffin@gmail.com>",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist"
  ],
  "exports": {
    ".": "./dist/index.js",
    "./utils": "./dist/utils.js"
  },
  "dependencies": {
    "@have/ai": "workspace:*",
    "@have/files": "workspace:*",
    "@have/pdf": "workspace:*",
    "@have/sql": "workspace:*",
    "@have/spider": "workspace:*",
    "@have/utils": "workspace:*",
    "@langchain/community": "^0.3.24",
    "cheerio": "^1.0.0",
    "yaml": "^2.7.0"
  },
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist",
    "prebuild": "npm run clean",
    "dev": "pnpm run build:watch & pnpm run test:watch"
  },
  "engines": {
    "node": "22.x"
  },
  "devDependencies": {
    "@faker-js/faker": "^9.4.0",
    "@paralleldrive/cuid2": "^2.2.2",
    "@types/node": "^22.13.0",
    "pdf-parse": "^1.1.1",
    "pdfjs-dist": "^4.10.38",
    "scribe.js-ocr": "^0.7.0",
    "vitest": "^3.1.1"
  }
}
</file>

<file path="packages/spider/src/crawl4ai.ts">
// request = {
//   "urls": "https://example.com",
//   "crawler_params": {
//       # Browser Configuration
//       "headless": True,                    # Run in headless mode
//       "browser_type": "chromium",          # chromium/firefox/webkit
//       "user_agent": "custom-agent",        # Custom user agent
//       "proxy": "http://proxy:8080",        # Proxy configuration

//       # Performance & Behavior
//       "page_timeout": 30000,               # Page load timeout (ms)
//       "verbose": True,                     # Enable detailed logging
//       "semaphore_count": 5,               # Concurrent request limit

//       # Anti-Detection Features
//       "simulate_user": True,               # Simulate human behavior
//       "magic": True,                       # Advanced anti-detection
//       "override_navigator": True,          # Override navigator properties

//       # Session Management
//       "user_data_dir": "./browser-data",   # Browser profile location
//       "use_managed_browser": True,         # Use persistent browser
//   }
// }

import { getLogger, NetworkError, TimeoutError } from '@have/utils';

/**
 * Factory function to create a Crawler instance
 * 
 * @returns A new Crawler instance
 */
export function getCrawler() {}

/**
 * Response from the crawler service after submitting a crawl request
 */
interface CrawlResponse {
  /**
   * Unique identifier for the crawl task
   */
  task_id: string;
}

/**
 * Status of a crawler task
 */
interface TaskStatus {
  /**
   * Current status of the task (e.g., "pending", "running", "completed")
   */
  status: string;
  
  /**
   * Additional fields in the response
   */
  [key: string]: any;
}

/**
 * Additional parameters for controlling content extraction
 */
interface ExtraParams {
  /**
   * Minimum number of words required for a text block to be included
   */
  word_count_threshold?: number;
  
  /**
   * Whether to extract only text content (no HTML)
   */
  only_text?: boolean;
  
  /**
   * Whether to bypass cache and force a fresh crawl
   */
  bypass_cache?: boolean;
  
  /**
   * Whether to process content within iframes
   */
  process_iframes?: boolean;
}

/**
 * Parameters for configuring the browser crawler
 */
interface CrawlerParams {
  /**
   * Browser Configuration
   */
  
  /**
   * Whether to run in headless mode
   */
  headless?: boolean;
  
  /**
   * Type of browser to use
   */
  browser_type?: 'chromium' | 'firefox' | 'webkit';
  
  /**
   * Custom user agent string
   */
  user_agent?: string;
  
  /**
   * Proxy configuration URL
   */
  proxy?: string;

  /**
   * Performance & Behavior
   */
  
  /**
   * Page load timeout in milliseconds
   */
  page_timeout?: number;
  
  /**
   * Whether to enable detailed logging
   */
  verbose?: boolean;
  
  /**
   * Maximum number of concurrent requests
   */
  semaphore_count?: number;
  
  /**
   * Whether to remove overlay elements like popups
   */
  remove_overlay_elements?: boolean;

  /**
   * Anti-Detection Features
   */
  
  /**
   * Whether to simulate human behavior
   */
  simulate_user?: boolean;
  
  /**
   * Whether to enable advanced anti-detection features
   */
  magic?: boolean;
  
  /**
   * Whether to override navigator properties
   */
  override_navigator?: boolean;

  /**
   * Session Management
   */
  
  /**
   * Path to browser profile directory
   */
  user_data_dir?: string;
  
  /**
   * Whether to use a persistent browser instance
   */
  use_managed_browser?: boolean;
}

/**
 * Request data for initiating a crawl
 */
interface CrawlRequest {
  /**
   * URL or array of URLs to crawl
   */
  urls: string | string[];
  
  /**
   * Browser and crawler configuration parameters
   */
  crawler_params?: CrawlerParams;
  
  /**
   * Additional parameters for content extraction
   */
  extra?: ExtraParams;
  
  /**
   * CSS selector for targeting specific elements
   */
  css_selector?: string;
}

/**
 * Client for interacting with the crawler service
 */
class Crawler {
  /**
   * Base URL of the crawler service
   */
  private baseUrl: string;

  /**
   * Creates a new Crawler instance
   * 
   * @param baseUrl - Base URL of the crawler service
   */
  constructor(baseUrl: string = 'http://localhost:11235') {
    this.baseUrl = baseUrl;
  }

  /**
   * Submits a crawl request and waits for its completion
   * 
   * @param requestData - Crawl request configuration
   * @param timeout - Maximum time to wait for completion in milliseconds
   * @returns Promise resolving to the task status with crawl results
   * @throws {TimeoutError} if the task times out
   * @throws {NetworkError} if there are network-related failures
   */
  async submitAndWait(
    requestData: CrawlRequest,
    timeout: number = 300000,
  ): Promise<TaskStatus> {
    // Submit crawl job
    const response = await fetch(`${this.baseUrl}/crawl`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestData),
    });

    if (!response.ok) {
      throw new NetworkError(
        `Failed to submit crawl request: ${response.status} ${response.statusText}`,
        { 
          url: `${this.baseUrl}/crawl`,
          status: response.status,
          statusText: response.statusText,
          requestData 
        }
      );
    }

    const responseData = await response.json() as CrawlResponse;
    if (!responseData.task_id) {
      throw new NetworkError(
        'Invalid response: missing task_id',
        { responseData, url: `${this.baseUrl}/crawl` }
      );
    }

    const { task_id } = responseData;
    getLogger().info(`Crawler task submitted`, { taskId: task_id, urls: requestData.urls });

    // Poll for result
    const startTime = Date.now();
    while (true) {
      if (Date.now() - startTime > timeout) {
        throw new TimeoutError(
          `Crawler task timed out`,
          { 
            taskId: task_id, 
            timeout, 
            elapsedTime: Date.now() - startTime 
          }
        );
      }

      const result = await fetch(`${this.baseUrl}/task/${task_id}`);
      
      if (!result.ok) {
        throw new NetworkError(
          `Failed to fetch task status: ${result.status} ${result.statusText}`,
          { 
            taskId: task_id,
            url: `${this.baseUrl}/task/${task_id}`,
            status: result.status,
            statusText: result.statusText 
          }
        );
      }

      const status = (await result.json()) as TaskStatus;

      if (status.status === 'completed') {
        return status;
      }

      await new Promise((resolve) => setTimeout(resolve, 2000));
    }
  }
}
</file>

<file path="packages/spider/src/index.spec.ts">
import { fetchPageSource, createWindow, processHtml } from './index.js';
import { it, expect } from 'vitest';

it('should fetch page source cheaply with caching', async () => {
  const result = (() => true)();
  expect(result).toBe(true);

  const source = await fetchPageSource({
    url: 'https://www.google.com',
    cheap: true,
  });

  const cached = await fetchPageSource({
    url: 'https://www.google.com',
    cheap: true,
  });

  expect(source).toBeDefined();
  expect(source).not.toBe('');
  expect(cached).toBe(source);
});

it('should fetch page source using DOM processing', async () => {
  const source = await fetchPageSource({
    url: 'https://www.google.com',
    cheap: false,
  });

  const cached = await fetchPageSource({
    url: 'https://www.google.com',
    cheap: false,
  });

  expect(cached).toBeDefined();
  expect(source).toBeDefined();
  expect(source).not.toBe('');
  expect(cached).toBe(source);
});

it('should create window instance', () => {
  const window = createWindow();
  expect(window).toBeDefined();
  expect(window.document).toBeDefined();
});

it('should process HTML correctly', async () => {
  const html = '<html><body><h1>Test</h1></body></html>';
  const processed = await processHtml(html);
  expect(processed).toContain('<h1>Test</h1>');
});
</file>

<file path="packages/sql/src/sqlite.ts">
import { createClient } from "@libsql/client";
import type {
  QueryResult,
  DatabaseInterface,
  TableInterface,
} from "./types.js";
import { buildWhere } from "./index.js";
import { DatabaseError, getLogger } from '@have/utils';

/**
 * Configuration options for SQLite database connections
 */
export interface SqliteOptions {
  /**
   * Connection URL for SQLite (e.g., "file::memory:", "file:mydb.sqlite")
   */
  url?: string;
  
  /**
   * Authentication token for Turso/LibSQL remote connections
   */
  authToken?: string;
}

/**
 * Creates a SQLite database adapter
 * 
 * @param options - SQLite connection options
 * @returns Database interface for SQLite
 */
export function getDatabase(options: SqliteOptions = {}): DatabaseInterface {
  const { url = "file::memory:", authToken } = options;
  const client = createClient({ url, authToken });

  /**
   * Inserts one or more records into a table
   * 
   * @param table - Table name
   * @param data - Single record or array of records to insert
   * @returns Promise resolving to operation result
   * @throws Error if the insert operation fails
   */
  const insert = async (
    table: string,
    data: Record<string, any> | Record<string, any>[],
  ): Promise<QueryResult> => {
    let sql: string;
    let values: any[];

    if (Array.isArray(data)) {
      const keys = Object.keys(data[0]);
      const placeholders = data
        .map(() => `(${keys.map(() => "?").join(", ")})`)
        .join(", ");
      sql = `INSERT INTO ${table} (${keys.join(", ")}) VALUES ${placeholders}`;
      values = data.reduce(
        (acc, row) => acc.concat(Object.values(row)),
        [] as any[],
      );
    } else {
      const keys = Object.keys(data);
      const placeholders = keys.map(() => "?").join(", ");
      sql = `INSERT INTO ${table} (${keys.join(", ")}) VALUES (${placeholders})`;
      values = Object.values(data);
    }
    try {
      const result = await client.execute({ sql: sql, args: values });
      return { operation: "insert", affected: result.rowsAffected };
    } catch (e) {
      throw new DatabaseError('Failed to insert records into table', {
        table,
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Retrieves a single record matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records
   * @returns Promise resolving to matching record or null if not found
   * @throws Error if the query fails
   */
  const get = async (
    table: string,
    where: Record<string, any>,
  ): Promise<Record<string, any> | null> => {
    const keys = Object.keys(where);
    const values = Object.values(where);
    const whereClause = keys.map((key) => `${key} = ?`).join(" AND ");
    const sql = `SELECT * FROM ${table} WHERE ${whereClause}`;
    try {
      const result = await client.execute({ sql: sql, args: values });
      return result.rows[0] || null;
    } catch (e) {
      throw new DatabaseError('Failed to retrieve record from table', {
        table,
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Retrieves multiple records matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records
   * @returns Promise resolving to array of matching records
   * @throws Error if the query fails
   */
  const list = async (
    table: string,
    where: Record<string, any>,
  ): Promise<Record<string, any>[]> => {
    const { sql: whereClause, values } = buildWhere(where);
    const sql = `SELECT * FROM ${table} ${whereClause}`;
    try {
      const result = await client.execute({ sql, args: values });
      return result.rows;
    } catch (e) {
      throw new DatabaseError('Failed to list records from table', {
        table,
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Updates records matching the where criteria
   * 
   * @param table - Table name
   * @param where - Criteria to match records to update
   * @param data - New data to set
   * @returns Promise resolving to operation result
   * @throws Error if the update operation fails
   */
  const update = async (
    table: string,
    where: Record<string, any>,
    data: Record<string, any>,
  ): Promise<QueryResult> => {
    const keys = Object.keys(data);
    const values = Object.values(data);
    const setClause = keys.map((key) => `${key} = ?`).join(", ");
    const whereKeys = Object.keys(where);
    const whereValues = Object.values(where);
    const whereClause = whereKeys.map((key) => `${key} = ?`).join(" AND ");

    const sql = `UPDATE ${table} SET ${setClause} WHERE ${whereClause}`;
    try {
      const result = await client.execute({
        sql,
        args: [...values, ...whereValues],
      });
      return { operation: "update", affected: result.rowsAffected };
    } catch (e) {
      throw new DatabaseError('Failed to update records in table', {
        table,
        sql,
        values: [...values, ...whereValues],
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Gets a record matching the where criteria or inserts it if not found
   * 
   * @param table - Table name
   * @param where - Criteria to match existing record
   * @param data - Data to insert if no record found
   * @returns Promise resolving to the record (either retrieved or newly inserted)
   * @throws Error if the operation fails or if the record cannot be retrieved after insert
   */
  const getOrInsert = async (
    table: string,
    where: Record<string, any>,
    data: Record<string, any>,
  ): Promise<Record<string, any>> => {
    const result = await get(table, where);
    if (result) return result;
    await insert(table, data);

    const inserted = await get(table, where);
    if (!inserted) {
      throw new DatabaseError('Failed to insert and retrieve record', {
        table,
        where,
        data,
      });
    }
    return inserted;
  };

  /**
   * Checks if a table exists in the database
   * 
   * @param tableName - Name of the table to check
   * @returns Promise resolving to boolean indicating if the table exists
   */
  const tableExists = async (tableName: string): Promise<boolean> => {
    const tableExists =
      !!(await pluck`SELECT name FROM sqlite_master WHERE type='table' AND name=${tableName}`);
    return tableExists;
  };

  /**
   * Creates a table-specific interface for simplified table operations
   * 
   * @param tableName - Table name
   * @returns TableInterface for the specified table
   */
  const table = (tableName: string): TableInterface => ({
    insert: (data) => insert(tableName, data),
    get: (where) => get(tableName, where),
    list: (where) => list(tableName, where),
  });

  /**
   * Parses a tagged template literal into a SQL query and values
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Object with SQL query and values array
   */
  const parseTemplate = (strings: TemplateStringsArray, ...vars: any[]) => {
    let sql = strings[0];
    const values = [];
    for (let i = 0; i < vars.length; i++) {
      values.push(vars[i]);
      sql += "?" + strings[i + 1];
    }
    return { sql, values };
  };

  /**
   * Executes a SQL query using template literals and returns a single value
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to a single value (first column of first row)
   * @throws Error if the query fails
   */
  const pluck = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<any> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    try {
      const result = await client.execute({ sql, args: values });
      return result.rows[0]?.[Object.keys(result.rows[0])[0]] ?? null;
    } catch (e) {
      throw new DatabaseError('Failed to execute pluck query', {
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Executes a SQL query using template literals and returns a single row
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to a single result record or null
   * @throws Error if the query fails
   */
  const single = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<Record<string, any> | null> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    try {
      const result = await client.execute({ sql, args: values });
      return result.rows[0] || null;
    } catch (e) {
      throw new DatabaseError('Failed to execute single query', {
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Executes a SQL query using template literals and returns multiple rows
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise resolving to array of result records
   * @throws Error if the query fails
   */
  const many = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<Record<string, any>[]> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    try {
      const result = await client.execute({ sql, args: values });
      return result.rows;
    } catch (e) {
      throw new DatabaseError('Failed to execute many query', {
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Executes a SQL query using template literals without returning results
   * 
   * @param strings - Template strings
   * @param vars - Variables to interpolate into the query
   * @returns Promise that resolves when the query completes
   * @throws Error if the query fails
   */
  const execute = async (
    strings: TemplateStringsArray,
    ...vars: any[]
  ): Promise<void> => {
    const { sql, values } = parseTemplate(strings, ...vars);
    try {
      await client.execute({ sql, args: values });
    } catch (e) {
      throw new DatabaseError('Failed to execute query', {
        sql,
        values,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  /**
   * Executes a raw SQL query with parameterized values
   * 
   * @param str - SQL query string
   * @param values - Variables to use as parameters
   * @returns Promise resolving to query result with rows and metadata
   * @throws Error if the query fails
   */
  const query = async (str: string, ...values: any[]) => {
    const sql = str;
    const args = Array.isArray(values[0]) ? values[0] : values;
    try {
      const result = await client.execute({ sql, args });
      return {
        command: sql.split(" ")[0].toUpperCase(),
        rowCount: result.rowsAffected ?? result.rows.length,
        oid: null,
        fields: Object.keys(result.rows[0] || {}).map((name) => ({
          name,
          tableID: 0,
          columnID: 0,
          dataTypeID: 0,
          dataTypeSize: -1,
          dataTypeModifier: -1,
          format: "text",
        })),
        rows: result.rows,
      };
    } catch (e) {
      throw new DatabaseError('Failed to execute raw query', {
        sql,
        args,
        originalError: e instanceof Error ? e.message : String(e),
      });
    }
  };

  // Shorthand aliases for query methods
  const oo = many;   // (o)bjective-(o)bjects: returns multiple rows
  const oO = single; // (o)bjective-(O)bject: returns a single row
  const ox = pluck;  // (o)bjective-(x): returns a single value
  const xx = execute; // (x)ecute-(x)ecute: executes without returning

  return {
    client,
    query,
    insert,
    update,
    get,
    list,
    getOrInsert,
    table,
    tableExists,
    many,
    single,
    pluck,
    execute,
    oo,
    oO,
    ox,
    xx,
  };
}
</file>

<file path="packages/utils/src/index.ts">
import { tmpdir } from 'os';
import path from 'path';
import { URL } from 'url';

import { v4 as uuidv4 } from 'uuid';

export enum ErrorCode {
  VALIDATION_ERROR = 'VALIDATION_ERROR',
  API_ERROR = 'API_ERROR',
  FILE_ERROR = 'FILE_ERROR',
  NETWORK_ERROR = 'NETWORK_ERROR',
  DATABASE_ERROR = 'DATABASE_ERROR',
  PARSING_ERROR = 'PARSING_ERROR',
  TIMEOUT_ERROR = 'TIMEOUT_ERROR',
  UNKNOWN_ERROR = 'UNKNOWN_ERROR',
}

export class BaseError extends Error {
  public readonly code: ErrorCode;
  public readonly context?: Record<string, unknown>;
  public readonly timestamp: Date;

  constructor(
    message: string,
    code: ErrorCode = ErrorCode.UNKNOWN_ERROR,
    context?: Record<string, unknown>,
  ) {
    super(message);
    this.name = this.constructor.name;
    this.code = code;
    this.context = context;
    this.timestamp = new Date();
    Error.captureStackTrace(this, this.constructor);
  }

  toJSON() {
    return {
      name: this.name,
      message: this.message,
      code: this.code,
      context: this.context,
      timestamp: this.timestamp.toISOString(),
      stack: this.stack,
    };
  }
}

export class ValidationError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.VALIDATION_ERROR, context);
  }
}

export class ApiError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.API_ERROR, context);
  }
}

export class FileError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.FILE_ERROR, context);
  }
}

export class NetworkError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.NETWORK_ERROR, context);
  }
}

export class DatabaseError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.DATABASE_ERROR, context);
  }
}

export class ParsingError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.PARSING_ERROR, context);
  }
}

export class TimeoutError extends BaseError {
  constructor(message: string, context?: Record<string, unknown>) {
    super(message, ErrorCode.TIMEOUT_ERROR, context);
  }
}

export interface Logger {
  debug(message: string, context?: Record<string, unknown>): void;
  info(message: string, context?: Record<string, unknown>): void;
  warn(message: string, context?: Record<string, unknown>): void;
  error(message: string, context?: Record<string, unknown>): void;
}

class ConsoleLogger implements Logger {
  debug(message: string, context?: Record<string, unknown>): void {
    if (context) {
      console.debug(message, context);
    } else {
      console.debug(message);
    }
  }

  info(message: string, context?: Record<string, unknown>): void {
    if (context) {
      console.info(message, context);
    } else {
      console.info(message);
    }
  }

  warn(message: string, context?: Record<string, unknown>): void {
    if (context) {
      console.warn(message, context);
    } else {
      console.warn(message);
    }
  }

  error(message: string, context?: Record<string, unknown>): void {
    if (context) {
      console.error(message, context);
    } else {
      console.error(message);
    }
  }
}

class NoOpLogger implements Logger {
  debug(): void {}
  info(): void {}
  warn(): void {}
  error(): void {}
}

let globalLogger: Logger = new ConsoleLogger();

export const setLogger = (logger: Logger): void => {
  globalLogger = logger;
};

export const getLogger = (): Logger => {
  return globalLogger;
};

export const disableLogging = (): void => {
  globalLogger = new NoOpLogger();
};

export const enableLogging = (): void => {
  globalLogger = new ConsoleLogger();
};

/**
 * Default temporary directory for SDK tests
 */
export const TMP_DIR = path.resolve(`${tmpdir()}/.have-sdk/tests`);

/**
 * Converts a URL to a file path by joining hostname and pathname
 * 
 * @param url - The URL to convert to a path
 * @returns The path representation of the URL (hostname/pathname)
 */
export const urlPath = (url: string) => {
  const parsedUrl = new URL(url);
  return path.join(parsedUrl.hostname, parsedUrl.pathname);
};

/**
 * Extracts the filename from a URL's pathname
 * 
 * @param url - The URL to extract filename from
 * @returns The filename from the URL or 'index.html' if no filename found
 */
export const urlFilename = (url: string) => {
  const parsedUrl = new URL(url);
  const filename = path.basename(parsedUrl.pathname);
  return filename || 'index.html';
};

/**
 * Generates a unique UUID v4 identifier
 * 
 * @returns A UUID v4 string
 */
export const makeId = () => {
  return uuidv4();
};

/**
 * Converts a string to a URL-friendly slug
 * 
 * @param str - The string to convert to a slug
 * @returns A lowercase, hyphenated slug with special characters removed
 */
export const makeSlug = (str: string) => {
  const from =
    'àáâäæãåāăąçćčđďèéêëēėęěğǵḧîïíīįìıİłḿñńǹňôöòóœøōõőṕŕřßśšşșťțûüùúūǘůűųẃẍÿýžźż+·/_,:;';
  const to =
    'aaaaaaaaaacccddeeeeeeeegghiiiiiiiilmnnnnoooooooooprrsssssttuuuuuuuuuwxyyzzz--------------';
  const textToCompare = new RegExp(
    from.split('').join('|').replace(/\+/g, '\\+'),
    'g',
  );

  return str
    .toString()
    .toLowerCase()
    .replace('&', '-38-')
    .replace(/\s+/g, '-') // Replace spaces with -
    .replace(textToCompare, (c) => to.charAt(from.indexOf(c))) // Replace special characters
    .replace(/[&.]/g, '-') // Replace DOT with -
    .replace(/[^\w-º+]+/g, '') // Remove all non-word characters, except for º, + and -
    .replace(/--+/g, '-') // Replace multiple - with single -
    .replace(/^-+/, '') // Trim - from start of text
    .replace(/-+$/, ''); // Trim - from end of text
};

/**
 * Gets current time in milliseconds using high-resolution timer
 * 
 * @returns Current time in milliseconds
 */
export const timeNow = () => {
  const time = process.hrtime();
  return Math.round(time[0] * 1e3 + time[1] / 1e6);
};

/**
 * Repeatedly calls a function until it returns a defined value or times out
 * 
 * @param it - Function to call repeatedly that returns a Promise
 * @param options - Configuration options
 * @param options.timeout - Maximum time to wait in milliseconds (0 = no timeout)
 * @param options.delay - Delay between attempts in milliseconds
 * @returns Promise that resolves with the first defined result or rejects on timeout
 */
export function waitFor(
  it: () => Promise<any>,
  { timeout = 0, delay = 1000 }: { timeout?: number; delay?: number },
) {
  return new Promise((resolve, reject) => {
    const beginTime = timeNow();
    (async function waitATick() {
      const result = await it();
      if (typeof result !== 'undefined') {
        return resolve(result);
      }
      if (timeout > 0) {
        if (timeNow() > beginTime + timeout) {
          return reject(
            new TimeoutError('Function call timed out', {
              timeout,
              delay,
              elapsedTime: timeNow() - beginTime,
            }),
          );
        }
      }
      setTimeout(waitATick, delay);
    })();
  });
}

/**
 * Creates a Promise that resolves after a specified duration
 * 
 * @param duration - Time to sleep in milliseconds
 * @returns Promise that resolves after the specified duration
 */
export const sleep = (duration: number) => {
  return new Promise<void>((resolve) => {
    getLogger().debug(`Sleeping for ${duration}ms`);
    setTimeout(resolve, duration);
  });
};

/**
 * Type guard to check if a value is an array
 * 
 * @param obj - Value to check
 * @returns True if the value is an array, false otherwise
 */
export const isArray = (obj: unknown): obj is unknown[] => {
  return Array.isArray(obj);
};

/**
 * Recursively converts all object keys to camelCase
 * 
 * @param obj - Object to convert
 * @returns Object with all keys converted to camelCase
 */
export const keysToCamel = (obj: unknown): unknown => {
  if (isPlainObject(obj)) {
    const n: Record<string, unknown> = {};
    Object.keys(obj as Record<string, unknown>).forEach(
      (k) =>
        (n[camelCase(k)] = keysToCamel((obj as Record<string, unknown>)[k])),
    );
    return n;
  } else if (isArray(obj)) {
    return (obj as unknown[]).map((i) => keysToCamel(i));
  }
  return obj;
};

/**
 * Converts a domain string to camelCase
 * 
 * @param domain - Domain string to convert
 * @returns camelCase version of the domain string
 */
export const domainToCamel = (domain: string): string => camelCase(domain);

/**
 * Recursively converts all object keys to snake_case
 * 
 * @param obj - Object to convert
 * @returns Object with all keys converted to snake_case
 */
export const keysToSnake = (obj: unknown): unknown => {
  if (isPlainObject(obj)) {
    const n: Record<string, unknown> = {};
    Object.keys(obj as Record<string, unknown>).forEach(
      (k) =>
        (n[snakeCase(k)] = keysToSnake((obj as Record<string, unknown>)[k])),
    );
    return n;
  } else if (isArray(obj)) {
    return (obj as unknown[]).map((i) => keysToSnake(i));
  }
  return obj;
};

/**
 * Parses an Amazon date string format (YYYYMMDDTHHMMSSZ) to a Date object
 * 
 * @param dateStr - Amazon format date string (YYYYMMDDTHHMMSSZ)
 * @returns Parsed Date object
 * @throws Error if the date string format is invalid
 */
export const parseAmazonDateString = (dateStr: string): Date => {
  const regex =
    /^([0-9]{4})([0-9]{2})([0-9]{2})T([0-9]{2})([0-9]{2})([0-9]{2})([A-Z0-9]+)/;
  const match = dateStr.match(regex);
  if (!match) {
    throw new ParsingError('Could not parse Amazon date string', {
      dateString: dateStr,
      expectedFormat: 'YYYYMMDDTHHMMSSZ',
    });
  }
  const [matched, year, month, day, hour, minutes, seconds, timezone] = match;
  if (matched !== dateStr) {
    throw new ParsingError('Invalid Amazon date string format', {
      dateString: dateStr,
      matched,
      expectedFormat: 'YYYYMMDDTHHMMSSZ',
    });
  }

  const date = new Date(
    `${year}-${month}-${day}T${hour}:${minutes}:${seconds}${timezone}`,
  );
  return date;
};

/**
 * Creates a visual progress indicator by cycling through a sequence of characters
 * 
 * @param tick - Current tick state or null to initialize
 * @param options - Configuration options
 * @param options.chars - Array of characters to cycle through
 * @returns The next character in the sequence
 */
export const logTicker = (
  tick: string | null,
  options: { chars?: string[] } = {},
) => {
  const { chars = ['.', '..', '...'] } = options;
  if (tick) {
    const index = chars.indexOf(tick);
    return index + 1 >= chars.length ? chars[0] : chars[index + 1];
  } else {
    return chars[0];
  }
};

// export function addInterval(dateStr: string, interval: string): string {
//   // For dates without time, assume start of day in UTC
//   let date = dateFns.parse(dateStr, 'yyyy-MM-dd', new Date());
//   if (!dateFns.isValid(date)) {
//     date = dateFns.parseISO(dateStr);
//     if (!dateFns.isValid(date)) {
//       throw new Error('Invalid date string provided');
//     }
//   }

//   // split interval into value and unit
//   let [value, unit] = interval.split(' ');

//   // use pluralize to convert the unit to singular form
//   unit = pluralize.singular(unit);

//   // create an object for the add function with proper typing
//   const addOptions: dateFns.Duration = {};
//   const unitValue = Number(value);

//   switch (unit) {
//     case 'year':
//       addOptions.years = unitValue;
//       break;
//     case 'month':
//       addOptions.months = unitValue;
//       break;
//     case 'week':
//       addOptions.weeks = unitValue;
//       break;
//     case 'day':
//       addOptions.days = unitValue;
//       break;
//     case 'hour':
//       addOptions.hours = unitValue;
//       break;
//     case 'minute':
//       addOptions.minutes = unitValue;
//       break;
//     case 'second':
//       addOptions.seconds = unitValue;
//       break;
//     default:
//       throw new Error(`Unsupported time unit: ${unit}`);
//   }

//   const calculatedDate = dateFns.add(date, addOptions);
//   return dateFns.format(calculatedDate, 'yyyy-MM-dd HH:mm:ss');
// }

/**
 * Checks if a string is a valid URL
 * 
 * @param url - String to check
 * @returns True if the string is a valid URL, false otherwise
 */
export const isUrl = (url: string): boolean => {
  try {
    const parsed = new URL(url);
    return true;
  } catch (err) {
    return false;
  }
};

/**
 * Type guard to check if a value is a plain object
 * 
 * @param obj - Value to check
 * @returns True if the value is a plain object, false otherwise
 */
export const isPlainObject = (obj: unknown): obj is Record<string, unknown> => {
  return typeof obj === 'object' && obj !== null && !Array.isArray(obj);
};

/**
 * Converts a string to camelCase
 * 
 * @param str - String to convert
 * @returns camelCase version of the string
 */
export const camelCase = (str: string): string => {
  return str
    .toLowerCase()
    .replace(/[-_]+/g, ' ')
    .replace(/[^\w\s]/g, '')
    .replace(/\s(.)/g, (_, char) => char.toUpperCase())
    .replace(/\s/g, '')
    .replace(/^(.)/, (_, char) => char.toLowerCase());
};

/**
 * Converts a string to snake_case
 * 
 * @param str - String to convert
 * @returns snake_case version of the string
 */
export const snakeCase = (str: string): string => {
  return str
    .replace(/([A-Z])/g, '_$1')
    .toLowerCase()
    .replace(/^_/, '')
    .replace(/[-\s]+/g, '_');
};

/**
 * Extracts and parses a date from a string
 * 
 * @param str - The string to parse (typically a filename)
 * @returns Date object if found, null otherwise
 */
export const dateInString = (str: string): Date | null => {
  // Get just the str without extension and path
  const cleanFilename =
    str.split('/').pop()?.replace('.pdf', '').toLowerCase() || '';

  const yearMatch = cleanFilename.match(/20\d{2}/);
  if (!yearMatch) return null;
  const year = parseInt(yearMatch[0], 10);
  const yearIndex = cleanFilename.indexOf(yearMatch[0]);

  const monthPatterns = {
    january: 1,
    jan: 1,
    february: 2,
    feb: 2,
    march: 3,
    mar: 3,
    april: 4,
    apr: 4,
    may: 5,
    june: 6,
    jun: 6,
    july: 7,
    jul: 7,
    august: 8,
    aug: 8,
    september: 9,
    sep: 9,
    october: 10,
    oct: 10,
    november: 11,
    nov: 11,
    december: 12,
    dec: 12,
  };

  // Find month and its position
  let foundMonth: number | null = null;
  let monthStart = -1;
  let monthName = '';

  for (const [name, monthNum] of Object.entries(monthPatterns)) {
    const monthIndex = cleanFilename.indexOf(name);
    if (monthIndex !== -1) {
      foundMonth = monthNum;
      monthStart = monthIndex;
      monthName = name;
      break;
    }
  }

  if (!foundMonth) return null;

  // Look for a day number before or after the month
  const beforeMonth = cleanFilename.substring(
    Math.max(0, monthStart - 15),
    monthStart,
  );
  const afterMonth = cleanFilename.substring(
    monthStart + monthName.length,
    Math.min(cleanFilename.length, monthStart + monthName.length + 15),
  );

  const dayMatch =
    beforeMonth.match(/(\d{1,2})\s*$/) || // number at the end before month
    afterMonth.match(/^\s*(\d{1,2})/) || // number at the start after month
    afterMonth.match(/(\d{1,2})/); // any number after month

  const day = dayMatch ? parseInt(dayMatch[1], 10) : null;
  if (!day) return null;

  // Construct and validate date
  const date = new Date(year, foundMonth - 1, day);
  return !isNaN(date.getTime()) ? date : null;
};

/**
 * Formats a date string into a human-readable format using the system locale
 * 
 * @param dateString - Date string to format
 * @returns Formatted date string (e.g., "January 1, 2023")
 */
export const prettyDate = (dateString: string) => {
  const date = new Date(dateString);
  return new Intl.DateTimeFormat(undefined, {
    year: 'numeric',
    month: 'long',
    day: 'numeric'
  }).format(date);
};
</file>

<file path="scripts/generate-docs.js">
#!/usr/bin/env node

import { execSync } from 'child_process';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const ROOT_DIR = path.resolve(__dirname, '..');
const DOCS_DIR = path.join(ROOT_DIR, 'docs/manual');

// Ensure docs directory exists
if (!fs.existsSync(DOCS_DIR)) {
  fs.mkdirSync(DOCS_DIR, { recursive: true });
}

// Execute TypeDoc to generate documentation
console.log('Generating API documentation...');
try {
  execSync('npx typedoc', { 
    stdio: 'inherit',
    cwd: ROOT_DIR 
  });
  console.log('API documentation generated successfully!');
  
  // Ensure .nojekyll file exists to prevent GitHub Pages from using Jekyll
  const nojekyllPath = path.join(DOCS_DIR, '.nojekyll');
  if (!fs.existsSync(nojekyllPath)) {
    fs.writeFileSync(nojekyllPath, '');
    console.log('Created .nojekyll file for GitHub Pages');
  }

  // Generate an index.html if it doesn't exist
  const indexPath = path.join(DOCS_DIR, 'index.html');
  if (!fs.existsSync(indexPath)) {
    const moduleLinks = fs.readdirSync(DOCS_DIR)
      .filter(item => fs.statSync(path.join(DOCS_DIR, item)).isDirectory() && !item.startsWith('.'))
      .map(moduleName => `<li><a href="./${moduleName}/index.html">${moduleName}</a></li>`)
      .join('\n    ');
    
    // Read the package.json to get the current version
    const packageJson = JSON.parse(fs.readFileSync(path.join(ROOT_DIR, 'package.json'), 'utf8'));
    const version = packageJson.version || 'latest';
    
    const indexContent = `<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HAVE SDK API Documentation</title>
  <meta name="description" content="API documentation for HAppy VErtical (HAVE) SDK - a TypeScript monorepo for building vertical AI agents">
  <meta name="keywords" content="AI, TypeScript, SDK, documentation, vertical AI, agent framework">
  <style>
    :root {
      --primary-color: #1a73e8;
      --secondary-color: #34a853;
      --background-color: #f8f9fa;
      --text-color: #202124;
      --card-background: #ffffff;
      --card-border: #e0e0e0;
      --header-background: #f1f3f4;
    }
    
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
      line-height: 1.6;
      color: var(--text-color);
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
      background-color: var(--background-color);
    }
    
    header {
      background-color: var(--header-background);
      padding: 20px;
      border-radius: 8px;
      margin-bottom: 30px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    h1, h2, h3 {
      color: var(--primary-color);
    }
    
    a {
      color: var(--primary-color);
      text-decoration: none;
      transition: color 0.2s ease;
    }
    
    a:hover {
      text-decoration: underline;
      color: var(--secondary-color);
    }
    
    ul {
      padding-left: 20px;
    }
    
    .package-list {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
      gap: 20px;
      margin-top: 20px;
    }
    
    .package-item {
      border: 1px solid var(--card-border);
      border-radius: 8px;
      padding: 20px;
      background-color: var(--card-background);
      box-shadow: 0 2px 4px rgba(0,0,0,0.05);
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    
    .package-item:hover {
      transform: translateY(-3px);
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    
    .package-item h3 {
      margin-top: 0;
      border-bottom: 1px solid #eee;
      padding-bottom: 10px;
    }
    
    .package-item a {
      display: inline-block;
      margin-top: 10px;
      font-weight: 500;
    }
    
    .github-link {
      display: inline-block;
      margin-top: 15px;
      padding: 8px 16px;
      background-color: var(--primary-color);
      color: white;
      border-radius: 4px;
      font-weight: 500;
      text-align: center;
    }
    
    .github-link:hover {
      background-color: var(--secondary-color);
      text-decoration: none;
    }
    
    .footer {
      margin-top: 50px;
      padding-top: 20px;
      border-top: 1px solid var(--card-border);
      font-size: 0.9em;
      color: #666;
      text-align: center;
    }
    
    @media (max-width: 768px) {
      .package-list {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>
  <header>
    <h1>HAVE SDK API Documentation</h1>
    <p>
      The HAppy VErtical (HAVE) SDK is a TypeScript monorepo designed for building vertical AI agents.
    </p>
    <a href="https://github.com/happyvertical/sdk" class="github-link" target="_blank">View on GitHub</a>
  </header>
  
  <section>
    <h2>Core Principles</h2>
    <ul>
      <li>Pure TypeScript implementation to avoid CommonJS vs ESM compatibility issues</li>
      <li>Minimized dependencies through a modular monorepo architecture</li>
      <li>Compartmentalized code to keep AI agents lean and focused</li>
      <li>Support for testing and scaling with minimal overhead</li>
      <li>Standardized interfaces across different packages</li>
    </ul>
  </section>

  <section>
    <h2>Package Documentation</h2>
    <div class="package-list">
      <div class="package-item">
        <h3>@have/ai</h3>
        <p>A standardized interface for AI model interactions, currently supporting OpenAI</p>
        <a href="./ai/src/README.html">View Documentation</a>
      </div>
      <div class="package-item">
        <h3>@have/files</h3>
        <p>Tools for interacting with file systems (local and remote)</p>
        <a href="./files/src/README.html">View Documentation</a>
      </div>
      <div class="package-item">
        <h3>@have/pdf</h3>
        <p>Utilities for parsing and processing PDF documents</p>
        <a href="./pdf/src/README.html">View Documentation</a>
      </div>
      <div class="package-item">
        <h3>@have/smrt</h3>
        <p>Core library for building AI agents with standardized collections and objects</p>
        <a href="./smrt/src/README.html">View Documentation</a>
      </div>
      <div class="package-item">
        <h3>@have/spider</h3>
        <p>Web crawling and content parsing tools</p>
        <a href="./spider/src/README.html">View Documentation</a>
      </div>
      <div class="package-item">
        <h3>@have/sql</h3>
        <p>Database interaction with support for SQLite and Postgres</p>
        <a href="./sql/src/README.html">View Documentation</a>
      </div>
      <div class="package-item">
        <h3>@have/utils</h3>
        <p>Shared utility functions used across packages</p>
        <a href="./utils/src/README.html">View Documentation</a>
      </div>
    </div>
  </section>

  <div class="footer">
    <p>Generated on ${new Date().toLocaleDateString()} | HAVE SDK v${version}</p>
    <p>© ${new Date().getFullYear()} Happy Vertical</p>
  </div>
</body>
</html>`;
    
    fs.writeFileSync(indexPath, indexContent);
    console.log('Generated custom index.html');
  }
} catch (error) {
  console.error('Error generating documentation:', error);
  process.exit(1);
}
</file>

<file path="scripts/validate-build.js">
#!/usr/bin/env node

import fs from 'fs/promises';
import path from 'path';

/**
 * Validates that all packages referenced in the build script actually exist
 */
async function validateBuild() {
  try {
    // Read package.json
    const packageJsonPath = path.join(process.cwd(), 'package.json');
    const packageJsonContent = await fs.readFile(packageJsonPath, 'utf8');
    const packageJson = JSON.parse(packageJsonContent);
    
    // Extract build script
    const buildScript = packageJson.scripts?.build;
    if (!buildScript) {
      console.error('❌ No build script found in package.json');
      process.exit(1);
    }
    
    // Extract package names from build script using regex
    const packageMatches = buildScript.match(/--filter @have\/\w+/g) || [];
    const referencedPackages = packageMatches.map(match => 
      match.replace('--filter @have/', '')
    );
    
    console.log('📦 Packages referenced in build script:', referencedPackages);
    
    // Check if packages directory exists
    const packagesDir = path.join(process.cwd(), 'packages');
    try {
      await fs.access(packagesDir);
    } catch {
      console.error('❌ packages directory not found');
      process.exit(1);
    }
    
    // Get actual package directories
    const packageDirs = await fs.readdir(packagesDir, { withFileTypes: true });
    const existingPackages = packageDirs
      .filter(dirent => dirent.isDirectory())
      .map(dirent => dirent.name);
    
    console.log('📁 Existing packages:', existingPackages);
    
    // Check for missing packages
    const missingPackages = referencedPackages.filter(
      pkg => !existingPackages.includes(pkg)
    );
    
    if (missingPackages.length > 0) {
      console.error('❌ Missing packages referenced in build script:', missingPackages);
      console.error('   Either create these packages or remove them from the build script');
      process.exit(1);
    }
    
    // Check for extra packages not in build script
    const extraPackages = existingPackages.filter(
      pkg => !referencedPackages.includes(pkg)
    );
    
    if (extraPackages.length > 0) {
      console.warn('⚠️  Packages exist but not in build script:', extraPackages);
      console.warn('   Consider adding them to the build script if they need building');
      
      // Validate that each package has a package.json
      for (const pkg of extraPackages) {
        const pkgJsonPath = path.join(packagesDir, pkg, 'package.json');
        try {
          await fs.access(pkgJsonPath);
          const pkgContent = await fs.readFile(pkgJsonPath, 'utf8');
          const pkgJson = JSON.parse(pkgContent);
          if (pkgJson.scripts?.build) {
            console.warn(`   📦 ${pkg} has a build script but is not in the main build`);
          }
        } catch {
          console.warn(`   ❌ ${pkg} missing package.json`);
        }
      }
    }
    
    console.log('✅ Build validation passed - all referenced packages exist');
    return true;
    
  } catch (error) {
    console.error('❌ Error during build validation:', error.message);
    process.exit(1);
  }
}

validateBuild();
</file>

<file path=".claude/commands/README.md">
# Claude Issue Commands

Simple, powerful issue management that automatically advances issues through the complete workflow.

## Individual Issue Command

### `/issue <issue_number>`

**Claude automatically analyzes and executes the appropriate workflow action.**

This command intelligently advances issues through the workflow by detecting the current state and performing the next appropriate action:

**Workflow Progression:**
- **Fresh** → Triages (search duplicates, assess clarity, move to backlog/icebox)
- **Icebox** → Reviews relevance, moves to backlog or closes if stale
- **Backlog** → Checks Definition of Ready, requests info or moves to to do
- **To Do** → Assigns self, creates branch, starts implementation
- **Developing** → Implements solution or creates PR when ready
- **Quality Assurance** → Handles feedback or merges when approved
- **Deploying** → Triggers deployment if automated
- **Done** → Monitors for issues, closes when stable

**Re-run Behavior:**
When run again on the same issue, Claude checks for new comments/feedback and acts accordingly.

## Examples

```
/issue 1    # Analyze and advance issue #1
/issue 22   # Work on issue #22
```

## Lane Commands

Process all issues assigned to you in a specific workflow stage:

### `/fresh [notes]`
Processes all "Fresh" issues - performs triage, checks for duplicates, moves to backlog/icebox

### `/icebox [notes]`
Processes all "Icebox" issues - reviews relevance, promotes to backlog or closes stale

### `/backlog [notes]`
Processes all "Backlog" issues - applies Definition of Ready, moves ready items to "To Do"

### `/todo [notes]`
Processes all "To Do" issues - creates branches, starts implementation, moves to "Developing"

### `/develop [notes]`
Processes all "Developing" issues - continues development, creates PRs when ready

### `/qa [notes]`
Processes all "Quality Assurance" issues - handles feedback, merges approved PRs

### `/deploy [notes]`
Processes all "Deploying" issues - merges and deploys to production

### `/issues-close [notes]`
Processes all "Done" issues - monitors stability, closes completed work

## Notes Parameter
All lane commands accept optional notes to guide processing:
- `"all in the same pr"` - Group multiple issues in single PR
- `"prioritize security"` - Focus on security-related issues first
- `"quick triage"` - Fast processing with minimal analysis

## Setup

These commands automatically follow the workflow defined in `docs/workflow/KANBAN.md` and require:
- GitHub CLI (`gh`) authenticated
- Git repository with proper remote configuration
- Appropriate permissions to manage issues and create PRs
</file>

<file path=".github/workflows/claude.yaml">
name: Claude PR Assistant

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review:
    types: [submitted]

jobs:
  claude-code-action:
    if: |
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && contains(github.event.issue.body, '@claude'))
    runs-on: ubuntu-latest
    permissions:
      contents: read         # Required to checkout repository
      pull-requests: read   # Required to read PR comments and reviews
      issues: read          # Required to read issue comments
      id-token: write      # Required by claude-code-action for OIDC authentication
    steps:
      - name: Validate required secrets
        run: |
          if [ -z "${{ secrets.ANTHROPIC_API_KEY }}" ]; then
            echo "::error::ANTHROPIC_API_KEY secret is not configured"
            exit 1
          fi
          echo "✅ Required secrets are configured"

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude PR Action
        uses: anthropics/claude-code-action@v1.1.0  # Pinned to specific version
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
</file>

<file path=".github/workflows/on-pr-master-dependabot.yml">
name: Dependabot Automation

on:
  pull_request:
    branches: [ master ]

permissions:
  contents: write      # Required for auto-merge functionality
  pull-requests: write # Required for approving and merging PRs

jobs:
  dependabot:
    runs-on: ubuntu-latest
    if: ${{ github.actor == 'dependabot[bot]' }}
    steps:
      - name: Validate security requirements
        run: |
          if [ -z "${{ secrets.GITHUB_TOKEN }}" ]; then
            echo "::error::GITHUB_TOKEN is not available"
            exit 1
          fi
          echo "✅ Security requirements validated"
          echo "::notice::Processing dependabot PR with enhanced security checks"

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          
      - name: Setup pnpm
        uses: pnpm/action-setup@v4  # Pinned to specific major version
        with:
          version: 9  # Pinned to specific major version instead of latest
          
      - name: Install dependencies
        run: pnpm install
        
      - name: Build packages
        run: pnpm build
        
      - name: Run tests
        run: pnpm test
        
      - name: Dependabot metadata
        id: metadata
        uses: dependabot/fetch-metadata@v2.2.0  # Pinned to specific version
        with:
          github-token: "${{ secrets.GITHUB_TOKEN }}"
          
      - name: Security scan for dependency changes
        run: |
          echo "::notice::Checking for security vulnerabilities in dependencies"
          echo "Dependency update type: ${{ steps.metadata.outputs.update-type }}"
          echo "Package ecosystem: ${{ steps.metadata.outputs.package-ecosystem }}"
          echo "Package name: ${{ steps.metadata.outputs.dependency-names }}"
          pnpm audit --audit-level moderate || {
            echo "::error::Security vulnerabilities found in dependencies"
            exit 1
          }
          echo "✅ No security vulnerabilities detected"

      - name: Approve PR with security validation
        if: ${{ steps.metadata.outputs.update-type != 'version-update:semver-major' }}
        run: |
          echo "::notice::Approving dependabot PR after security validation"
          gh pr review --approve "$PR_URL"
        env:
          PR_URL: ${{ github.event.pull_request.html_url }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Enable auto-merge for minor and patch updates
        if: ${{ steps.metadata.outputs.update-type == 'version-update:semver-minor' || steps.metadata.outputs.update-type == 'version-update:semver-patch' }}
        run: |
          echo "::notice::Enabling auto-merge for ${{ steps.metadata.outputs.update-type }} update"
          gh pr merge --auto --merge "$PR_URL"
        env:
          PR_URL: ${{ github.event.pull_request.html_url }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Require manual review for major updates
        if: ${{ steps.metadata.outputs.update-type == 'version-update:semver-major' }}
        run: |
          echo "Major version update detected. Manual review required."
          echo "This PR will not be automatically approved or merged."
          gh pr comment "$PR_URL" --body "⚠️ **Major version update detected**. Manual review is required before merging."
        env:
          PR_URL: ${{ github.event.pull_request.html_url }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="docs/workflow/DEFINITION_OF_READY.md">
# Definition of Ready

## Overview

An issue is considered "Ready" when it contains sufficient information and context for a developer to begin work without significant clarification or discovery. This definition ensures efficient development and reduces context switching.

The Definition of Ready is checked when transitioning issues from "To Do" to "Developing" status, serving as a quality gate before development begins.

## Criteria

### 1. User Story / Problem Statement
- [ ] Clear description of the problem being solved or feature being added
- [ ] User story follows format: "As a [user type], I want [goal] so that [benefit]" (for features)
- [ ] For bugs: Steps to reproduce, expected behavior, and actual behavior are documented

### 2. Acceptance Criteria
- [ ] Specific, measurable conditions that must be met for the issue to be considered complete
- [ ] Written in clear, testable language
- [ ] Edge cases and error scenarios are considered
- [ ] Performance requirements specified (if applicable)

### 3. Implementation Gameplan
- [ ] Detailed plan of what code changes will be made
- [ ] Specific files/modules to be modified or created are identified
- [ ] Major architectural decisions and patterns are documented
- [ ] Integration points with existing code are clear
- [ ] Dependencies on other issues or external systems are identified
- [ ] Breaking changes or migration requirements are noted
- [ ] Potential risks or implementation considerations are acknowledged

### 4. Design Assets (if applicable)
- [ ] UI mockups or wireframes provided for frontend changes
- [ ] API contracts defined for backend changes
- [ ] Database schema changes documented

### 5. Estimation
- [ ] Relative complexity estimated (e.g., story points, t-shirt sizes)
- [ ] Time estimate provided if using time-based planning

### 6. No Blockers
- [ ] All dependencies are resolved or have clear timelines
- [ ] Required permissions or access are available
- [ ] Prerequisite issues are completed or in progress

### 7. Testing Considerations
- [ ] Test scenarios outlined
- [ ] Test data requirements identified
- [ ] Integration test requirements specified

## Examples

### Good Acceptance Criteria
 "When a user clicks the 'Export' button, a CSV file containing all visible table data should download with columns matching the table headers"

### Poor Acceptance Criteria
"Add export functionality"

### Good Bug Report
 
```
Steps to Reproduce:
1. Navigate to /dashboard
2. Click on "Add Widget"
3. Select "Chart" type
4. Click "Save" without entering a name

Expected: Validation error appears
Actual: Application crashes with 500 error
```

### Poor Bug Report
"Dashboard is broken"

## Workflow Integration

The Definition of Ready is applied when transitioning issues from "Todo" to "In Progress" status in the GitHub Project board. This ensures that:

1. **Todo items can be rougher** - Allowing for faster triage and prioritization without full refinement
2. **Just-in-time validation** - Readiness is verified only when development is about to begin
3. **Reduced waste** - Effort is not spent refining items that may never be worked on
4. **Fresh context** - Requirements and implementation gameplan are validated at the point of implementation

If an issue fails the Definition of Ready check, it remains in "Todo" status with feedback on what needs to be addressed before development can begin.
</file>

<file path="packages/ai/package.json">
{
  "name": "@have/ai",
  "version": "0.0.50",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "exports": {
    ".": "./dist/index.js"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist",
    "prebuild": "npm run clean",
    "dev": "pnpm run build:watch & pnpm run test:watch"
  },
  "engines": {
    "node": "22.x"
  },
  "dependencies": {
    "@have/utils": "workspace:*",
    "openai": "^4.78.1"
  },
  "devDependencies": {
    "@types/node": "^22.13.0",
    "vitest": "^3.1.1"
  }
}
</file>

<file path="packages/files/src/index.ts">
import { statSync, createWriteStream, Dirent, existsSync } from 'node:fs';
import {
  copyFile,
  mkdir,
  readdir,
  writeFile,
  readFile,
} from 'node:fs/promises';
import { tmpdir } from 'node:os';
import * as path from 'node:path';
import { dirname } from 'node:path';
import { URL } from 'node:url';

/**
 * Default temporary directory for caching and intermediate files
 */
const TMP_DIR = path.resolve(`${tmpdir()}/kissd`);

/**
 * Checks if a path is a file
 * 
 * @param file - Path to check
 * @returns File stats if the path is a file, false otherwise
 */
export const isFile = (file: string): false | ReturnType<typeof statSync> => {
  try {
    const fileStat = statSync(file);
    return fileStat.isDirectory() ? false : fileStat;
  } catch {
    return false;
  }
};

/**
 * Checks if a path is a directory
 * 
 * @param dir - Path to check
 * @returns True if the path is a directory, false if it doesn't exist
 * @throws Error if the path exists but is not a directory
 */
export const isDirectory = (dir: string): boolean => {
  try {
    const dirStat = statSync(dir);
    if (dirStat.isDirectory()) return true;
    throw new Error(`${dir} exists but isn't a directory`);
  } catch (error) {
    if (error instanceof Error && error.message.includes('ENOENT')) {
      return false;
    }
    throw error;
  }
};

/**
 * Creates a directory if it doesn't exist
 * 
 * @param dir - Directory path to create
 * @returns Promise that resolves when the directory exists or has been created
 */
export const ensureDirectoryExists = async (dir: string): Promise<void> => {
  if (!isDirectory(dir)) {
    console.log(`Creating directory: ${dir}`);
    await mkdir(dir, { recursive: true });
  }
};

/**
 * Uploads data to a URL using PUT method
 * 
 * @param url - URL to upload data to
 * @param data - String or Buffer data to upload
 * @returns Promise that resolves with the Response object
 * @throws Error if the upload fails
 */
export const upload = async (
  url: string,
  data: string | Buffer,
): Promise<Response> => {
  try {
    const response = await fetch(url, {
      method: 'PUT',
      body: data,
      headers: { 'Content-Type': 'application/octet-stream' },
    });

    if (!response.ok) {
      throw new Error(`unexpected response ${response.statusText}`);
    }
    return response;
  } catch (error) {
    const err = error as Error;
    console.error(`Error uploading data to ${url}\nError: ${err.message}`);
    throw error; // Re-throw to allow proper error handling
  }
};

/**
 * Downloads a file from a URL and saves it to a local file
 * 
 * @param url - URL to download from
 * @param filepath - Local file path to save to
 * @returns Promise that resolves when the download is complete
 * @throws Error if the download fails
 */
export async function download(url: string, filepath: string): Promise<void> {
  try {
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`Unexpected response ${response.statusText}`);
    }

    const fileStream = createWriteStream(filepath);
    
    return new Promise<void>((resolve, reject) => {
      fileStream.on('error', reject);
      fileStream.on('finish', resolve);
      
      response.body?.pipeTo(
        new WritableStream({
          write(chunk) {
            fileStream.write(Buffer.from(chunk));
          },
          close() {
            fileStream.end();
          },
          abort(reason) {
            fileStream.destroy();
            reject(reason);
          },
        }),
      ).catch(reject);
    });
  } catch (error) {
    const err = error as Error;
    console.error('Error downloading file:', err);
    throw error;
  }
}

/**
 * Downloads a file with caching support
 * 
 * @param url - URL to download from
 * @param targetPath - Optional custom target path
 * @returns Promise that resolves with the path to the downloaded file
 */
export const downloadFileWithCache = async (
  url: string,
  targetPath: string | null = null,
): Promise<string> => {
  const parsedUrl = new URL(url);

  console.log(targetPath);
  const downloadPath =
    targetPath ||
    `${TMP_DIR}/downloads/${parsedUrl.hostname}${parsedUrl.pathname}`;

  console.log('downloadPath', downloadPath);
  if (!isFile(downloadPath)) {
    await ensureDirectoryExists(dirname(downloadPath));
    await download(url, downloadPath);
  }
  return downloadPath;
};

/**
 * Options for listing files in a directory
 */
interface ListFilesOptions {
  /**
   * Optional regular expression to filter files by name
   */
  match?: RegExp;
}

/**
 * Lists files in a directory with optional filtering
 * 
 * @param dirPath - Directory path to list files from
 * @param options - Filtering options
 * @returns Promise that resolves with an array of file names
 */
export const listFiles = async (
  dirPath: string,
  options: ListFilesOptions = { match: /.*/ },
): Promise<string[]> => {
  const entries: Dirent[] = await readdir(dirPath, { withFileTypes: true });
  const files = entries
    .filter((entry: Dirent) => entry.isFile())
    .map((entry: Dirent) => entry.name);

  return options.match
    ? files.filter((item) => options.match?.test(item))
    : files;
};

/**
 * Gets data from cache if available and not expired
 * 
 * @param file - Cache file identifier
 * @param expiry - Cache expiry time in milliseconds
 * @returns Promise that resolves with the cached data or undefined if not found/expired
 */
export async function getCached(file: string, expiry: number = 300000) {
  const cacheFile = path.resolve(TMP_DIR, file);
  const cached = existsSync(cacheFile);
  if (cached) {
    const stats = statSync(cacheFile);
    const modTime = new Date(stats.mtime);
    const now = new Date();
    const isExpired = expiry && now.getTime() - modTime.getTime() > expiry;
    if (!isExpired) {
      return await readFile(cacheFile, 'utf8');
    }
  }
}

/**
 * Sets data in cache
 * 
 * @param file - Cache file identifier
 * @param data - Data to cache
 * @returns Promise that resolves when the data is cached
 */
export async function setCached(file: string, data: string) {
  const cacheFile = path.resolve(TMP_DIR, file);
  await ensureDirectoryExists(path.dirname(cacheFile));
  await writeFile(cacheFile, data);
}

/**
 * Map of file extensions to MIME types
 */
const mimeTypes: { [key: string]: string } = {
  '.html': 'text/html',
  '.js': 'application/javascript',
  '.json': 'application/json',
  '.css': 'text/css',
  '.png': 'image/png',
  '.jpg': 'image/jpeg',
  '.jpeg': 'image/jpeg',
  '.gif': 'image/gif',
  '.txt': 'text/plain',
  '.doc': 'application/msword',
  '.docx':
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
  '.xls': 'application/vnd.ms-excel',
  '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
  '.pdf': 'application/pdf',
  '.xml': 'application/xml',
  '.zip': 'application/zip',
  '.rar': 'application/x-rar-compressed',
  '.mp3': 'audio/mpeg',
  '.mp4': 'video/mp4',
  '.avi': 'video/x-msvideo',
  '.mov': 'video/quicktime',
  // Add more mappings as needed
};

/**
 * Gets the MIME type for a file or URL based on its extension
 * 
 * @param fileOrUrl - File path or URL to get MIME type for
 * @returns MIME type string, defaults to 'application/octet-stream' if not found
 */
export function getMimeType(fileOrUrl: string): string {
  const urlPattern = /^[a-zA-Z][a-zA-Z\d+\-.]*:\/\//; // Matches any valid URL scheme
  let extension: string;

  if (urlPattern.test(fileOrUrl)) {
    // It's a URL, extract the pathname
    const url = new URL(fileOrUrl);
    extension = path.extname(url.pathname);
  } else {
    // It's a file path
    extension = path.extname(fileOrUrl);
  }

  return mimeTypes[extension.toLowerCase()] || 'application/octet-stream';
}

export * from './fetch.js';
export * from './filesystem.js';
</file>

<file path="packages/files/package.json">
{
  "name": "@have/files",
  "version": "0.0.50",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "exports": {
    ".": "./dist/index.js"
  },
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist tsconfig.tsbuildinfo",
    "prebuild": "npm run clean",
    "dev": "pnpm run build:watch & pnpm run test:watch"
  },
  "engines": {
    "node": "22.x"
  },
  "dependencies": {
    "@have/utils": "workspace:*"
  },
  "devDependencies": {
    "@types/node": "^22.13.0",
    "vitest": "^3.1.1"
  },
  "files": [
    "dist"
  ]
}
</file>

<file path="packages/pdf/package.json">
{
  "name": "@have/pdf",
  "version": "0.0.50",
  "description": "Modern PDF processing utilities with text extraction and OCR support using unpdf and Tesseract.js",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist"
  ],
  "exports": {
    ".": "./dist/index.js"
  },
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist",
    "prebuild": "npm run clean",
    "dev": "pnpm run build:watch & pnpm run test:watch"
  },
  "engines": {
    "node": ">=22.0.0"
  },
  "dependencies": {
    "tesseract.js": "^6.0.0",
    "unpdf": "^1.0.6"
  },
  "devDependencies": {
    "@types/node": "^22.13.0",
    "vitest": "^3.1.1"
  }
}
</file>

<file path="packages/spider/src/index.ts">
import path from 'path';
import { Window } from 'happy-dom';
import { request } from 'undici';
import { urlFilename, urlPath, getLogger, ValidationError, NetworkError, ParsingError, isUrl } from '@have/utils';
import * as cheerio from 'cheerio';

import { fetchText, getCached, setCached } from '@have/files';

/**
 * Options for fetching a web page's source
 */
interface FetchPageSourceOptions {
  /**
   * URL to fetch
   */
  url: string;
  
  /**
   * Whether to use a simple HTTP fetch instead of DOM processing
   */
  cheap: boolean;
  
  /**
   * Whether to use cached content if available
   */
  cache?: boolean;
  
  /**
   * Cache expiry time in milliseconds
   */
  cacheExpiry?: number;
  
  /**
   * Custom headers to include with the request
   */
  headers?: Record<string, string>;
  
  /**
   * Timeout for the request in milliseconds
   */
  timeout?: number;
}

/**
 * Fetches the HTML source of a web page using either a simple HTTP request or DOM processing
 * 
 * @param options - Configuration options for the fetch operation
 * @returns Promise resolving to the HTML content of the page
 * @throws {ValidationError} if the URL is invalid
 * @throws {NetworkError} if there are network-related failures
 */
export async function fetchPageSource(
  options: FetchPageSourceOptions,
): Promise<string> {
  const { 
    url, 
    cheap = true, 
    cacheExpiry = 300000, 
    headers = {}, 
    timeout = 30000 
  } = options;

  // Validate URL
  if (!url || typeof url !== 'string') {
    throw new ValidationError('URL is required and must be a string', { url });
  }

  if (!isUrl(url)) {
    throw new ValidationError('Invalid URL format', { url });
  }

  if (cheap) {
    const cachedFile = path.join(urlPath(url), '.cheap', urlFilename(url));
    const cached = await getCached(cachedFile, cacheExpiry);
    if (cached) {
      getLogger().info('Using cached page source', { url, cacheFile: cachedFile });
      return cached;
    }

    const content = await fetchText(url);
    await setCached(cachedFile, content);
    return content;
  }

  const cachedFile = path.join(urlPath(url), urlFilename(url));
  const cached = await getCached(cachedFile, cacheExpiry);
  if (cached) {
    return cached;
  }

  try {
    const defaultHeaders = {
      'User-Agent': 'Mozilla/5.0 (compatible; HAppyVertical Spider/1.0)',
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
      'Accept-Language': 'en-US,en;q=0.5',
      'Accept-Encoding': 'gzip, deflate',
      'DNT': '1',
      'Connection': 'keep-alive',
      'Upgrade-Insecure-Requests': '1',
      ...headers
    };

    const response = await request(url, {
      method: 'GET',
      headers: defaultHeaders,
      headersTimeout: timeout,
      bodyTimeout: timeout
    });

    if (response.statusCode >= 400) {
      throw new NetworkError(
        `HTTP ${response.statusCode}: ${response.headers['status'] || 'Request failed'}`,
        { url, statusCode: response.statusCode, headers: response.headers }
      );
    }

    const content = await response.body.text();
    
    // Process the HTML with happy-dom to ensure it's well-formed
    const window = new Window();
    const document = window.document;
    document.documentElement.innerHTML = content;
    
    const processedContent = document.documentElement.outerHTML;
    
    await setCached(cachedFile, processedContent);
    return processedContent;
  } catch (error) {
    if (error instanceof Error) {
      throw new NetworkError(
        `Failed to fetch page source: ${error.message}`,
        { url, error: error.message, stack: error.stack }
      );
    }
    throw error;
  }
}

/**
 * Parses an HTML page to extract links or content
 * 
 * @param indexSource - HTML source to parse
 * @returns Promise resolving to an array of URLs extracted from the page
 * @throws {ValidationError} if the HTML source is invalid
 * @throws {ParsingError} if HTML parsing fails
 */
export async function parseIndexSource(indexSource: string): Promise<string[]> {
  if (!indexSource || typeof indexSource !== 'string') {
    throw new ValidationError('HTML source is required and must be a string', { indexSource });
  }

  try {
    const $ = cheerio.load(indexSource);

    const items: string[] = [];
    let content: string = '';

    // Check if it's an index page by looking for multiple items
    if ($('a').length > 1) {
      $('a').each((_, element) => {
        const item = $(element).attr('href');
        // console.log({ item });
        if (item) items.push(item);
      });
    } else {
      // Assume it's a content page if not an index
      content = $('body').text().trim();
    }
    return items;
  } catch (error) {
    if (error instanceof Error) {
      throw new ParsingError(
        `Failed to parse HTML source: ${error.message}`,
        { error: error.message, stack: error.stack }
      );
    }
    throw error;
  }
}

/**
 * Creates a new happy-dom window instance for DOM manipulation
 * 
 * @returns A new Window instance
 */
export function createWindow(): Window {
  return new Window();
}

/**
 * Processes HTML content using happy-dom to ensure proper DOM structure
 * 
 * @param html - HTML content to process
 * @returns Promise resolving to the processed HTML
 * @throws {ParsingError} if HTML processing fails
 */
export async function processHtml(html: string): Promise<string> {
  try {
    const window = new Window();
    const document = window.document;
    document.documentElement.innerHTML = html;
    
    return document.documentElement.outerHTML;
  } catch (error) {
    if (error instanceof Error) {
      throw new ParsingError(
        `Failed to process HTML: ${error.message}`,
        { error: error.message, stack: error.stack }
      );
    }
    throw error;
  }
}

export default {
  fetchPageSource,
  parseIndexSource,
  createWindow,
  processHtml,
};
</file>

<file path="packages/spider/package.json">
{
  "name": "@have/spider",
  "version": "0.0.50",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist"
  ],
  "exports": {
    ".": "./dist/index.js"
  },
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist",
    "prebuild": "npm run clean",
    "dev": "pnpm run build:watch & pnpm run test:watch"
  },
  "description": "Web scraping and content extraction using happy-dom and undici for lightweight, fast operations",
  "engines": {
    "node": ">=22.0.0"
  },
  "dependencies": {
    "@have/files": "workspace:*",
    "@have/utils": "workspace:*",
    "@mozilla/readability": "^0.5.0",
    "cheerio": "^1.0.0",
    "happy-dom": "^18.0.1",
    "undici": "^7.11.0"
  },
  "devDependencies": {
    "@types/node": "^22.13.0",
    "vitest": "^3.1.1"
  }
}
</file>

<file path="packages/sql/package.json">
{
  "name": "@have/sql",
  "version": "0.0.50",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist"
  ],
  "exports": {
    ".": "./dist/index.js"
  },
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist",
    "prebuild": "npm run clean",
    "dev": "pnpm run build:watch & pnpm run test:watch"
  },
  "keywords": [],
  "author": "willgriffin@gmail.com",
  "license": "ISC",
  "dependencies": {
    "@have/utils": "workspace:*",
    "@libsql/client": "^0.14.0",
    "sqlite-vss": "^0.1.2",
    "pg": "^8.13.1"
  },
  "devDependencies": {
    "@types/node": "^22.13.0",
    "@types/pg": "^8.11.10",
    "vitest": "^3.1.1"
  },
  "engines": {
    "node": "22.x"
  }
}
</file>

<file path="packages/utils/package.json">
{
  "name": "@have/utils",
  "version": "0.0.50",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "exports": {
    ".": "./dist/index.js"
  },
  "scripts": {
    "test": "vitest",
    "test:watch": "vitest --watch",
    "build": "tsc -b",
    "build:watch": "tsc -b --watch",
    "clean": "rm -rf dist tsconfig.tsbuildinfo"
  },
  "engines": {
    "node": "22.x"
  },
  "dependencies": {
    "@paralleldrive/cuid2": "^2.2.2",
    "date-fns": "^3.3.1",
    "pluralize": "^8.0.0",
    "uuid": "^9.0.1"
  },
  "devDependencies": {
    "@types/node": "^22.13.0",
    "@types/pluralize": "^0.0.33",
    "@types/uuid": "^10.0.0",
    "vitest": "^3.1.1"
  },
  "files": [
    "dist"
  ]
}
</file>

<file path="typedoc.json">
{
  "entryPoints": ["packages/*/src/index.ts"],
  "entryPointStrategy": "packages",
  "out": "docs/manual",
  "name": "HAVE SDK API Reference",
  "excludePrivate": true,
  "excludeInternal": true,
  "theme": "default",
  "readme": "README.md",
  "plugin": ["typedoc-plugin-markdown"],
  "categorizeByGroup": true,
  "includeVersion": true,
  "hideGenerator": true
}
</file>

<file path="lefthook.yml">
pre-commit:
  commands:
    format:
      glob: "*.{js,ts,cjs,mjs,jsx,tsx,json}"
      run: |
        if [ -n "{staged_files}" ]; then
          pnpm biome format --write {staged_files}
        fi
      stage_fixed: true
    lint:
      glob: "*.{js,ts,cjs,mjs,jsx,tsx,json}"
      run: |
        if [ -n "{staged_files}" ]; then
          pnpm biome lint --write {staged_files}
        fi
      stage_fixed: true
    validate-package-json:
      glob: "**/package.json"
      run: node scripts/validate-package-json.js {staged_files}
    validate-changeset-config:
      glob: ".changeset/**/*"
      run: node scripts/validate-changeset-config.js
</file>

<file path=".claude/commands/pr.md">
# PR Workflow Command

This command follows the established workflow for managing pull requests according to the project's Kanban workflow with comprehensive CI/CD integration.

## Usage
```
/pr <pr_number>
```

## Description
Manages a pull request through the workflow stages with full CI/CD pipeline integration, GitHub Actions status monitoring, and Definition of Done validation.

## Instructions for Claude

When this command is used with a PR number:

1. **Get PR Information & CI Status**
   - Use `gh pr view <pr_number> --json state,title,body,url,headRefName,baseRefName,mergeable,reviewDecision,statusCheckRollup` to get comprehensive PR details
   - **CI/CD Status Analysis**: Check GitHub Actions workflow status with `gh pr checks <pr_number>`
   - **Detailed Check Analysis**: For failing checks, use `gh run view <run_id>` to get specific failure details
   - **Test Results**: Parse test output and identify specific failing tests
   - **Conflict Detection**: Check for other open PRs on the same issue
   - **Validate PR State**: Ensure PR is in valid state for workflow progression
   - **Deployment Status**: Monitor deployment progress through GitHub Actions logs

2. **Definition of Done Validation**
   Before any status progression, validate all applicable DoD criteria:
   - **Code Quality**: Verify no linter warnings/errors with `gh pr checks <pr_number>`
   - **Testing**: Ensure all tests pass in CI pipeline
   - **Documentation**: Check if README.md updates are needed
   - **ADR Requirements**: Verify ADR creation for architectural decisions
   - **Security**: Validate no secrets are hard-coded
   - **Review Process**: Confirm peer review approval
   - **Closing Keywords**: Verify PR description includes `closes #123`, `fixes #123`, or `resolves #123`

3. **Follow Workflow Progression**
   Based on current PR state and DoD validation, update the GitHub Project Status field:
   
   - **Draft PR**: Ensure related issue has "In Progress" status
   - **Ready for Review**: Move issue to "Review & Testing" status
   - **Approved PR with passing CI**: Issue remains "Review & Testing" until merge
   - **Merged PR**: Update related issue(s) to "Deployed" status after successful deployment

4. **Update Project Board**
   - Update the Status field for associated issue(s) in the GitHub Project
   - Link PR to related issues if not already linked
   - **Issue Closing**: Ensure PR description uses closing keywords (`closes #123`, `fixes #123`, `resolves #123`) to automatically close issues when merged
   - Use GitHub's built-in automation where possible

5. **Comprehensive CI/CD Integration**
   - **GitHub Actions Status**: Use `gh pr checks <pr_number>` to get detailed check status
   - **Workflow Analysis**: Parse workflow results from `on-merged-master.yaml`, `claude.yaml`, and `on-pr-master-dependabot.yml`
   - **Build Status**: Monitor build process and package compilation
   - **Test Results**: Analyze test failures and provide specific error analysis
   - **Dependency Checks**: Verify `pnpm audit` security scans pass
   - **Deployment Tracking**: Monitor deployment progress through workflow logs
   - **Rollback Procedures**: Provide rollback instructions for failed deployments

6. **Review & Testing Management**
   - If PR needs review and no reviewers assigned, suggest adding reviewers
   - **CI Status Validation**: Check all CI/CD checks before status changes
   - **Review Status Validation**: Verify approval state before progression
   - Verify all conversations are resolved if approved
   - Ensure both review approval AND CI passing before allowing merge
   - **Error Handling**: Provide detailed CI failure analysis and common fixes
   - **DoD Compliance**: Validate all Definition of Done criteria are met

7. **Provide Comprehensive Status Summary**
   Give a detailed summary of:
   - Current PR status and workflow stage
   - CI/CD pipeline status with specific check results
   - Definition of Done compliance status
   - Any Status field updates made
   - Next steps or blockers with specific actions
   - Related issues and their status
   - Deployment status and rollback procedures if needed
   - Error analysis and recommended fixes for failing checks

## Workflow Stage Mapping

| PR State | Project Status | CI Requirements | DoD Validation | Next Action |
|----------|----------------|-----------------|----------------|--------------|
| Draft | In Progress | Basic lint/format | Partial | Complete development |
| Ready for Review | Review & Testing | All checks passing | Full validation | Await review + CI |
| Approved + CI Passing | Review & Testing | All checks passing | Complete | Merge PR |
| Merged | Deployed | Deployment success | Complete | Verify deployment and close issues |

## Validation Safeguards

### PR State Validation
- Verify PR exists and is accessible
- Check PR is not closed or abandoned
- Validate PR author and permissions

### Conflict Detection
- Check for multiple PRs addressing same issue
- Warn about potential merge conflicts
- Validate base branch is up to date

### Enhanced Status Transition Rules
- **In Progress → Review & Testing**: Requires basic CI checks (lint, format) and DoD partial validation
- **Review & Testing → Deployed**: Requires all CI checks passing, peer review approval, and complete DoD validation
- **Deployed**: Only after successful merge AND deployment workflow completion
- **Rollback Procedures**: Clear instructions for reverting deployments if issues arise
- **CI Prerequisites**: All GitHub Actions workflows must pass before status advancement
- **DoD Compliance**: Full Definition of Done validation required before merge
- **Closing Keywords**: Verify PR description includes appropriate closing keywords for related issues
- **Prevent Invalid Transitions**: Block status changes that don't meet workflow requirements

### Enhanced Error Recovery & Analysis
- **CI Failures**: 
  - Parse specific workflow logs from GitHub Actions
  - Identify failing test cases and provide fix suggestions
  - Check for common issues: linting, type errors, dependency conflicts
  - Provide commands to reproduce errors locally
- **Review Blockers**: 
  - List pending reviewers and their status
  - Identify unresolved conversations
  - Check for requested changes
- **Merge Conflicts**: 
  - Guide through resolution with specific commands
  - Suggest rebase vs merge strategies
- **Deployment Failures**: 
  - Monitor deployment workflow progress
  - Parse deployment logs for specific errors
  - Provide rollback procedures with specific commands
  - Check for infrastructure or configuration issues
- **DoD Compliance**: 
  - Identify missing DoD criteria
  - Suggest specific actions to achieve compliance
  - Validate ADR requirements for architectural changes

## CI/CD Commands Integration

### GitHub Actions Status Commands
```bash
# Get comprehensive PR check status
gh pr checks <pr_number>

# View specific workflow run details
gh run view <run_id>

# Get workflow logs for debugging
gh run view <run_id> --log

# Monitor deployment status
gh run list --workflow="Merged to Master" --limit=5
```

### Definition of Done Validation Commands
```bash
# Check linting status
pnpm lint

# Run tests locally
pnpm test

# Security audit
pnpm audit

# Build verification
pnpm build
```

## Example Usage

```
/pr 123
```

This will:
1. **Comprehensive Status Check**: Analyze PR #123 with CI/CD pipeline integration
2. **DoD Validation**: Verify all Definition of Done criteria are met
3. **CI Analysis**: Parse GitHub Actions results and identify specific failures
4. **Deployment Monitoring**: Track deployment progress through workflow logs
5. **Conflict Detection**: Check for merge conflicts and other blocking issues
6. **Project Board Update**: Update GitHub Project Status field based on workflow stage
7. **Error Analysis**: Provide detailed failure analysis and fix recommendations
8. **Rollback Guidance**: Offer rollback procedures if deployment fails
9. **Next Steps**: Clear action items for PR progression or issue resolution
</file>

<file path="CLAUDE.md">
# HAppy VErtical SDK: Architecture and Development Guide

## Overview

The HAppy VErtical (HAVE) SDK is a TypeScript monorepo designed for building vertical AI agents. It follows these core principles:

- Pure TypeScript implementation to avoid CommonJS vs ESM compatibility issues
- Minimized dependencies through a modular monorepo architecture
- Compartmentalized code to keep AI agents lean and focused
- Support for testing and scaling with minimal overhead
- Standardized interfaces across different packages

## Monorepo Structure

The SDK is organized as a pnpm workspace with several packages that provide specific functionality:

- **ai**: A standardized interface for AI model interactions, currently supporting OpenAI
- **files**: Tools for interacting with file systems (local and remote)
- **pdf**: Utilities for parsing and processing PDF documents
- **smrt**: Core library for building AI agents with standardized collections and objects 
- **spider**: Web crawling and content parsing tools (renamed from "web")
- **sql**: Database interaction with support for SQLite and Postgres
- **utils**: Shared utility functions used across packages

## Development Patterns

### Dependency Management

- Package versioning is synchronized across the monorepo
- Internal dependencies use `workspace:*` to reference other packages
- External dependencies are kept to a minimum
- Node.js v22.x is required for all packages

### Build Process

The build process follows a specific order to respect internal dependencies:

1. `@have/utils` (base utilities used by all packages)
2. `@have/files` (file system interactions)
3. `@have/spider` (web crawling)
4. `@have/sql` (database interactions)
5. `@have/pdf` (PDF processing)
6. `@have/ai` (AI model interfaces)
7. `@have/smrt` (agent framework that depends on all the above)

### Code Style and Conventions

- Code formatting is enforced by Biome
- Spaces (2) for indentation
- Single quotes for strings
- Line width of 80 characters
- ESM module format exclusively
- Each package has its own tsconfig that extends from the root
- Use camelCase for variables and functions, PascalCase for classes
- Use conventional commits
- Dont include claude branding in commit messages
- use pnpm 

### Testing

- Tests are written using Vitest
- Each package has its own test suite
- Run tests with `pnpm test` or `pnpm test:watch`

### Common Development Commands

```bash
# Install dependencies
pnpm install

# Run tests
pnpm test

# Build all packages in correct order
pnpm build

# Watch mode development
pnpm dev

# Lint code
pnpm lint

# Format code
pnpm format
```

## Cross-Package Dependencies

The packages have these dependency relationships:

- `utils`: No internal dependencies
- `files`: Depends on `utils`
- `spider`: Depends on `utils` and `files`
- `sql`: No internal dependencies
- `pdf`: No internal dependencies
- `ai`: No internal dependencies
- `smrt`: Depends on all other packages

When adding new features, maintain this dependency hierarchy to avoid circular dependencies.

## Contribution Guidelines

1. Ensure code passes Biome linting (`pnpm lint`)
2. Write tests for new functionality
3. Update package documentation when adding new features
4. Follow existing code patterns in each package
5. Run the full test suite before submitting changes

## Development Workflow

HAppy VErtical follows a standardized development workflow across all projects. The workflow documentation serves as the organization's source of truth:

- **[Definition of Ready](./docs/workflow/DEFINITION_OF_READY.md)**: Criteria that must be met before an issue can be started
- **[Definition of Done](./docs/workflow/DEFINITION_OF_DONE.md)**: Checklist for completing Pull Requests
- **[Kanban Process](./docs/workflow/KANBAN.md)**: Kanban CI/CD workflow with automation setup

All HAppy VErtical projects should reference and follow these workflow standards to ensure consistency across the organization.

### GitHub Issue Management

When creating Pull Requests, use closing keywords in the PR description or commit messages to automatically close related issues when the PR is merged:

- `closes #123` - Closes issue #123 when PR is merged
- `fixes #123` - Closes issue #123 when PR is merged  
- `resolves #123` - Closes issue #123 when PR is merged

Example PR description:
```
## Summary
Implement user authentication system

## Changes
- Add login/logout functionality
- Implement JWT token management
- Add user session handling

Closes #45
Fixes #67
```

This ensures issues are automatically moved through the workflow and closed when work is complete.

## Tooling Configuration

- **TypeScript**: Configured for ES2022 with strict type checking
- **Biome**: Used for linting and formatting
- **pnpm**: Package management with workspace support
- **Vitest**: Testing framework
- **Changesets**: Used for versioning and publishing packages
- **TypeDoc**: Used for generating API documentation

## Documentation

The SDK includes automatic API documentation generation using TypeDoc. The documentation is stored in the `docs/manual` directory and can be viewed by opening `docs/manual/index.html` in a browser.

Documentation is generated as part of the build process, but can also be generated separately:

```bash
pnpm docs
```

The build pipeline integrates documentation generation after all packages are built and before repomix is run:

```bash
pnpm build  # Includes documentation generation
```

This repository is designed to support building AI agents with minimal overhead and maximum flexibility.

##
</file>

<file path="package.json">
{
  "name": "@have/sdk",
  "private": true,
  "author": "Will Griffin <willgriffin@gmail.com>",
  "type": "module",
  "version": "0.0.50",
  "scripts": {
    "format": "biome format --write .",
    "format-check": "biome format --check .",
    "prepare": "lefthook install",
    "test": "vitest",
    "test:watch": "vitest --watch",
    "test:run": "vitest --run",
    "test:packages": "pnpm -r test --run",
    "lint": "biome lint .",
    "lint:fix": "biome lint --apply .",
    "clean": "pnpm -r exec rm -rf dist && pnpm -r exec rm -rf tsconfig.tsbuildinfo",
    "build": "pnpm validate-build && pnpm clean && pnpm --filter @have/utils build && pnpm --filter @have/files build && pnpm --filter @have/spider build && pnpm --filter @have/sql build && pnpm --filter @have/pdf build && pnpm --filter @have/ai build && pnpm --filter @have/smrt build && pnpm docs && pnpm repomix",
    "docs": "node scripts/generate-docs.js",
    "validate-build": "node scripts/validate-build.js",
    "dev": "pnpm -r build:watch",
    "publish-packages": "pnpm build && changeset publish",
    "release": "standard-version --no-verify",
    "postrelease": "node scripts/update-package-versions.cjs"
  },
  "devDependencies": {
    "@biomejs/biome": "^1.5.3",
    "@changesets/cli": "^2.27.12",
    "conventional-changelog-cli": "^5.0.0",
    "lefthook": "^1.11.13",
    "repomix": "^0.3.6",
    "standard-version": "^9.5.0",
    "typedoc": "^0.28.4",
    "typedoc-plugin-markdown": "^4.6.3",
    "typescript": "^5.7.3",
    "vitest": "^3.1.1"
  },
  "resolutions": {
    "@types/node": "22.13.0"
  },
  "engines": {
    "node": "22.x"
  }
}
</file>

<file path="README.md">
# HAppy VErtical SDK (HAVE SDK)

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

A modular TypeScript SDK for building vertical AI agents with minimal dependencies and maximum flexibility.

## Overview

HAVE SDK is designed with these core principles:

- **Pure TypeScript** implementation to avoid CommonJS vs ESM compatibility issues
- **Minimal dependencies** through a carefully designed monorepo architecture
- **Compartmentalized code** to keep AI agents lean and focused
- **Easy testing and scaling** with minimal overhead
- **Standardized interfaces** across different packages

## Packages

| Package | Description |
|---------|-------------|
| [@have/ai](./packages/ai/) | Standardized interface for AI model interactions, currently supporting OpenAI |
| [@have/files](./packages/files/) | Tools for interacting with file systems (local and remote) |
| [@have/pdf](./packages/pdf/) | Utilities for parsing and processing PDF documents |
| [@have/smrt](./packages/smrt/) | Core library for building AI agents with standardized collections and objects |
| [@have/spider](./packages/spider/) | Web crawling and content parsing tools |
| [@have/sql](./packages/sql/) | Database interaction with support for SQLite and Postgres |
| [@have/utils](./packages/utils/) | Shared utility functions used across packages |

## Installation

```bash
# Install with npm
npm install @have/smrt

# Or with yarn
yarn add @have/smrt

# Or with pnpm
pnpm add @have/smrt
```

You can also install individual packages based on your needs:

```bash
pnpm add @have/ai @have/files @have/spider
```

## Getting Started

```typescript
import { Agent } from '@have/smrt';
import { OpenAIModel } from '@have/ai';

// Create a new agent
const agent = new Agent({
  model: new OpenAIModel({ apiKey: process.env.OPENAI_API_KEY }),
  // Configure additional tools as needed
});

// Use the agent
const result = await agent.run('Analyze this text and extract key insights');
console.log(result);
```

See each package's README for more detailed usage examples.

## Development

```bash
# Install dependencies
pnpm install

# Run tests
pnpm test

# Build all packages in correct order
pnpm build

# Watch mode development
pnpm dev

# Lint code
pnpm lint

# Format code
pnpm format
```

## Documentation

### Local Documentation

The SDK provides automatically generated HTML documentation in the `docs/manual` directory.
This is generated during the build process and can be viewed by opening `docs/manual/index.html` in your browser.

You can generate the documentation separately by running:

```bash
pnpm docs
```

### Online Documentation

The latest API documentation is available online at:

[https://happyvertical.github.io/sdk/](https://happyvertical.github.io/sdk/)

This documentation is automatically updated whenever changes are merged to the master branch.

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for details on how to submit pull requests, the development process, and coding standards.

### Development Workflow

HAppy VErtical follows standardized development workflows across all projects:

- **[Definition of Ready](./docs/workflow/DEFINITION_OF_READY.md)** - Criteria before starting work
- **[Definition of Done](./docs/workflow/DEFINITION_OF_DONE.md)** - PR completion checklist  
- **[Workflow Process](./docs/workflow/KANBAN.md)** - Kanban CI/CD process

These workflow standards are the organization-wide source of truth. See the [workflow documentation](./docs/workflow/) for implementation details.

## License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.
</file>

<file path=".github/workflows/on-merged-master.yaml">
name: Merged to Master

on:
  push:
    branches:
      - master

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Required for creating git tags/releases and pushing changes
      packages: write  # Required for publishing packages to GitHub registry
      pages: write    # Required for GitHub Pages deployment (currently disabled)
      id-token: write # Required for GitHub Pages OIDC authentication
    steps:
      - name: Validate required secrets and permissions
        run: |
          if [ -z "${{ secrets.GITHUB_TOKEN }}" ]; then
            echo "::error::GITHUB_TOKEN is not available"
            exit 1
          fi
          echo "✅ Required secrets are available"
          echo "✅ Workflow permissions validated"

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for changesets
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          registry-url: 'https://npm.pkg.github.com'
          scope: '@have'

      - name: Install pnpm
        run: npm install -g pnpm

      - name: Show pnpm version
        run: pnpm --version

      - name: Cache pnpm store
        uses: actions/cache@v4
        with:
          path: ~/.pnpm-store
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install

      - name: Install Playwright Browsers
        run: pnpm dlx playwright install

      - name: Cache Playwright Browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Build packages
        run: pnpm build

      - name: Configure Git
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

      - name: Bump version and release
        run: pnpm run release

      - name: Commit and push changes
        run: |
          git add .
          git commit -m "chore(release): bump version to $(node -p "require('./package.json').version")"
          git push --follow-tags

      - name: "Convert and add TODOs to github issues"
        uses: "alstr/todo-to-issue-action@v5.0.0"  # Pinned to specific version
        with:
          INSERT_ISSUE_URLS: "true"
          IDENTIFIERS: '[{"name": "TODO", "labels": []}]'
          PROJECT_SECRET: ${{ secrets.GITHUB_TOKEN }}
          PROJECT: happyvertical/sdk

      - name: Commit and Push Changes
        run: |
          git add -A
          if [[ `git status --porcelain` ]]; then
            git commit -m "chore(release): Automatically added GitHub issue links to TODOs"
            git push origin master
          else
            echo "No changes to commit"
          fi

      - name: Publish packages
        run: |
          # Secure token handling - create .npmrc in memory without exposing token
          echo "@have:registry=https://npm.pkg.github.com" > .npmrc
          echo "//npm.pkg.github.com/:_authToken=\${NODE_AUTH_TOKEN}" >> .npmrc
          pnpm run publish-packages
        env:
          NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          NPM_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # disabled until repo goes public          
      # - name: Setup Pages
      #   uses: actions/configure-pages@v4
        
      # - name: Upload documentation artifact
      #   uses: actions/upload-pages-artifact@v3
      #   with:
      #     path: 'docs/manual'
          
      # - name: Deploy to GitHub Pages
      #   id: deployment
      #   uses: actions/deploy-pages@v4
</file>

<file path=".gitea/workflows/agents/claude/on-assigned-issue.yaml">
name: Create PR from Issue Assignment

on:
  issues:
    types: [assigned]

jobs:
  create-pr-on-assignment:
    runs-on: ubuntu-latest
    if: gitea.event.issue.assignee.login == 'claude'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Get issue details
        id: get-issue-details
        run: |
          echo "Fetching issue details..."
          echo "Gitea Token: ${{ secrets.GITEA_TOKEN }}"

          # Install gitea-cli if needed
          if ! command -v tea &> /dev/null; then
            echo "Installing gitea-cli..."
            curl -sL https://dl.gitea.io/tea/main/tea-main-linux-amd64 -o tea
            chmod +x tea
            mv tea /usr/local/bin/
          fi
          REPO_NAME="${{ gitea.repository }}"
          echo $REPO_NAME
          ISSUE_NUM="${{ gitea.event.issue.number }}"
          echo $ISSUE_NUM
          tea login add -u https://git.grffn.net/ --token ${{ secrets.GITEA_TOKEN }}
          echo "logged in successfully"
          # tea issue view "$REPO_NAME#$ISSUE_NUM" --format '{{ .Title }}''
          ISSUE_TITLE=$(tea issue --repo $REPO_NAME $ISSUE_NUM --format '{{ .Title }}')
          echo $ISSUE_TITLE
          ISSUE_BODY=$(tea issue --repo $REPO_NAME $ISSUE_NUM)
          echo $ISSUE_BODY
          echo "title=$ISSUE_TITLE" >> $GITHUB_OUTPUT
          echo "body=<<EOF" >> $GITHUB_OUTPUT
          echo "$ISSUE_BODY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
      
      - name: Install Claude Code CLI
        run: npm install -g @anthropic-ai/claude-code
      
      - name: Create branch and changes
        run: |
          # Use issue number for branch name
          BRANCH_NAME="issue-${{ gitea.event.issue.number }}"
          git checkout -b $BRANCH_NAME
          git push --set-upstream origin $BRANCH_NAME
          REPO_NAME="${{ gitea.repository }}"
          echo $REPO_NAME
          ISSUE_NUM="${{ gitea.event.issue.number }}"
          echo $ISSUE_NUM

          ISSUE_BODY=$(tea issue --repo $REPO_NAME $ISSUE_NUM)

          echo "Create changes to address this issue: 
          Title: ${{ steps.get-issue-details.outputs.title }} 
          Description: ${{ steps.get-issue-details.outputs.body }}"

          # Call Claude Code with issue content as context
          claude  --print "Create changes to address this issue: 
          Title: ${{ steps.get-issue-details.outputs.title }} 
          Description: $ISSUE_BODY"
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      
      - name: Push changes and create PR
        run: |
          git config user.name "Gitea Actions Bot"
          git config user.email "actions-bot@example.com"
          git add .
          git commit -m "Address issue #${{ gitea.event.issue.number }}" || echo "No changes to commit"
          git push origin $BRANCH_NAME          
          # Create PR using tea CLI
          REPO_NAME="${{ gitea.repository }}"
          tea pulls create \
            --repo "$REPO_NAME" \
            --base "main" \
            --head "$BRANCH_NAME" \
            --title "Fix #${{ gitea.event.issue.number }}: ${{ steps.get-issue-details.outputs.title }}" \
            --body "This PR addresses issue #${{ gitea.event.issue.number }} \
              Original issue description: \
              ${{ steps.get-issue-details.outputs.body }}"          
          # Add a comment on the issue with the PR link
          PR_URL="${{ vars.GITEA_URL }}/$REPO_NAME/pulls/$(tea pulls list --repo "$REPO_NAME" --head "$BRANCH_NAME" --format '{{ .Index }}')"
          tea issue comment "$REPO_NAME#${{ gitea.event.issue.number }}" --body "I've created PR: $PR_URL to address this issue."
          tea issue label "$REPO_NAME#${{ gitea.event.issue.number }}" --add "pr-created"
</file>

</files>
